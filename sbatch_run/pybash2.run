#!/bin/bash
#SBATCH --job-name=python
#SBATCH --account=torri #comment for shared
#SBATCH --partition=torri #comment for shared

##SBATCH --partition=shared

#SBATCH --time=3-00:00:00
#SBATCH --cpus-per-task=1
##SBATCH --ntasks=1
#SBATCH --nodes=1
#SBATCH  --tasks-per-node=1
##SBATCH --cpus-per-task=1 #max 16
#SBATCH --mem=0
##SBATCH --constraint=“ib_hdr” # will allow a mix of HDR200 and HDR100 nodes
##SBATCH --constraint=“ib_hdr100” # explicitly only allow HDR100 nodes
##SBATCH --distribution=“*:*:*” # Set the distribution to defaults if doing sbatch from interactive session
#SBATCH --error=pybash2-%A.err
#SBATCH --output=pybash2-%A.out
## Remote notification
#SBATCH --mail-type=BEGIN,END,FAIL,REQUEUE,TIME_LIMIT_80
#SBATCH --mail-user=air673@hawaii.edu

module purge
module load lang/Anaconda3
export PYTHONUNBUFFERED=TRUE #allows print statements during run
python -u pybash2.py

# Configure the Intel MPI parameters
##export I_MPI_FABRICS=shm:ofi
##export I_MPI_PMI_LIBRARY=/lib64/libpmi.so
##export I_MPI_HYDRA_TOPOLIB=ipl # May be required if newer libfabric and intel MPI is used
### FOR HDR NETWORK #####
# https://ofiwg.github.io/libfabric/master/man/
# https://ofiwg.github.io/libfabric/v1.9.1/man/
##export FI_PROVIDER=“shm,verbs;ofi_rxm”
##export FI_MR_CACHE_MONITOR=disabled # currently a bug exists that a segfault could happen
##export FI_VERBS_MR_CACHE_ENABLE=0 # currently a bug exists that a segfault could happen
##export FI_VERBS_INLINE_SIZE=256
##export FI_UNIVERSE_SIZE=${SLURM_NTASKS} # should equal at least the max number of tasks one task will communicate with
##export FI_VERBS_IFACE=i
###### ######## ###### ##
# Configure the Intel MPI parameters
# dapl is supported by Intel 2018, but is deprecated in newer version of Intel
#export I_MPI_FABRICS=shm:dapl
#export I_MPI_PMI_LIBRARY=/lib64/libpmi.so
