{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64feaab-8094-4b6f-b945-7098711d727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in Packages and Data\n",
    "\n",
    "#Importing Packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.gridspec as gridspec\n",
    "import xarray as xr\n",
    "import os; import time\n",
    "import pickle\n",
    "import h5py\n",
    "###############################################################\n",
    "def coefs(coefficients,degree):\n",
    "    coef=coefficients\n",
    "    coefs=\"\"\n",
    "    for n in range(degree, -1, -1):\n",
    "        string=f\"({coefficients[len(coef)-(n+1)]:.1e})\"\n",
    "        coefs+=string + f\"x^{n}\"\n",
    "        if n != 0:\n",
    "            coefs+=\" + \"\n",
    "    return coefs\n",
    "###############################################################\n",
    "\n",
    "#Importing Model Data\n",
    "check=False\n",
    "dir='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "data=xr.open_dataset(dir+'cm1r20.3/run/cm1out_test7tundra-7_062217.nc') #***\n",
    "true_time=data['time']\n",
    "parcel=xr.open_dataset(dir+'cm1r20.3/run/cm1out_pdata_test5tundra-7_062217.nc') #***\n",
    "times=data['time'].values/(1e9 * 60); times=times.astype(float);\n",
    "# parcel=parcel.isel(time=times.astype(int)) \n",
    "\n",
    "#Restricts the timesteps of the data from timesteps0 to 140\n",
    "data=data.isel(time=np.arange(0,140+1))\n",
    "parcel=parcel.isel(time=np.arange(0,140+1))\n",
    "\n",
    "def grid_location(x,y,z):\n",
    "    xf=data['xf'].values*1000; which_xh=np.searchsorted(xf,x)-1; which_xh=np.where(which_xh == -1, 0, which_xh) #finds which x layer parcel in\n",
    "    yf=data['yf'].values*1000; which_yh=np.searchsorted(yf,y)-1; which_yh=np.where(which_yh == -1, 0, which_yh) #finds which y layer parcel in\n",
    "    zf=data['zf'].values*1000; which_zh=np.searchsorted(zf,z)-1; which_zh=np.where(which_zh == -1, 0, which_zh) #finds which z layer parcel in   \n",
    "    return which_zh,which_yh,which_xh\n",
    "    \n",
    "# def grid_location(x,y,z): #faster\n",
    "#     #finding xf and yf\n",
    "#     ybins=data['yf'].values*1000; dy=ybins[1]-ybins[0] #1000\n",
    "#     xbins=data['xf'].values*1000; dx=xbins[1]-xbins[0] #1000\n",
    "#     dy=np.round(dy);dx=np.round(dx)\n",
    "\n",
    "#     #digitizing\n",
    "#     zf=data['zf'].values*1000; which_zh=np.searchsorted(zf,z)-1; which_zh=np.where(which_zh == -1, 0, which_zh) #finds which z layer parcel in \n",
    "#     if which_zh.ndim==0:\n",
    "#         which_zh=np.array([which_zh])\n",
    "#     which_yh=np.floor(y/dy).astype(int)+np.where(data['yf']==0)[0]\n",
    "#     which_xh=np.floor(x/dx).astype(int)+np.where(data['xf']==0)[0]\n",
    "\n",
    "#     #fixing boundaries\n",
    "#     which_zh[np.where(which_zh==len(data['zh']))]-=1\n",
    "#     which_yh[np.where(which_yh==len(data['yh']))]-=1\n",
    "#     which_xh[np.where(which_xh==len(data['xh']))]-=1\n",
    "#     return which_zh,which_yh,which_xh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a220e3-f0cd-4880-a87b-1243e4087270",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "#Flag Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7717dfcd-5687-47b2-bd03-6cddbf1666d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save parcel_flag_array storing parcel id at location in eulerian grid at all times\n",
    "\n",
    "yes_run=False\n",
    "yes_run=True #uncomment if running\n",
    "\n",
    "if yes_run==True: \n",
    "\n",
    "    if 'emptylike' not in globals():\n",
    "        print('making empty array')\n",
    "        flag_array = np.empty_like(data['qc']) #uses a variable that is (zh,yh,xh)\n",
    "        empty_array = flag_array.copy()[0]\n",
    "\n",
    "        output_file = dir+f'tracking_algorithms/plots/parcel_flag_array.h5' \n",
    "        with h5py.File(output_file, 'w') as f:\n",
    "            f.create_dataset('flag_array', data=flag_array, compression=\"gzip\")\n",
    "        del flag_array\n",
    "        emptylike=True\n",
    "        print('done')\n",
    "          \n",
    "    ############################################################\n",
    "    for t in np.arange(len(data['time'])):\n",
    "    # for t in [33,34]: #TESTING\n",
    "        print(f'current timestep {t}')\n",
    "        parcel_flag=empty_array.copy()\n",
    "\n",
    "        indexes=parcel['xh'].values \n",
    "        xs=parcel['x'].isel(time=t).values\n",
    "        ys=parcel['y'].isel(time=t).values\n",
    "        zs=parcel['z'].isel(time=t).values\n",
    "\n",
    "        position=grid_location(xs,ys,zs)\n",
    "\n",
    "        #OLD\n",
    "        # parcel_flag[position]=indexes\n",
    "        #NEW\n",
    "        position_tuples = list(zip(position[0], position[1], position[2]))\n",
    "        one,two=np.unique(position_tuples,return_counts=True,axis=0)\n",
    "        parcel_flag[tuple(zip(*one))] += two\n",
    "\n",
    "        with h5py.File(output_file, 'a') as f:\n",
    "            f['flag_array'][t] = parcel_flag\n",
    "        del parcel_flag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e13b70-c9dd-4cbf-9cdd-ede29d446367",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# OUT_NZ\n",
    "\n",
    "# out=xr.open_dataset(dir+'tracking_algorithms/trackout/parcel_tracking4tundra-7_062217.nc')['out_arr'].values;out=out.astype(object);out[:, [0,1,2,4,5]] = out[:, [0,1,2,4,5]].astype(int) #***\n",
    "# save=xr.open_dataset(dir+'tracking_algorithms/trackout/parcel_tracking4tundra-7_062217.nc')['save_arr'].values;save=save.astype(object);save[:, [0,1,2,4,5]] = save[:, [0,1,2,4,5]].astype(int) #***\n",
    "\n",
    "out=xr.open_dataset(dir+'tracking_algorithms/trackout/parcel_tracking_everywhere.nc')['out_arr'].values;out=out.astype(object);out[:, [0,1,2,4,5]] = out[:, [0,1,2,4,5]].astype(int) #***\n",
    "save=xr.open_dataset(dir+'tracking_algorithms/trackout/parcel_tracking_everywhere.nc')['save_arr'].values;save=save.astype(object);save[:, [0,1,2,4,5]] = save[:, [0,1,2,4,5]].astype(int) #***\n",
    "\n",
    "out_nz=out[~np.all(out == 0, axis=1)];#print('list of first 10 SBZ parcels'); print(out_nz[:15])\n",
    "save_nz=save[~np.all(save == 0, axis=1)];save_nz=save_nz[np.where(np.unique(save_nz[1:-1,0]))];#print('list of first 10 ignored parcels');print(save_nz[:5])\n",
    "\n",
    "###############################################################################\n",
    "#remove duplicates\n",
    "lst=[]\n",
    "unique_values, counts = np.unique(out_nz[:,0], return_counts=True); duplicates = unique_values[counts > 1]\n",
    "for elem in duplicates:\n",
    "    idx = np.where(out_nz[:,0] == elem)[0] \n",
    "    extras=idx[np.where(out_nz[idx,5]!=np.nanmin(out_nz[idx,5]))]\n",
    "    lst.extend([x for x in extras])\n",
    "mask=np.ones(len(out_nz), dtype=bool); mask[lst] = False\n",
    "out_nz=out_nz[mask]; \n",
    "placeholder=out_nz.copy(); run=True\n",
    "###############################################################################\n",
    "\n",
    "# out_nz=out[~np.all(out == 0, axis=1)];print('list of first 10 SBZ parcels'); print(out_nz[:15])\n",
    "# save_nz=save[~np.all(save == 0, axis=1)];save_nz=save_nz[np.where(np.unique(save_nz[1:-1,0]))];print('list of first 10 ignored parcels');print(save_nz[:5])\n",
    "# print(f'there are a total of {len(out_nz)} SBZ parcels and {len(save_nz)} forgotten parcels')\n",
    "\n",
    "#search for deep convective parcels within lagrangian tracking output     \n",
    "##############################################################\n",
    "def threshold(zthresh):\n",
    "    out_nz=placeholder.copy()\n",
    "    \n",
    "    deep_out_ind=[]; extendrange=[]\n",
    "    times=data['time'].values/(1e9 * 60); times=times.astype(float);\n",
    "    for ind in range(len(out_nz)): \n",
    "        #searchs if next most local max goes above zthresh\n",
    "        nummins=120; numsteps=int(nummins/times[1])\n",
    "        aboverange=np.arange(out_nz[ind,5],out_nz[ind,5]+numsteps,1) #range of times between current time and numsteps later\n",
    "        aboverange=aboverange[aboverange<len(data['time'])] #caps out at max time\n",
    "        above=parcel['z'].isel(xh=out_nz[ind,0],time=aboverange).values/1000\n",
    "    \n",
    "        dt=1\n",
    "        #takes dabove/dt\n",
    "        f=above\n",
    "        ddx = (\n",
    "                f[1:  ]\n",
    "                -\n",
    "                f[0:-1]\n",
    "            ) / (\n",
    "            2 * dt\n",
    "        )\n",
    "        signs = np.sign(ddx)\n",
    "        signs_diff=np.diff(signs)\n",
    "        local_maxes=np.where((signs_diff != 0) & (signs_diff < 0))[0]+1 #make sure +1 is here\n",
    "        if len(local_maxes)==0:\n",
    "            local_maxes=[0]\n",
    "        \n",
    "        if np.any(above[local_maxes[0]]>zthresh):\n",
    "            extendrange.append(local_maxes[0]) #save to extend xlim of plot later\n",
    "            deep_out_ind.append(ind)\n",
    "    \n",
    "    out_nz=out_nz[deep_out_ind,:]\n",
    "    # print(f'> {zthresh} km. {len(out_nz)} leftover parcels')\n",
    "    return out_nz, extendrange\n",
    "    # print(out_nz)\n",
    "##############################################################\n",
    "\n",
    "# convectivelevel=4 #4km\n",
    "convectivelevel=6 #6km\n",
    "# convectivelevel=8 #8km\n",
    "# convectivelevel=10 #8km\n",
    "[out_nz,extendrange]=threshold(convectivelevel)\n",
    "\n",
    "print('list of first 10 SBZ parcels'); print(out_nz[:15])\n",
    "print(f'there are a total of {len(out_nz)} SBZ parcels and {len(save_nz)} forgotten parcels')\n",
    "\n",
    "\n",
    "#DEEP CONVECTIVE VERSION (run only if using Deep Convective)\n",
    "#Setting up run for deep convective parcels\n",
    "# np.arange(Np)==>out_nz[:,0]\n",
    "# np.arange(Nt)==>out_nz[:,4]\n",
    "col4 = out_nz[:, 4].astype(int)\n",
    "# col5 = out_nz[:, 5].astype(int)\n",
    "col5 = out_nz[:, 5].astype(int) + 2 #add another 10 minutes to each trajectory\n",
    "ranges = [np.arange(start, end + 1) for start, end in zip(col4, col5)]\n",
    "indexes = out_nz[:,0]\n",
    "indexranges = [np.full_like(r, fill_value=index) for r, index in zip(ranges, indexes)]\n",
    "totalelements=sum(len(arr) for arr in ranges)\n",
    "print(f'running for total of {totalelements} elements')\n",
    "\n",
    "x=parcel['x'];y=parcel['y'];z=parcel['z']\n",
    "which_z,which_y,which_x=grid_location(x,y,z)\n",
    "\n",
    "#Makes np.where style mask for deep parcel locations\n",
    "\n",
    "deep_t,deep_z,deep_y,deep_x=[],[],[],[]\n",
    "for R in np.arange(len(ranges)):\n",
    "    tup=(ranges[R],indexranges[R])\n",
    "    deep_t.extend(ranges[R])\n",
    "    deep_z.extend(which_z[tup])\n",
    "    deep_y.extend(which_y[tup])\n",
    "    deep_x.extend(which_x[tup])\n",
    "deep_t=np.array(deep_t);deep_z=np.array(deep_z);deep_y=np.array(deep_y);deep_x=np.array(deep_x)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfa1c51-71da-4b85-ad85-13928cc7872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save DEEP_parcel_flag_array storing parcel id at location in eulerian grid at all times\n",
    "# all locations marked 1 regardless of how many lagrangian parcels located there.\n",
    "\n",
    "yes_run=False\n",
    "yes_run=True #uncomment if running\n",
    "\n",
    "if yes_run==True: \n",
    "\n",
    "    if 'emptylike' not in globals():\n",
    "        print('loading neccessary variables')\n",
    "        variable='w'; w_data=data[variable] #get w data\n",
    "        w_data=w_data.interp(zf=data['zh']).data #interpolation w data z coordinate from zh to zf\n",
    "        variable='qc'; qc_data=data[variable].data # get qc data\n",
    "        variable='qi'; qi_data=data[variable].data # get qc data\n",
    "        qc_plus_qi=qc_data+qi_data\n",
    "        print('done loading')\n",
    "        emptylike=True\n",
    "    \n",
    "    for type in ['all','cloudy','updraft','cloudyupdraft']:\n",
    "        print(f\"currently on type {type}\")\n",
    "    \n",
    "\n",
    "        print('making empty array')\n",
    "        flag_array = np.empty_like(data['qc']) #uses a variable that is (zh,yh,xh)     \n",
    "        position=(deep_t,deep_z,deep_y,deep_x)\n",
    "\n",
    "        w_thresh=1\n",
    "        qcqi_thresh=1e-6\n",
    "        if type=='all':\n",
    "            #thresholding flag_array\n",
    "            flag_array[position]=1 \n",
    "            pass\n",
    "        elif type=='cloudy':\n",
    "            mask1 = position\n",
    "            mask2 = np.where(qc_plus_qi >= qcqi_thresh) #*\n",
    "        elif type=='updraft':\n",
    "            mask1 = position\n",
    "            mask2 = np.where(w_data >= w_thresh)\n",
    "        elif type=='cloudyupdraft':\n",
    "            mask1 = position\n",
    "            mask2 = np.where((w_data >= w_thresh) & (qc_plus_qi >= qcqi_thresh))\n",
    "\n",
    "\n",
    "        if type!='all': #only happens when applying threshold\n",
    "            mask1_set = set(zip(*mask1))  \n",
    "            mask2_set = set(zip(*mask2)) \n",
    "            shared_positions = mask1_set.intersection(mask2_set)\n",
    "            position_array = np.array(list(shared_positions))\n",
    "            position2 = (position_array[:, 0], position_array[:, 1], position_array[:, 2], position_array[:, 3])\n",
    "            #thresholding flag_array\n",
    "            flag_array[position2]=1     \n",
    "\n",
    "        \n",
    "        if type=='all':\n",
    "            output_file = dir+f'tracking_algorithms/plots/DEEP_all_flag_array.h5' \n",
    "        elif type=='cloudy':\n",
    "            output_file = dir+f'tracking_algorithms/plots/DEEP_cloudy_flag_array_wthresh{w_thresh}.h5' \n",
    "        elif type=='updraft':\n",
    "            output_file = dir+f'tracking_algorithms/plots/DEEP_updraft_flag_array_wthresh{w_thresh}.h5' \n",
    "        elif type=='cloudyupdraft':\n",
    "            output_file = dir+f'tracking_algorithms/plots/DEEP_cloudyupdraft_flag_array_wthresh{w_thresh}.h5' \n",
    "        with h5py.File(output_file, 'w') as f:\n",
    "            f.create_dataset('flag_array', data=flag_array, compression=\"gzip\")\n",
    "        del flag_array\n",
    "        \n",
    "        print('done')\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f61395-8be5-446d-accb-40b2892dbe18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1734052-dbc5-424d-873a-8deeabf89134",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#created flag array with locations of all parcels in eulerian frame\n",
    "#only save locations where the parcel is in a cloudy region\n",
    "\n",
    "# save flag_array updraft, cloudy, or cloudy_updraft\n",
    "yes_run=False \n",
    "yes_run=True #uncomment if running\n",
    "\n",
    "def make_flag_array(type):\n",
    "    global w_thresh\n",
    "    w_thresh=1\n",
    "    qc_thresh=1e-6\n",
    "    qcqi_thresh=1e-6\n",
    "    \n",
    "    if yes_run==True: \n",
    "        \n",
    "        if 'emptylike' not in globals():\n",
    "            print('making empty array')\n",
    "            flag_array = np.empty_like(data['qc'])\n",
    "            empty_array = flag_array.copy()[0]\n",
    "            if type=='cloudy':\n",
    "                output_file = dir+f'tracking_algorithms/plots/cloudy_flag_array_wthresh{w_thresh}.h5' \n",
    "            elif type=='updraft':\n",
    "                output_file = dir+f'tracking_algorithms/plots/updraft_flag_array_wthresh{w_thresh}.h5' \n",
    "            elif type=='cloudyupdraft':\n",
    "                output_file = dir+f'tracking_algorithms/plots/cloudyupdraft_flag_array_wthresh{w_thresh}.h5' \n",
    "            else:\n",
    "                raise ValueError('Invalid type: {}. Please use \"cloudy\", \"updraft\", or \"cloudyupdraft\".'.format(type))\n",
    "        \n",
    "            with h5py.File(output_file, 'w') as f:\n",
    "                f.create_dataset('flag_array', data=flag_array, compression=\"gzip\")\n",
    "            del flag_array\n",
    "            emptylike=True\n",
    "            print('done')\n",
    "        \n",
    "        ############################################################\n",
    "        for t in np.arange(len(data['time'])):\n",
    "        # for t in [33,34]: #TESTING\n",
    "            print(f'current timestep {t}')\n",
    "            initial_flag=empty_array.copy()\n",
    "            parcel_flag=empty_array.copy()\n",
    "            \n",
    "            xs=parcel['x'].isel(time=t).values\n",
    "            ys=parcel['y'].isel(time=t).values\n",
    "            zs=parcel['z'].isel(time=t).values\n",
    "\n",
    "            position=grid_location(xs,ys,zs)\n",
    "            initial_flag[position]=1\n",
    "            \n",
    "            for which_zh in np.arange(len(data['zh'])):\n",
    "                w_plane=data['w'].isel(time=t).interp(zf=data['zh']).isel(zh=which_zh).values #xy plane of w\n",
    "                qc_plane=data['qc'].isel(time=t,zh=which_zh).values #xy plane of qc\n",
    "                qi_plane=data['qi'].isel(time=t,zh=which_zh).values #xy plane of qi\n",
    "                qc_plus_qi_plane=qc_plane+qi_plane\n",
    "                \n",
    "                if type=='cloudy':\n",
    "                    cloudy_where=np.where((initial_flag[which_zh] == 1) & (qc_plane>=qc_thresh)) \n",
    "                    parcel_flag[which_zh][cloudy_where]=1\n",
    "                elif type=='updraft':\n",
    "                    updraft_where=np.where((initial_flag[which_zh] == 1) & (w_plane>=w_thresh))\n",
    "                    parcel_flag[which_zh][updraft_where]=1\n",
    "                elif type=='cloudyupdraft':\n",
    "                    cloudyupdraft_where=np.where((initial_flag[which_zh] == 1) & (w_plane>=w_thresh) & (qc_plus_qi_plane>=qcqi_thresh))\n",
    "                    parcel_flag[which_zh][cloudyupdraft_where]=1\n",
    "                         \n",
    "            # flag_array[t]=parcel_flag #OLD\n",
    "            with h5py.File(output_file, 'a') as f:\n",
    "                f['flag_array'][t] = parcel_flag\n",
    "            del parcel_flag\n",
    "            ############################################################\n",
    "\n",
    "make_flag_array(type='cloudy')\n",
    "make_flag_array(type='updraft')\n",
    "make_flag_array(type='cloudyupdraft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97dc2bf-b454-4e8c-a4f0-9132f7e24506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019e68a7-e96c-4fcb-8172-8daf9e5043e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "#Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89828438-e069-4c2e-bf8b-ddd2d8c51938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_search_2d(array): #2-D Shape Searching Algorithm (based on breadth-first search) \n",
    "    array=array.copy()\n",
    "    final_array=np.zeros_like(array)\n",
    "    #Functions \n",
    "    def next_most_1_2d(array): #finds next most 1\n",
    "        if len(np.where(array==1)[0])!=0:\n",
    "            result=tuple(arr[0] for arr in np.where(array==1))\n",
    "            return result\n",
    "        else:\n",
    "            return (-1,-1)\n",
    "    ######################################################################   \n",
    "    def find_neighbor_index_2d(array,container): #find the indexes of all neighbors\n",
    "                    \n",
    "        #neighbors that are 0 or exist in visit or queue are not appended to neighbors\n",
    "        tup,neighbors=[],[]\n",
    "        container=[tuple(sub) for sub in container]; \n",
    "        nx,ny=array.shape[1]-1,array.shape[0]-1\n",
    "        \n",
    "        for index in container:\n",
    "            if index==(-1,-1):\n",
    "                pass\n",
    "            else: \n",
    "                ######################################################################   \n",
    "                #code for inside #all possible neighbors\n",
    "                tup=[[index[0]-1,index[1]-1],[index[0]-1,index[1]],[index[0]-1,index[1]+1], \n",
    "                    [index[0],index[1]-1],[index[0],index[1]+1],\n",
    "                    [index[0]+1,index[1]-1],[index[0]+1,index[1]],[index[0]+1,index[1]+1]] \n",
    "                #code for edges and corners \n",
    "                tup=[[np.mod(sublist[0],ny+1),sublist[1]] for sublist in tup] #use if y boundaries are periodic\n",
    "                # tup=[sublist[0],sublist[1],[np.mod(sublist[1],nx+1)] for sublist in tup] #use if x boundaries are periodic\n",
    "                tup=[sublist for sublist in tup if 0 <= sublist[1] <= nx] #use if x not periodic \n",
    "                if tup: [neighbors.append(sub) for sub in tup]\n",
    "                ######################################################################             \n",
    "            if neighbors:  #only unique elements that are not visited\n",
    "                neighbors=np.unique(neighbors,axis=0).tolist()\n",
    "                neighbors=[tuple(sub) for sub in neighbors if array[tuple(sub)] != 0 and tuple(sub) not in container]\n",
    "\n",
    "        if not neighbors:\n",
    "            neighbors=np.full((0,2),0,dtype=int)    \n",
    "        else: neighbors=np.array(neighbors)\n",
    "        return neighbors\n",
    "    ###################################################################### \n",
    "    \n",
    "    #The Algorithm\n",
    "    n,k=0,0 #n is current shape, k is number of iterations on current shape \n",
    "    while True:\n",
    "        n+=1;\n",
    "        # if np.mod(n,5)==0:\n",
    "        #     print(\"Current shape number: \" + str(n))\n",
    "        visit=np.full((0,2),0,dtype=int) #creates visit variable for current shapes\n",
    "        queue=np.full((0,2),0,dtype=int) #creates queue variable for possible neighbors\n",
    "\n",
    "        ######################################################################\n",
    "        #finds the first 1 in the array, append to visit, and zero out\n",
    "        index=next_most_1_2d(array);\n",
    "        visit=np.concatenate([visit,[index]]);array[index]=0;\n",
    "        \n",
    "        #finds neighbors of first index and appends to queue\n",
    "        neighbors=find_neighbor_index_2d(array,visit) \n",
    "        queue=np.concatenate([queue,neighbors]) \n",
    "\n",
    "        #for rest of individual shape\n",
    "        while queue.size!=0: \n",
    "            k+=1\n",
    "            #add all queued indexes to visit and find unique,nonzero,and nonvisited neighbor\n",
    "            visit=np.concatenate([visit,queue]); neighbors=find_neighbor_index_2d(array,queue) \n",
    "            #zero out queued in array and empty out queue\n",
    "            array[queue[:, 0], queue[:, 1]] = 0; queue=np.full((0,2),0,dtype=int) \n",
    "            #append found neighbor to queue\n",
    "            queue=np.concatenate([queue,neighbors])\n",
    "                \n",
    "            #failsafes to end while loop\n",
    "            if queue.size==0: \n",
    "                break #breaks single loop for current shape if no more neighbor\n",
    "            if k>=array.shape[0]*array.shape[1]: #k failsafe, set to max volume of array\n",
    "                break    \n",
    "        #sets output array's value to shape number\n",
    "        if index!=(-1,-1):\n",
    "            final_array[visit[:, 0], visit[:, 1]] = n #sets final array location to shape number\n",
    "        ######################################################################  \n",
    "        \n",
    "        #failsafes to end while loop\n",
    "        if index==(-1,-1):\n",
    "            # print(\"no more shapes are left\")\n",
    "            # print(f\"total of {n-1} shapes were found\")\n",
    "            break  #break loop if no more 1s\n",
    "        if n>=array.shape[0]*array.shape[1]:  #n failsafe, set to max volume of array\n",
    "            print(\"failsafe reached\")\n",
    "            break  #break loop if no more 1s\n",
    "    return final_array\n",
    "##########################################################################################################################################  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90465d2f-52cf-4b0f-b28f-15dfc2b1ad73",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "#Loading Deep convective Parcels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e945625d-1e83-4650-97f8-0991c0d7c34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "# Average Cloud Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5501f3c-1f3e-4871-b77b-0376bd72be37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import parcel_flag_array\n",
    "w_thresh=1\n",
    "import h5py\n",
    "# input_file = dir+'tracking_algorithms/plots/parcel_flag_array.h5' \n",
    "# with h5py.File(input_file, 'r') as f:\n",
    "#     parcel_flag_array = np.array(f['flag_array'])\n",
    "\n",
    "#import cloudy, updraft, or updraft_cloudy flag_array\n",
    "input_file = dir+f'tracking_algorithms/plots/cloudy_flag_array.h5' \n",
    "with h5py.File(input_file, 'r') as f:\n",
    "    cloudy_flag_array = np.array(f['flag_array'])\n",
    "\n",
    "input_file = dir+f'tracking_algorithms/plots/updraft_flag_array_wthresh{w_thresh}.h5' \n",
    "with h5py.File(input_file, 'r') as f:\n",
    "    updraft_flag_array = np.array(f['flag_array'])\n",
    "\n",
    "input_file = dir+f'tracking_algorithms/plots/cloudyupdraft_flag_array_wthresh{w_thresh}.h5' \n",
    "with h5py.File(input_file, 'r') as f:\n",
    "    cloudyupdraft_flag_array = np.array(f['flag_array'])\n",
    "\n",
    "zerowthresh=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508d4319-2f31-43c5-afef-0ffb7d736f3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d39d09-de45-4de6-a87a-94d947c1c427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TESTING TESTING ***\n",
    "# test=cloudy_flag_array[33]==updraft_flag_array[33]\n",
    "# test=cloudy_flag_array==updraft_flag_array\n",
    "# percent=np.where(test==False)[0].shape[0]*100/(test.shape[0]*test.shape[1]*test.shape[2]*test.shape[3])\n",
    "# print(f'{percent}%')\n",
    "\n",
    "# test=cloudyupdraft_flag_array[33]==updraft_flag_array[33]\n",
    "# test=cloudyupdraft_flag_array==updraft_flag_array\n",
    "# percent=np.where(test==False)[0].shape[0]*100/(test.shape[0]*test.shape[1]*test.shape[2]*test.shape[3])\n",
    "# print(f'{percent}%')\n",
    "\n",
    "# test=cloudyupdraft_flag_array[33]==cloudy_flag_array[33]\n",
    "# test=cloudyupdraft_flag_array==cloudy_flag_array\n",
    "# percent=np.where(test==False)[0].shape[0]*100/(test.shape[0]*test.shape[1]*test.shape[2]*test.shape[3])\n",
    "# print(f'{percent}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322b3d65-6a99-4cd4-bbb3-d392c6870fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Job Array\n",
    "\n",
    "num_jobs=30 #how many total jobs are being run? i.e. array=1-100 ==> num_jobs=100 #***\n",
    "\n",
    "job_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0)) #this is the current SBATCH job id\n",
    "if job_id==0: job_id=1\n",
    "num_parcels=len(data['time']) #total number of parcels\n",
    "job_range = num_parcels//num_jobs #number of parcels per job \n",
    "\n",
    "# Calculate start and end based on job_id\n",
    "start_job = (job_id - 1) * job_range\n",
    "end_job = start_job + job_range\n",
    "if job_id==num_jobs: end_job=num_parcels-1\n",
    "print(f'running for timesteps {start_job}-{end_job-1}')\n",
    "\n",
    "data=data.isel(time=slice(start_job,end_job))\n",
    "# index_adjust = int(data['time'][0].values/1e9/60)\n",
    "parcel=parcel.isel(time=slice(start_job,end_job))\n",
    "\n",
    "parcel_flag_array=parcel_flag_array[slice(start_job,end_job)]\n",
    "cloudy_flag_array=cloudy_flag_array[slice(start_job,end_job)]\n",
    "updraft_flag_array=updraft_flag_array[slice(start_job,end_job)]\n",
    "cloudyupdraft_flag_array=cloudyupdraft_flag_array[slice(start_job,end_job)]\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d782cb9-2d07-4b0c-b8e2-56908a7169a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Collected data of averaged BFS clouds for all clouds, updrafts, and cloudy updrafts (need to change the code up to make it a 2d histogram)\n",
    "types=['cloudy','updraft','cloudyupdraft']\n",
    "\n",
    "for type in types:\n",
    "    print(f'current type: {type}')\n",
    "    # if type=='cloudyupdraft': break #TESTING\n",
    "    \n",
    "    if type=='cloudy':\n",
    "        flag_array=cloudy_flag_array.copy()\n",
    "    elif type=='updraft':\n",
    "        flag_array=updraft_flag_array.copy()\n",
    "    elif type=='cloudyupdraft':\n",
    "        flag_array=cloudyupdraft_flag_array.copy()\n",
    "    \n",
    "    def all_clouds_single_layer(t,which_zh,cloudy_bfs,vars):\n",
    "        #appends the means of each individual cloud into a list\n",
    "        max_ind=int(np.nanmax(cloudy_bfs))\n",
    "        for ind in np.arange(1,max_ind+1):\n",
    "            position=np.where(cloudy_bfs==ind)\n",
    "    \n",
    "            if np.any(position)==True:\n",
    "                for variable in vars:\n",
    "                    if variable=='w':\n",
    "                        var_cloud_data=data[variable].isel(time=t).interp(zf=data['zh']).isel(zh=which_zh)[position].values\n",
    "                    else:\n",
    "                        var_cloud_data=data[variable].isel(time=t).isel(zh=which_zh)[position].values\n",
    "                    if variable=='qv' or variable=='qc':\n",
    "                        var_cloud_data*=1000\n",
    "                        \n",
    "                    var_cloud_data=np.mean(var_cloud_data)\n",
    "                    globals()[f\"{variable}_values\"].append(var_cloud_data)\n",
    "                    if variable=='w': #only perform once\n",
    "                        zlevels.append(which_zh)\n",
    "        return (globals()[f\"{variable}_values\"] for variable in vars), zlevels\n",
    "    \n",
    "    def average_clouds(t,vars):\n",
    "        global zlevels\n",
    "        parcel_flag=flag_array[t]\n",
    "        for which_zh in np.arange(len(data['zh'])):\n",
    "            if np.mod(which_zh,5)==0: print(f'currently working on zlevel: {which_zh}')\n",
    "            flag_plane=parcel_flag[which_zh]\n",
    "            cloudy_bfs=shape_search_2d(flag_plane)\n",
    "            (w_values, qv_values, qc_values, th_values),zlevels=all_clouds_single_layer(t,which_zh,cloudy_bfs,vars) #outputs list of cloud averages\n",
    "        return (globals()[f\"{variable}_values\"] for variable in vars), zlevels\n",
    "            \n",
    "    vars=['w','qv','qc','th']\n",
    "    for variable in vars:\n",
    "        globals()[f\"{variable}_values\"] = []\n",
    "        zlevels=[]\n",
    "    for t in np.arange(len(data['time'])):\n",
    "    # for t in [33,34,35]: #TESTING\n",
    "        if np.mod(t,1)==0: print(f'current time: {t}')\n",
    "        (w_values, qv_values, qc_values, th_values),zlevels=average_clouds(t,vars) #already includes all variables\n",
    "\n",
    "\n",
    "    output_file=dir+f'tracking_algorithms/plots/h5/cloud_BFS_{type}_{job_id}_wthresh{w_thresh}.h5'\n",
    "    with h5py.File(output_file, 'w') as hdf: \n",
    "        hdf.create_dataset('w_values', data=w_values)\n",
    "        hdf.create_dataset('qv_values', data=qv_values)\n",
    "        hdf.create_dataset('qc_values', data=qc_values)\n",
    "        hdf.create_dataset('th_values', data=th_values)\n",
    "        hdf.create_dataset('zlevels', data=zlevels)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb92218-f0c9-415a-bf7b-9541b959d44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "#DEEP PROFILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c3a0d0-7d7b-413e-8c42-dad7687e93aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import parcel_flag_array\n",
    "w_thresh=1\n",
    "import h5py\n",
    "# input_file = dir+'tracking_algorithms/plots/parcel_flag_array.h5' \n",
    "# with h5py.File(input_file, 'r') as f:\n",
    "#     parcel_flag_array = np.array(f['flag_array'])\n",
    "\n",
    "#import all, cloudy, updraft, or updraft_cloudy flag_array\n",
    "input_file = dir+f'tracking_algorithms/plots/DEEP_all_flag_array.h5' \n",
    "with h5py.File(input_file, 'r') as f:\n",
    "    deep_all_parcel_flag_array = np.array(f['flag_array'])\n",
    "    \n",
    "input_file = dir+f'tracking_algorithms/plots/DEEP_cloudy_flag_array_wthresh{w_thresh}.h5' \n",
    "with h5py.File(input_file, 'r') as f:\n",
    "    cloudy_flag_array = np.array(f['flag_array'])\n",
    "\n",
    "input_file = dir+f'tracking_algorithms/plots/DEEP_updraft_flag_array_wthresh{w_thresh}.h5' \n",
    "with h5py.File(input_file, 'r') as f:\n",
    "    updraft_flag_array = np.array(f['flag_array'])\n",
    "\n",
    "input_file = dir+f'tracking_algorithms/plots/DEEP_cloudyupdraft_flag_array_wthresh{w_thresh}.h5' \n",
    "with h5py.File(input_file, 'r') as f:\n",
    "    cloudyupdraft_flag_array = np.array(f['flag_array'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39529883-a24c-4713-b514-b2ec36c5062c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Collected data of averaged BFS clouds for all clouds, updrafts, and cloudy updrafts (need to change the code up to make it a 2d histogram)\n",
    "\n",
    "#DEEP PARCEL TESTING\n",
    "\n",
    "types=['all','cloudy','updraft','cloudyupdraft']\n",
    "\n",
    "for type in types:\n",
    "    print(f'current type: {type}')\n",
    "\n",
    "    if type=='all': #NEED THRESHOLDED DEEP FLAG ARRAY\n",
    "        flag_array=deep_all_parcel_flag_array.copy()\n",
    "    if type=='cloudy': #NEED THRESHOLDED DEEP FLAG ARRAY\n",
    "        flag_array=cloudy_flag_array.copy()\n",
    "    elif type=='updraft':\n",
    "        flag_array=updraft_flag_array.copy()\n",
    "    elif type=='cloudyupdraft':\n",
    "        flag_array=cloudyupdraft_flag_array.copy()\n",
    "    \n",
    "    def all_clouds_single_layer(t,which_zh,cloudy_bfs,vars):\n",
    "        #appends the means of each individual cloud into a list\n",
    "        max_ind=int(np.nanmax(cloudy_bfs))\n",
    "        for ind in np.arange(1,max_ind+1):\n",
    "            position=np.where(cloudy_bfs==ind)\n",
    "    \n",
    "            if np.any(position)==True:\n",
    "                for variable in vars:\n",
    "                    if variable=='w':\n",
    "                        var_cloud_data=data[variable].isel(time=t).interp(zf=data['zh']).isel(zh=which_zh)[position].values\n",
    "                    else:\n",
    "                        var_cloud_data=data[variable].isel(time=t).isel(zh=which_zh)[position].values\n",
    "                    if variable=='qv' or variable=='qc':\n",
    "                        var_cloud_data*=1000\n",
    "                        \n",
    "                    var_cloud_data=np.mean(var_cloud_data)\n",
    "                    globals()[f\"{variable}_values\"].append(var_cloud_data)\n",
    "                    if variable=='w': #only perform once\n",
    "                        zlevels.append(which_zh)\n",
    "        return (globals()[f\"{variable}_values\"] for variable in vars), zlevels\n",
    "    \n",
    "    def average_clouds(t,vars):\n",
    "        global zlevels\n",
    "        parcel_flag=flag_array[t]\n",
    "        for which_zh in np.arange(len(data['zh'])):\n",
    "            if np.mod(which_zh,5)==0: print(f'currently working on zlevel: {which_zh}')\n",
    "            flag_plane=parcel_flag[which_zh]\n",
    "            cloudy_bfs=shape_search_2d(flag_plane)\n",
    "            (w_values, qv_values, qc_values, th_values),zlevels=all_clouds_single_layer(t,which_zh,cloudy_bfs,vars) #outputs list of cloud averages\n",
    "        return (globals()[f\"{variable}_values\"] for variable in vars), zlevels\n",
    "            \n",
    "    vars=['w','qv','qc','th']\n",
    "    for variable in vars:\n",
    "        globals()[f\"{variable}_values\"] = []\n",
    "        zlevels=[]\n",
    "    for t in np.arange(len(data['time'])):\n",
    "    # for t in [33,34,35]: #TESTING\n",
    "        if np.mod(t,1)==0: print(f'current time: {t}')\n",
    "        (w_values, qv_values, qc_values, th_values),zlevels=average_clouds(t,vars) #already includes all variables\n",
    "\n",
    "\n",
    "    if type=='all':\n",
    "        output_file=dir+f'tracking_algorithms/plots/h5/DEEP_cloud_BFS_{type}.h5'\n",
    "    else:\n",
    "        output_file=dir+f'tracking_algorithms/plots/h5/DEEP_cloud_BFS_{type}_wthresh{w_thresh}.h5'\n",
    "    with h5py.File(output_file, 'w') as hdf: \n",
    "        hdf.create_dataset('w_values', data=w_values)\n",
    "        hdf.create_dataset('qv_values', data=qv_values)\n",
    "        hdf.create_dataset('qc_values', data=qc_values)\n",
    "        hdf.create_dataset('th_values', data=th_values)\n",
    "        hdf.create_dataset('zlevels', data=zlevels)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e887ae1c-9bb9-46b7-ae06-40460f6f192e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9df983-a3ec-413d-99d8-1ea9079dca62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fae7ca-852f-4ab6-97bb-9449de1e314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "#Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eca595-2ff0-473e-8a74-dfa41cc1f79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_thresh=1\n",
    "\n",
    "#reading data back in \n",
    "def readdata(type,job_id):\n",
    "    with h5py.File(dir+f'tracking_algorithms/plots/h5/cloud_BFS_{type}_{job_id}_wthresh{w_thresh}.h5', 'r') as hdf:\n",
    "    # with h5py.File(dir+f'tracking_algorithms/plots/h5/cloud_BFS_{type}_{job_id}w>=0.h5', 'r') as hdf: #TESTING\n",
    "        w_values = hdf['w_values'][:]\n",
    "        qv_values = hdf['qv_values'][:]\n",
    "        qc_values = hdf['qc_values'][:]\n",
    "        th_values = hdf['th_values'][:]\n",
    "        zlevels = hdf['zlevels'][:]\n",
    "    zlevels=zlevels.astype(int)\n",
    "    return w_values,qv_values,qc_values,th_values,zlevels\n",
    "    \n",
    "\n",
    "type='cloudy'\n",
    "type='updraft'\n",
    "type='cloudyupdraft'\n",
    "num_jobs=30\n",
    "job_id=1; [w_values,qv_values,qc_values,th_values,zlevels]=readdata(type,job_id)\n",
    "vars = list(readdata(type, job_id))  # Store in a list\n",
    "\n",
    "for job_id in range(2, num_jobs + 1):\n",
    "    vars2 = list(readdata(type, job_id)) # Read the next job's data\n",
    "    for i in range(len(vars)):\n",
    "        vars[i] = np.concatenate((vars[i], vars2[i]))  # Update list elements\n",
    "w_values, qv_values, qc_values, th_values, zlevels = vars\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34933e7-d732-46f8-895c-ea912c00de25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram2d_colors():\n",
    "    colors_rgb = [\n",
    "        [44, 87, 169],    # Blue 1\n",
    "        [54, 92, 171],    # Blue 2\n",
    "        [66, 97, 175],    # Blue 3\n",
    "        [67, 107, 182],   # Blue 4\n",
    "        [70, 117, 187],   # Blue 5\n",
    "        [70, 125, 193],   # Blue 6\n",
    "        [74, 140, 204],   # Blue 7\n",
    "        [76, 157, 216],   # Blue 8\n",
    "        [75, 178, 232],   # Blue 9\n",
    "        [78, 192, 242],   # Blue 10\n",
    "        [110, 201, 242],   # Blue 11\n",
    "        [140, 207, 240],   # Blue 12\n",
    "        [161, 219, 245],   # Blue 13\n",
    "        [190, 226, 226],   # Blue 14\n",
    "    \n",
    "        [40, 150, 40],    # Green 1\n",
    "        [60, 160, 60],    # Green 2\n",
    "        [80, 180, 80],    # Green 3\n",
    "        [120, 200, 120],  # Green 4\n",
    "        \n",
    "        # [225, 237, 178],   # Yellow 1\n",
    "        [244, 239, 125],   # Yellow 2\n",
    "        [247, 232, 75],   # Yellow 3\n",
    "        [249, 213, 43],   # Yellow 4\n",
    "        [247, 194, 14],   # Orange 1\n",
    "        [245, 178, 15],   # Orange 2\n",
    "        [242, 159, 19],   # Orange 4\n",
    "        [238, 141, 22],   # Orange 5\n",
    "        [236, 121, 26],   # Orange 6\n",
    "        \n",
    "        [230, 77, 34],   # Red 1\n",
    "        [229, 51, 35],   # Red 2\n",
    "        [228, 37, 30],   # Red 3\n",
    "        [220, 29, 32],   # Red 4\n",
    "        [205, 28, 32],   # Red 5\n",
    "        \n",
    "    ]\n",
    "    colors = [[c / 255 for c in color] for color in colors_rgb]\n",
    "    \n",
    "    # #TESTING\n",
    "    # custom_cmap = mcolors.ListedColormap(colors)\n",
    "    \n",
    "    # # Create an array to display the colorbar\n",
    "    # data = np.linspace(0, 1, len(colors)).reshape(1, -1)\n",
    "    \n",
    "    # # Plotting the colorbar\n",
    "    # fig, ax = plt.subplots(figsize=(10, 2))\n",
    "    # cbar = ax.imshow(data, cmap=custom_cmap, aspect='auto')\n",
    "    # ax.set_axis_off() \n",
    "    return colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ac6d89-9978-47a2-a56d-2bf3efb5b3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #TESTING\n",
    "\n",
    "# print(f\"{len(np.where(qc_values>0.5)[0])*100/len(np.where(qc_values>-1000)[0])} %\")\n",
    "\n",
    "# # import numpy as np\n",
    "# # import matplotlib.pyplot as plt\n",
    "# # from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "\n",
    "# # # Sample data for bin_array (replace with your actual data)\n",
    "# # # bin_array = np.random.rand(10, 10) * 0.03  # Example data\n",
    "\n",
    "# # # Define a custom colormap\n",
    "# # colors = plt.cm.viridis(np.linspace(0, 1, 256))  # Use the 'viridis' colormap\n",
    "# # colors[0] = [1, 1, 1, 1]  # Set the first color (for 0) to white\n",
    "# # custom_cmap = ListedColormap(colors)\n",
    "\n",
    "# # # Create a boundary norm\n",
    "# # boundaries = np.linspace(0, 0.03, 256)  # Adjust as needed\n",
    "# # norm = BoundaryNorm(boundaries, ncolors=len(boundaries)-1)\n",
    "\n",
    "# # # Create the contour plot\n",
    "# # contour = plt.contourf(bin_array, levels=boundaries, cmap=custom_cmap, norm=norm)\n",
    "# # plt.colorbar(contour)\n",
    "\n",
    "# # plt.title(\"Filled Contour Plot with 0 as White\")\n",
    "# # plt.xlabel(\"X-axis Label\")\n",
    "# # plt.ylabel(\"Y-axis Label\")\n",
    "# # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cba5e57-42b2-4452-9618-008b23b8f217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TESTING TESTING **********\n",
    "# from matplotlib.colors import ListedColormap\n",
    "# from matplotlib.colors import BoundaryNorm\n",
    "# import seaborn as sns\n",
    "# fig = plt.figure(figsize=(15, 10),constrained_layout=True)\n",
    "# gs = gridspec.GridSpec(1, 1, figure=fig,)\n",
    "\n",
    "# axis = fig.add_subplot(gs[0])  # Create subplot in GridSpec\n",
    "\n",
    "# var='qv' #TESTING***\n",
    "\n",
    "# ####################################################\n",
    "# #Get Data\n",
    "# units={'w':'(m/s)','qv':'(g/kg)','qc':'(g/kg)','th':'(K)'}\n",
    "# values = globals()[f\"{var}_values\"]\n",
    "# Nz = len(data['zf']) if var == 'w' else len(data['zh'])\n",
    "\n",
    "# ####################### new\n",
    "# #standardized perturbations for qv and th\n",
    "# if var in ['qv','th']: #TESTING\n",
    "#     means=[]\n",
    "#     for zlevel in np.arange(np.nanmax(zlevels)+1):\n",
    "#         zlevelval=values[np.where(zlevels==zlevel)]\n",
    "#         if zlevelval.size > 0:  # Check if zlevelval is not empty\n",
    "#                 zlevelmean = np.mean(zlevelval)\n",
    "#                 zlevelstd = np.std(zlevelval)\n",
    "#         else:\n",
    "#             zlevelmean = 0  # Or handle it differently if needed\n",
    "#             zlevelstd = 1\n",
    "\n",
    "#         values[np.where(zlevels==zlevel)]-=zlevelmean\n",
    "#         values[np.where(zlevels==zlevel)]/=zlevelstd\n",
    "# ####################### new\n",
    "\n",
    "# #Split data into groups\n",
    "# if var=='w':\n",
    "#     percentiles=np.round(np.arange(np.nanmin(values),np.nanmax(values),0.05),1)\n",
    "# if var=='qv':\n",
    "#     percentiles=np.round(np.arange(np.nanmin(values),np.nanmax(values),0.01),2)\n",
    "#     if type=='updraft' and w_thresh==0:\n",
    "#         percentiles=np.round(np.arange(np.nanmin(values),np.nanmax(values),0.1),2)\n",
    "# if var=='qc':\n",
    "#     percentiles=np.round(np.arange(np.nanmin(values),np.nanmax(values),0.01),2)\n",
    "# if var=='th':\n",
    "#     percentiles=np.round(np.arange(np.nanmin(values),np.nanmax(values),0.05),1)\n",
    "#     if type=='cloudy':\n",
    "#         percentiles=np.round(np.arange(np.nanmin(values),np.nanmax(values),0.02),1)\n",
    "#     elif type=='updraft':\n",
    "#         percentiles=np.round(np.arange(np.nanmin(values),np.nanmax(values),0.1),1)\n",
    "# # percentiles = np.percentile(values, np.arange(0, 101, 4)) #TESTING PERCENTILES\n",
    "\n",
    "# #Group the data based on percentiles\n",
    "# groups = np.digitize(values, percentiles, 'left')\n",
    "# bin_array = np.zeros((Nz, len(percentiles)+1))\n",
    "# for zlevel, group in zip(zlevels, groups):\n",
    "#     bin_array[zlevel, group] += 1\n",
    "\n",
    "# #Normalization\n",
    "# row_sums = bin_array.sum(axis=1, keepdims=True)\n",
    "# row_sums[row_sums == 0] = 1 #avoids nans\n",
    "# bin_array /= row_sums\n",
    "\n",
    "# ####################### new\n",
    "# #Cumulative Sum \n",
    "# bin_array=np.cumsum(bin_array, axis=1)\n",
    "# bin_array[bin_array >= 0.99] = 0 #nanning out those >0.95\n",
    "# ####################### \n",
    "\n",
    "# #Plotting and Colorbar\n",
    "# ########################################################################################\n",
    "# ####################### \n",
    "# #custom discretized colorbar\n",
    "# colors=histogram2d_colors()\n",
    "# custom_cmap = mcolors.ListedColormap(colors)\n",
    "# #######################\n",
    "\n",
    "# ############ new\n",
    "# bin_array[bin_array==0]=-1\n",
    "# custom_cmap.set_under(\"white\")\n",
    "# ############ new\n",
    "\n",
    "# # Plotting with the custom colormap\n",
    "# im = axis.imshow(bin_array, cmap=custom_cmap, aspect=\"auto\", vmin=0, vmax=1)\n",
    "# # im = axis.imshow(bin_array, cmap=custom_cmap, aspect=\"auto\", vmin=0, vmax=0.3) #TESTING\n",
    "\n",
    "# # Add a colorbar for reference\n",
    "# cbar = plt.colorbar(im, ax=axis, orientation='vertical', fraction=0.05, pad=0.04, shrink=0.8,label='cumulative normalized count', extend=\"min\") #new\n",
    "# #add more tick labels\n",
    "# num_colors=len(colors)\n",
    "# cbar.set_ticks(np.linspace(0, 1, num_colors+1))  \n",
    "# cbar.set_ticklabels(np.round(np.linspace(0, 1, num_colors+1), 2))\n",
    "\n",
    "# # # continuous colorbar\n",
    "# # # https://seaborn.pydata.org/tutorial/color_palettes.html\n",
    "# # cmap = sns.color_palette(\"viridis\", as_cmap=True)\n",
    "# # cmap = sns.color_palette(\"plasma\", as_cmap=True)\n",
    "# # colors = cmap(np.linspace(0, 1, 256))\n",
    "# # colors[0] = [1, 1, 1, 1]  # Set the first color to white\n",
    "# # custom_cmap = mcolors.ListedColormap(colors)\n",
    "\n",
    "# # #discretized colorbar \n",
    "# # num_colors = 10  \n",
    "# # custom_colors = custom_cmap(np.linspace(0, 1, num_colors+1))\n",
    "# # custom_cmap = mcolors.ListedColormap(custom_colors)\n",
    "\n",
    "# # old code without count normalization\n",
    "# # num_colors=int(bin_array.max())+1\n",
    "# # colors = plt.cm.coolwarm(np.linspace(0, 1, num_colors))\n",
    "# # colors[0] = [1, 1, 1, 1]  # Set first color to white (RGBA)\n",
    "# # discrete_cmap = ListedColormap(colors)\n",
    "# # _skip = 1 if num_colors < 25  else num_colors // 25\n",
    "# # norm = BoundaryNorm(np.arange(-_skip/2, num_colors - _skip/2, _skip), num_colors)\n",
    "# # im = axis.imshow(bin_array, cmap=discrete_cmap, norm=norm, aspect=\"auto\")\n",
    "# # unique_counts = np.arange(0, bin_array.max()-_skip/2,_skip).astype(int)\n",
    "# # cbar.set_ticks(unique_counts)  # Ensure ticks are at integer values\n",
    "# # cbar.set_ticklabels(unique_counts.astype(int))\n",
    "# # cbar.minorticks_off()\n",
    "# ########################################################################################\n",
    "\n",
    "# ########################################################################################\n",
    "# # Add labels and title\n",
    "# axis.set_title(f\"2D Histogram for {var}\")\n",
    "# axis.set_xlabel(f\"{var} {units[var]}\")\n",
    "# if var in ['qv','th']:\n",
    "#     axis.set_title(f\"2D Histogram for {var}'_norm\")\n",
    "#     axis.set_xlabel(f\"{var}'_norm {units[var]}\")\n",
    "# axis.set_ylabel('z (km)')\n",
    "\n",
    "# # Set y-ticks and labels properly\n",
    "# axis.set_yticks(np.arange(Nz))\n",
    "# new_ytick_labels = np.round(data['zf'].values[:Nz], 2)\n",
    "# axis.set_yticklabels(new_ytick_labels, fontsize=9, rotation=0)\n",
    "\n",
    "# # Set x-ticks\n",
    "# if var=='w':\n",
    "#     tick_indices = np.arange(0, len(percentiles), 20)  # Adjust step as needed\n",
    "# if var=='qv':\n",
    "#     tick_indices = np.arange(0, len(percentiles), 30) \n",
    "#     if type=='cloudyupdraft' and w_thresh==0:\n",
    "#         tick_indices = np.arange(0, len(percentiles), 45) \n",
    "#     if type=='updraft' and w_thresh==0:\n",
    "#         tick_indices = np.arange(0, len(percentiles), 20) \n",
    "# if var=='qc':\n",
    "#     tick_indices = np.arange(0, len(percentiles), 10) \n",
    "# if var=='th':\n",
    "#     tick_indices = np.arange(0, len(percentiles), 10) \n",
    "#     if w_thresh==0:\n",
    "#         tick_indices = np.arange(0, len(percentiles), 15) \n",
    "#     if type=='updraft' and w_thresh==0:\n",
    "#         tick_indices = np.arange(0, len(percentiles), 25) \n",
    "# axis.set_xticks(tick_indices)\n",
    "# axis.set_xticklabels(percentiles[tick_indices], fontsize=8, rotation=0)\n",
    "\n",
    "\n",
    "# # Add a grid\n",
    "# # axis.xaxis.set_major_locator(plt.MultipleLocator(1))  # Show grid for every tick\n",
    "# axis.grid(visible=True, axis='x', color='black', linestyle='--', linewidth=0.5)\n",
    "# axis.grid(visible=True, axis='y', color='black', linestyle='--', linewidth=0.5)\n",
    "# axis.invert_yaxis()\n",
    "\n",
    "\n",
    "# # old set x-ticks\n",
    "# # axis.set_xticks(np.arange(len(percentiles)))\n",
    "# # axis.set_xticklabels(np.arange(len(percentiles)), fontsize=9, rotation=0)\n",
    "# # current_xticks = axis.get_xticks()\n",
    "# # new_xtick_labels = rounded_percentiles\n",
    "# # axis.set_xticklabels(new_xtick_labels, fontsize=9, rotation=0)\n",
    "\n",
    "\n",
    "# # #secondary x axis for values at each percentile group\n",
    "# # axis2=axis.secondary_xaxis(location='top')\n",
    "# # axis2.set_xticks(np.arange(len(percentiles)))\n",
    "# # axis2.set_xticklabels(np.arange(len(percentiles)), fontsize=9, rotation=0)\n",
    "# # current_xticks = axis2.get_xticks()\n",
    "# # new_xtick_labels = rounded_percentiles\n",
    "# # axis2.set_xticklabels(new_xtick_labels, fontsize=9, rotation=0)\n",
    "# ########################################################################################\n",
    "\n",
    "\n",
    "\n",
    "# # ########################################################################################\n",
    "# # # Add labels and title\n",
    "# # axis.set_title(f'2D Histogram for {var}')\n",
    "# # axis.set_xlabel(f'Percentile Group of {var} (%)')\n",
    "# # axis.set_ylabel('z (km)')\n",
    "\n",
    "# # # Set x-ticks\n",
    "# # axis.set_xticks(np.arange(len(percentiles)))\n",
    "# # axis.set_xticklabels(np.arange(len(percentiles)), fontsize=9, rotation=0)\n",
    "# # current_xticks = axis.get_xticks()\n",
    "# # new_xtick_labels = current_xticks * 4\n",
    "# # axis.set_xticklabels(new_xtick_labels, fontsize=9, rotation=0)\n",
    "\n",
    "# # # Set y-ticks and labels properly\n",
    "# # axis.set_yticks(np.arange(Nz))\n",
    "# # new_ytick_labels = np.round(data['zf'].values[:Nz], 2)\n",
    "# # axis.set_yticklabels(new_ytick_labels, fontsize=9, rotation=0)\n",
    "\n",
    "# # #secondary x axis for values at each percentile group\n",
    "# # axis2=axis.secondary_xaxis(location='top')\n",
    "# # axis2.set_xticks(np.arange(len(percentiles)))\n",
    "# # axis2.set_xticklabels(np.arange(len(percentiles)), fontsize=9, rotation=0)\n",
    "# # current_xticks = axis2.get_xticks()\n",
    "# # new_xtick_labels = rounded_percentiles\n",
    "# # axis2.set_xticklabels(new_xtick_labels, fontsize=9, rotation=0)\n",
    "\n",
    "# # # Add a grid\n",
    "# # axis.grid(visible=True, axis='x', color='black', linestyle='--', linewidth=0.5)\n",
    "# # axis.grid(visible=True, axis='y', color='black', linestyle='--', linewidth=0.5)\n",
    "# # axis.invert_yaxis()\n",
    "# # ########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54acbb49-e1b2-49ef-9ca0-7fa3567c94cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def twod_histogram(var, axis):\n",
    "    #Get Data\n",
    "    units={'w':'(m/s)','qv':'(g/kg)','qc':'(g/kg)','th':'(K)'}\n",
    "    values = globals()[f\"{var}_values\"]\n",
    "    Nz = len(data['zf']) if var == 'w' else len(data['zh'])\n",
    "    \n",
    "    ####################### new\n",
    "    #standardized perturbations for qv and th\n",
    "    if var in ['qv','th']: #TESTING\n",
    "        means=[]\n",
    "        for zlevel in np.arange(np.nanmax(zlevels)+1):\n",
    "            zlevelval=values[np.where(zlevels==zlevel)]\n",
    "            if zlevelval.size > 0:  # Check if zlevelval is not empty\n",
    "                    zlevelmean = np.mean(zlevelval)\n",
    "                    zlevelstd = np.std(zlevelval)\n",
    "            else:\n",
    "                zlevelmean = 0  # Or handle it differently if needed\n",
    "                zlevelstd = 1\n",
    "    \n",
    "            values[np.where(zlevels==zlevel)]-=zlevelmean\n",
    "            values[np.where(zlevels==zlevel)]/=zlevelstd\n",
    "    ####################### new\n",
    "    \n",
    "    #Split data into groups\n",
    "    if var=='w':\n",
    "        percentiles=np.round(np.arange(np.nanmin(values),np.nanmax(values),0.05),1)\n",
    "    if var=='qv':\n",
    "        percentiles=np.round(np.arange(np.nanmin(values),np.nanmax(values),0.01),2)\n",
    "        if type=='updraft' and w_thresh==0:\n",
    "            percentiles=np.round(np.arange(np.nanmin(values),np.nanmax(values),0.1),2)\n",
    "    if var=='qc':\n",
    "        percentiles=np.round(np.arange(np.nanmin(values),np.nanmax(values),0.01),2)\n",
    "    if var=='th':\n",
    "        percentiles=np.round(np.arange(np.nanmin(values),np.nanmax(values),0.05),1)\n",
    "        if type=='cloudy':\n",
    "            percentiles=np.round(np.arange(np.nanmin(values),np.nanmax(values),0.02),1)\n",
    "        elif type=='updraft':\n",
    "            percentiles=np.round(np.arange(np.nanmin(values),np.nanmax(values),0.1),1)\n",
    "    # percentiles = np.percentile(values, np.arange(0, 101, 4)) #TESTING PERCENTILES\n",
    "    \n",
    "    #Group the data based on percentiles\n",
    "    groups = np.digitize(values, percentiles, 'left')\n",
    "    bin_array = np.zeros((Nz, len(percentiles)+1))\n",
    "    for zlevel, group in zip(zlevels, groups):\n",
    "        bin_array[zlevel, group] += 1\n",
    "    \n",
    "    #Normalization\n",
    "    row_sums = bin_array.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0] = 1 #avoids nans\n",
    "    bin_array /= row_sums\n",
    "    \n",
    "    ####################### new\n",
    "    #Cumulative Sum \n",
    "    bin_array=np.cumsum(bin_array, axis=1)\n",
    "    bin_array[bin_array >= 0.99] = 0 #nanning out those >0.95\n",
    "    ####################### \n",
    "\n",
    "    #Plotting and Colorbar\n",
    "    ########################################################################################\n",
    "    ####################### new\n",
    "    #custom discretized colorbar\n",
    "    colors=histogram2d_colors()\n",
    "    custom_cmap = mcolors.ListedColormap(colors)\n",
    "    #######################\n",
    "\n",
    "    ############ new\n",
    "    bin_array[bin_array==0]=-1\n",
    "    custom_cmap.set_under(\"white\")\n",
    "    ############ new\n",
    "    \n",
    "    # Plotting with the custom colormap\n",
    "    im = axis.imshow(bin_array, cmap=custom_cmap, aspect=\"auto\", vmin=0, vmax=1)\n",
    "    \n",
    "    # Add a colorbar for reference\n",
    "    cbar = plt.colorbar(im, ax=axis, orientation='vertical', fraction=0.05, pad=0.04, shrink=0.8,label='cumulative normalized count', extend=\"min\") #new\n",
    "    #add more tick labels\n",
    "    num_colors=len(colors)\n",
    "    cbar.set_ticks(np.linspace(0, 1, num_colors+1))  \n",
    "    cbar.set_ticklabels(np.round(np.linspace(0, 1, num_colors+1), 2))\n",
    "    ########################################################################################\n",
    "\n",
    "    \n",
    "    #Labels and Ticks\n",
    "    ########################################################################################\n",
    "    \n",
    "    # Add labels and title\n",
    "    axis.set_title(f\"2D Histogram for {var}\")\n",
    "    axis.set_xlabel(f\"{var} {units[var]}\")\n",
    "    if var in ['qv','th']:\n",
    "        axis.set_title(f\"2D Histogram for {var}'_norm\")\n",
    "        axis.set_xlabel(f\"{var}'_norm {units[var]}\")\n",
    "    if var in ['qc']:\n",
    "        axis.set_title(f\"2D Histogram for qc+qi'\")\n",
    "        axis.set_xlabel(f\"qc+qi {units[var]}\")\n",
    "    axis.set_ylabel('z (km)')\n",
    "\n",
    "    \n",
    "    # Set y-ticks and labels properly\n",
    "    axis.set_yticks(np.arange(Nz))\n",
    "    new_ytick_labels = np.round(data['zf'].values[:Nz], 2)\n",
    "    axis.set_yticklabels(new_ytick_labels, fontsize=8, rotation=0)\n",
    "    \n",
    "    # Set x-ticks\n",
    "    if var=='w':\n",
    "        tick_indices = np.arange(0, len(percentiles), 20)  # Adjust step as needed\n",
    "    if var=='qv':\n",
    "        tick_indices = np.arange(0, len(percentiles), 30) \n",
    "        if type=='cloudyupdraft' and w_thresh==0:\n",
    "            tick_indices = np.arange(0, len(percentiles), 45) \n",
    "        if type=='updraft' and w_thresh==0:\n",
    "            tick_indices = np.arange(0, len(percentiles), 20) \n",
    "    if var=='qc':\n",
    "        tick_indices = np.arange(0, len(percentiles), 10) \n",
    "    if var=='th':\n",
    "        tick_indices = np.arange(0, len(percentiles), 10) \n",
    "        if w_thresh==0:\n",
    "            tick_indices = np.arange(0, len(percentiles), 15) \n",
    "        if type=='updraft' and w_thresh==0:\n",
    "            tick_indices = np.arange(0, len(percentiles), 25) \n",
    "    axis.set_xticks(tick_indices)\n",
    "    axis.set_xticklabels(percentiles[tick_indices], fontsize=6, rotation=0)\n",
    "    \n",
    "    # Add a grid\n",
    "    # axis.xaxis.set_major_locator(plt.MultipleLocator(1))  # Show grid for every tick\n",
    "    axis.grid(visible=True, axis='x', color='black', linestyle='--', linewidth=0.5)\n",
    "    axis.grid(visible=True, axis='y', color='black', linestyle='--', linewidth=0.5)\n",
    "    axis.invert_yaxis()\n",
    "    \n",
    "\n",
    "    ########################################################################################\n",
    "\n",
    "#Running\n",
    "####################################################################################\n",
    "fig = plt.figure(figsize=(30, 20),constrained_layout=True)\n",
    "gs = gridspec.GridSpec(2, 2, figure=fig,)\n",
    "\n",
    "if type=='cloudy':\n",
    "    fig.suptitle(f'2d Histogram of Averaged BFS Clouds (qc+qi>1e-6)')\n",
    "if type=='updraft':\n",
    "    fig.suptitle(f'2d Histogram of Averaged BFS Updrafts (w>{w_thresh})')\n",
    "if type=='cloudyupdraft':\n",
    "    fig.suptitle(f'2d Histogram of Averaged BFS Cloudy Updrafts (w>{w_thresh}) & (qc+qi>1e-6)')\n",
    "\n",
    "vars = ['w', 'qv', 'qc', 'th']\n",
    "# vars= ['w'] #TESTING\n",
    "for i, var in enumerate(vars):\n",
    "    ax = fig.add_subplot(gs[i])  # Create subplot in GridSpec\n",
    "    twod_histogram(var, ax)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#saving plot #uncomment later\n",
    "if type=='cloudy':\n",
    "    plt.savefig(dir+f'tracking_algorithms/plots/'+f'cloudy_2D_histogram_wthresh{w_thresh}.jpg', dpi=300)\n",
    "elif type=='updraft':\n",
    "    plt.savefig(dir+f'tracking_algorithms/plots/'+f'updraft_2D_histogram_wthresh{w_thresh}.jpg', dpi=300)\n",
    "elif type=='cloudyupdraft':\n",
    "    plt.savefig(dir+f'tracking_algorithms/plots/'+f'cloudyupdraft_2D_histogram_wthresh{w_thresh}.jpg', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4069dac-4833-4d4f-a66e-32c6e5335b08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7f3a52-16c7-4446-97ef-d66df90e5f49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d98743c-9db8-474a-ad0a-8de348a41871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e451b11e-6396-4b77-a77d-daba95d91b34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fee7a6-4087-49ad-9f8d-d14c79ccc1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "w_thresh=1\n",
    "\n",
    "for type in ['all','cloudy','updraft','cloudyupdraft']:\n",
    "    \n",
    "    input_file=dir+f'tracking_algorithms/plots/h5/DEEP_cloud_BFS_{type}_wthresh{w_thresh}.h5'\n",
    "    if type=='all':\n",
    "        input_file=dir+f'tracking_algorithms/plots/h5/DEEP_cloud_BFS_{type}.h5'\n",
    "    with h5py.File(input_file, 'r') as hdf: #TESTING\n",
    "            w_values = hdf['w_values'][:]\n",
    "            qv_values = hdf['qv_values'][:]\n",
    "            qc_values = hdf['qc_values'][:]\n",
    "            th_values = hdf['th_values'][:]\n",
    "            zlevels = hdf['zlevels'][:]\n",
    "            zlevels=zlevels.astype(int)\n",
    "    \n",
    "    def twod_histogram(var, axis):\n",
    "        #Get Data\n",
    "        units={'w':'(m/s)','qv':'(g/kg)','qc':'(g/kg)','th':'(K)'}\n",
    "        values = globals()[f\"{var}_values\"]\n",
    "        Nz = len(data['zf']) if var == 'w' else len(data['zh'])\n",
    "        \n",
    "        ####################### new\n",
    "        #standardized perturbations for qv and th\n",
    "        if var in ['qv','th']: #TESTING\n",
    "            means=[]\n",
    "            for zlevel in np.arange(np.nanmax(zlevels)+1):\n",
    "                zlevelval=values[np.where(zlevels==zlevel)]\n",
    "                if zlevelval.size > 0:  # Check if zlevelval is not empty\n",
    "                    zlevelmean = np.mean(zlevelval)\n",
    "                    zlevelstd = np.std(zlevelval)\n",
    "                else:\n",
    "                    zlevelmean = 0  # Or handle it differently if needed\n",
    "                    zlevelstd = 1\n",
    "                \n",
    "                values[np.where(zlevels==zlevel)]-=zlevelmean\n",
    "                # if np.isnan(values[np.where(zlevels==zlevel)]).item()!=True:\n",
    "                values[np.where(zlevels==zlevel)]/=zlevelstd\n",
    "\n",
    "                    \n",
    "        ####################### new\n",
    "        \n",
    "        #Split data into groups\n",
    "        if var=='w':\n",
    "            percentiles=np.round(np.arange(np.nanmin(values),np.nanmax(values),0.05),1)\n",
    "        if var=='qv':\n",
    "            percentiles=np.round(np.arange(np.nanmin(values),np.nanmax(values),0.01),2)\n",
    "        if var=='qc':\n",
    "            percentiles=np.round(np.arange(np.nanmin(values),np.nanmax(values),0.01),2)\n",
    "        if var=='th':\n",
    "            percentiles=np.round(np.arange(np.nanmin(values),np.nanmax(values),0.01),1)\n",
    "        # percentiles = np.percentile(values, np.arange(0, 101, 4)) #TESTING PERCENTILES\n",
    "        \n",
    "        #Group the data based on percentiles\n",
    "        groups = np.digitize(values, percentiles, 'left')\n",
    "        bin_array = np.zeros((Nz, len(percentiles)+1))\n",
    "        for zlevel, group in zip(zlevels, groups):\n",
    "            bin_array[zlevel, group] += 1\n",
    "        \n",
    "        #Normalization\n",
    "        row_sums = bin_array.sum(axis=1, keepdims=True)\n",
    "        row_sums[row_sums == 0] = 1 #avoids nans\n",
    "        bin_array /= row_sums\n",
    "        \n",
    "        ####################### new\n",
    "        #Cumulative Sum \n",
    "        bin_array=np.cumsum(bin_array, axis=1)\n",
    "        bin_array[bin_array >= 0.99] = 0 #nanning out those >0.95\n",
    "        ####################### \n",
    "    \n",
    "        #Plotting and Colorbar\n",
    "        ########################################################################################\n",
    "        ####################### new\n",
    "        #custom discretized colorbar\n",
    "        colors=histogram2d_colors()\n",
    "        custom_cmap = mcolors.ListedColormap(colors)\n",
    "        #######################\n",
    "    \n",
    "        ############ new\n",
    "        bin_array[bin_array==0]=-1\n",
    "        custom_cmap.set_under(\"white\")\n",
    "        ############ new\n",
    "        \n",
    "        # Plotting with the custom colormap\n",
    "        im = axis.imshow(bin_array, cmap=custom_cmap, aspect=\"auto\", vmin=0, vmax=1)\n",
    "        # im = axis.imshow(bin_array, cmap=custom_cmap, aspect=\"auto\", vmin=0, vmax=0.3) #TESTING\n",
    "        \n",
    "        # Add a colorbar for reference\n",
    "        cbar = plt.colorbar(im, ax=axis, orientation='vertical', fraction=0.05, pad=0.04, shrink=0.8,label='cumulative normalized count', extend=\"min\") #new\n",
    "        #add more tick labels\n",
    "        num_colors=len(colors)\n",
    "        cbar.set_ticks(np.linspace(0, 1, num_colors+1))  \n",
    "        cbar.set_ticklabels(np.round(np.linspace(0, 1, num_colors+1), 2))\n",
    "        ########################################################################################\n",
    "    \n",
    "        \n",
    "        #Labels and Ticks\n",
    "        ########################################################################################\n",
    "        \n",
    "        # Add labels and title\n",
    "        axis.set_title(f\"2D Histogram for {var}\")\n",
    "        axis.set_xlabel(f\"{var} {units[var]}\")\n",
    "        if var in ['qv','th']:\n",
    "            axis.set_title(f\"2D Histogram for {var}'_norm\")\n",
    "            axis.set_xlabel(f\"{var}'_norm {units[var]}\")\n",
    "        axis.set_ylabel('z (km)')\n",
    "    \n",
    "        \n",
    "        # Set y-ticks and labels properly\n",
    "        axis.set_yticks(np.arange(Nz))\n",
    "        new_ytick_labels = np.round(data['zf'].values[:Nz], 2)\n",
    "        axis.set_yticklabels(new_ytick_labels, fontsize=8, rotation=0)\n",
    "        \n",
    "        # Set x-ticks\n",
    "        if var=='w':\n",
    "            tick_indices = np.arange(0, len(percentiles), 20)  # Adjust step as needed\n",
    "        if var=='qv':\n",
    "            tick_indices = np.arange(0, len(percentiles), 15) \n",
    "            if type=='updraft':\n",
    "                tick_indices = np.arange(0, len(percentiles), 20) \n",
    "        if var=='qc':\n",
    "            tick_indices = np.arange(0, len(percentiles), 10) \n",
    "        if var=='th':\n",
    "            tick_indices = np.arange(0, len(percentiles), 10) \n",
    "        axis.set_xticks(tick_indices)\n",
    "        axis.set_xticklabels(percentiles[tick_indices], fontsize=6, rotation=0)\n",
    "        \n",
    "        # Add a grid\n",
    "        # axis.xaxis.set_major_locator(plt.MultipleLocator(1))  # Show grid for every tick\n",
    "        axis.grid(visible=True, axis='x', color='black', linestyle='--', linewidth=0.5)\n",
    "        axis.grid(visible=True, axis='y', color='black', linestyle='--', linewidth=0.5)\n",
    "        axis.invert_yaxis()\n",
    "        \n",
    "    \n",
    "        ########################################################################################\n",
    "    \n",
    "    #Running\n",
    "    ####################################################################################\n",
    "    fig = plt.figure(figsize=(30, 20),constrained_layout=True)\n",
    "    gs = gridspec.GridSpec(2, 2, figure=fig,)\n",
    "    \n",
    "    if type=='all':\n",
    "        fig.suptitle(f'DEEP 2d Histogram of Averaged BFS Clouds (all)')\n",
    "    if type=='cloudy':\n",
    "        fig.suptitle(f'DEEP 2d Histogram of Averaged BFS Clouds (qc>1e-6)')\n",
    "    if type=='updraft':\n",
    "        fig.suptitle(f'DEEP 2d Histogram of Averaged BFS Updrafts (w>{w_thresh})')\n",
    "    if type=='cloudyupdraft':\n",
    "        fig.suptitle(f'DEEP 2d Histogram of Averaged BFS Cloudy Updrafts (w>{w_thresh}) & (qc+qi>1e-6)')\n",
    "    \n",
    "    vars = ['w', 'qv', 'qc', 'th']\n",
    "    # vars= ['w'] #TESTING\n",
    "    for i, var in enumerate(vars):\n",
    "        ax = fig.add_subplot(gs[i])  # Create subplot in GridSpec\n",
    "        twod_histogram(var, ax)\n",
    "    \n",
    "    #saving plot #uncomment later\n",
    "    if type=='all':\n",
    "        plt.savefig(dir+f'tracking_algorithms/plots/'+f'DEEP_all_2D_histogram.jpg', dpi=300)\n",
    "    if type=='cloudy':\n",
    "        plt.savefig(dir+f'tracking_algorithms/plots/'+f'DEEP_cloudy_2D_histogram_wthresh{w_thresh}.jpg', dpi=300)\n",
    "    elif type=='updraft':\n",
    "        plt.savefig(dir+f'tracking_algorithms/plots/'+f'DEEP_updraft_2D_histogram_wthresh{w_thresh}.jpg', dpi=300)\n",
    "    elif type=='cloudyupdraft':\n",
    "        plt.savefig(dir+'tracking_algorithms/plots/'+f'DEEP_cloudyupdraft_2D_histogram_wthresh{w_thresh}.jpg', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7d1e85-3765-4d82-b09b-4d520f8f0221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccb3f40-4224-40bc-8976-3e9a3ca3fb56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133b5deb-20a1-4742-b084-1336b32ac5b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5e6b35-22dd-4996-a3d1-d7f04e6b840d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a7452a-9c96-439c-a0db-be00c611304d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df8c408-c72e-4810-b49c-bed153964496",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "#3D BFS Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceb56c1-f452-4781-876b-9c6129a6e795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_search_3d(array): #3-D Shape Searching Algorithm (based on breadth-first search) \n",
    "    final_array=np.zeros_like(array); total_1s = len(np.where(array==1)[0]) \n",
    "    #Functions \n",
    "    def next_most_1_3d(array): #finds next most 1\n",
    "        if len(np.where(array==1)[0])!=0:\n",
    "            result=tuple(arr[0] for arr in np.where(array==1))\n",
    "            return result\n",
    "        else:\n",
    "            return (-1,-1,-1)\n",
    "    ######################################################################   \n",
    "    def find_neighbor_index_3d(array,container): #find the indexes of all neighbors\n",
    "                    \n",
    "        #neighbors that are 0 or exist in visit or queue are not appended to neighbors\n",
    "        tup,neighbors=[],[]\n",
    "        \n",
    "        container=[tuple(sub) for sub in container]; \n",
    "        nx,ny,nz=array.shape[2]-1,array.shape[1]-1,array.shape[0]-1\n",
    "        \n",
    "        for index in container:\n",
    "            if index==(-1,-1,-1):\n",
    "                pass\n",
    "            else:\n",
    "                ######################################################################  \n",
    "                #code for inside #all possible neighbors\n",
    "                tup=[[index[0]-1,index[1]-1,index[2]-1],[index[0]-1,index[1]-1,index[2]],[index[0]-1,index[1]-1,index[2]+1], \n",
    "                     [index[0]-1,index[1],index[2]-1],[index[0]-1,index[1],index[2]],[index[0]-1,index[1],index[2]+1],\n",
    "                     [index[0]-1,index[1]+1,index[2]-1],[index[0]-1,index[1]+1,index[2]],[index[0]-1,index[1]+1,index[2]+1],\n",
    "\n",
    "                     [index[0],index[1]-1,index[2]-1],[index[0],index[1]-1,index[2]],[index[0],index[1]-1,index[2]+1],\n",
    "                     [index[0],index[1],index[2]-1],[index[0],index[1],index[2]],[index[0],index[1],index[2]+1],\n",
    "                     [index[0],index[1]+1,index[2]-1],[index[0],index[1]+1,index[2]],[index[0],index[1]+1,index[2]+1],\n",
    "\n",
    "                     [index[0]+1,index[1]-1,index[2]-1],[index[0]+1,index[1]-1,index[2]],[index[0]+1,index[1]-1,index[2]+1],\n",
    "                     [index[0]+1,index[1],index[2]-1],[index[0]+1,index[1],index[2]],[index[0]+1,index[1],index[2]+1],\n",
    "                     [index[0]+1,index[1]+1,index[2]-1],[index[0]+1,index[1]+1,index[2]],[index[0]+1,index[1]+1,index[2]+1],  \n",
    "                    ]  \n",
    "                #code for edges and corners \n",
    "                tup=[sublist for sublist in tup if 0 <= sublist[0] <= nz] \n",
    "                tup=[[sublist[0],np.mod(sublist[1],ny+1),sublist[2]] for sublist in tup] #use if y boundaries are periodic\n",
    "                # tup=[sublist[0],sublist[1],[np.mod(sublist[2],nx+1)] for sublist in tup] #use if x boundaries are periodic\n",
    "                tup=[sublist for sublist in tup if 0 <= sublist[2] <= nx] #use if x not periodic\n",
    "                if tup: [neighbors.append(sub) for sub in tup]\n",
    "                ######################################################################   \n",
    "            if neighbors: #only if neighbors exists\n",
    "                neighbors=np.unique(neighbors,axis=0).tolist()\n",
    "                neighbors=[tuple(sub) for sub in neighbors if array[tuple(sub)] != 0 and tuple(sub) not in container] \n",
    "        \n",
    "        if not neighbors:\n",
    "            neighbors=np.full((0,3),0,dtype=int)    \n",
    "        else: neighbors=np.array(neighbors)\n",
    "        return neighbors\n",
    "    ###################################################################### \n",
    "    \n",
    "    #The Algorithm\n",
    "    n,k=0,0 #n is current shape, k is number of iterations on current shape \n",
    "    \n",
    "    while True:\n",
    "        n+=1;\n",
    "        if np.mod(n,100)==0:\n",
    "             found_1s=len(np.where(array==1)[0]); print(\"Current shape number: \" + str(n) + \" (\" + str(round(100*(1-found_1s/total_1s),2)) + \"%)\")\n",
    "        visit=np.full((0,3),0,dtype=int) #creates visit variable for current shapes\n",
    "        queue=np.full((0,3),0,dtype=int) #creates queue variable for possible neighbors\n",
    "\n",
    "        ######################################################################\n",
    "        #finds the first 1 in the array, append to visit, and zero out\n",
    "        index=next_most_1_3d(array); \n",
    "        visit=np.concatenate([visit,[index]]);array[index]=0;\n",
    "\n",
    "        #finds neighbors of first index and appends to queue\n",
    "        neighbors=find_neighbor_index_3d(array,visit) \n",
    "        queue=np.concatenate([queue,neighbors])\n",
    "        \n",
    "        #for rest of individual shape\n",
    "        while queue.size!=0: \n",
    "            k+=1\n",
    "            #add all queued indexes to visit and find unique,nonzero,and nonvisited neighbor\n",
    "            visit=np.concatenate([visit,queue]);neighbors=find_neighbor_index_3d(array,queue) \n",
    "            #zero out queued in array and empty out queue\n",
    "            array[queue[:,0],queue[:,1],queue[:,2]]=0; queue=np.full((0,3),0,dtype=int) \n",
    "            #append found neighbor to queue\n",
    "            queue=np.concatenate([queue,neighbors]) #append additional neighbors to queue\n",
    "            \n",
    "            #failsafes to end while loop    \n",
    "            if queue.size==0: \n",
    "                break #breaks single loop for current shape if no more neighbor\n",
    "            if k>=array.shape[0]*array.shape[1]*array.shape[2]: #k failsafe, set to max volume of array\n",
    "                break   \n",
    "        #sets output array's value to shape number\n",
    "        if index!=(-1,-1,-1):\n",
    "            final_array[visit[:,0],visit[:,1],visit[:,2]] = n #sets final array location to shape number\n",
    "        ######################################################################  \n",
    "        \n",
    "        #failsafes to end while loop    \n",
    "        if index==(-1,-1,-1):\n",
    "            print(\"no more shapes are left\")\n",
    "            print(f\"total of {n-1} shapes were found\")\n",
    "            break  #break loop if no more 1s\n",
    "        if n>=array.shape[0]*array.shape[1]*array.shape[2]:  #n failsafe, set to max volume of array\n",
    "            print(\"failsafe reached\")\n",
    "            break  #break loop if no more 1s\n",
    "    #Save final array to external nc file\n",
    "    xr.DataArray(final_array, dims=('z','y', 'x'),name='var').to_netcdf('cloud_shapes_3d.nc')\n",
    "    return final_array #15s/1t==>35m/140t\n",
    "##########################################################################################################################################  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1522228-6b6f-46e7-b945-b1bf18cce2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binned_scatter3d(values,zh,axis):\n",
    "    num_groups = 10\n",
    "    min_val, max_val = np.nanmin(values), np.nanmax(values)\n",
    "    bins = np.linspace(min_val, max_val, num_groups + 1)\n",
    "    group_indices = np.digitize(values, bins, right=False) - 1  # subtract 1 to make it zero-indexed\n",
    "    # colors = cm.seismic(np.linspace(0, 1, num_groups))\n",
    "    colors = cm.Spectral(np.linspace(0, 1, num_groups))\n",
    "    \n",
    "    # ####### Optional\n",
    "    # original_cmap = cm.Spectral\n",
    "    \n",
    "    # # Function to darken higher values\n",
    "    # def darken_colormap(cmap, factor):\n",
    "    #     # Extract the RGB values from the colormap\n",
    "    #     colors = cmap(np.linspace(0, 1, 256))\n",
    "    #     # Darken the higher colors by scaling the RGB values\n",
    "    #     colors[:, :3] *= np.linspace(1, 1-factor, 256)[:, np.newaxis]\n",
    "    #     # Ensure alpha values are set to 1 (fully opaque)\n",
    "    #     colors[:, -1] = 1.0\n",
    "    #     # Create a new colormap from the modified colors\n",
    "    #     return mcolors.ListedColormap(colors)\n",
    "    \n",
    "    # # Create a darkened colormap\n",
    "    # darkened_cmap = darken_colormap(original_cmap, factor=0.2) #what percent to darken the colors\n",
    "    \n",
    "    # # Generate colors for the number of groups\n",
    "    # colors = darkened_cmap(np.linspace(0, 1, num_groups))\n",
    "    # ####### \n",
    "    \n",
    "    for i in range(num_groups):\n",
    "        mask = group_indices == i\n",
    "        # axis.scatter(np.array(values)[mask], zh[mask], color=colors[i],alpha=1,label=f'> {bins[i]:.2f}')\n",
    "        axis.scatter(np.array(values)[mask], np.array(zh)[mask], color=colors[i],alpha=1,label=f'> {bins[i]:.2f}')\n",
    "    axis.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6541ed-3be1-44c4-afa3-cf6c2217587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_clouds_single_layer(t,cloudy_bfs,variable):\n",
    "    #appends the means of each individual cloud into a list\n",
    "    max_ind=int(np.nanmax(cloudy_bfs))\n",
    "    for ind in np.arange(1,max_ind+1):\n",
    "        if np.mod(ind,100)==0: print(f'currently working cloud # {ind}/{max_ind}')\n",
    "        position=np.where(cloudy_bfs==ind)\n",
    "    \n",
    "        if np.any(position)==True:\n",
    "            if variable=='w':\n",
    "                var_cloud_data=data[variable].isel(time=t).interp(zf=data['zh'])[position].values\n",
    "            else:\n",
    "                var_cloud_data=data[variable].isel(time=t)[position].values\n",
    "            if variable=='qv' or variable=='qc':\n",
    "                var_cloud_data*=1000\n",
    "            var_cloud_data=np.mean(var_cloud_data)\n",
    "            clouds.append(var_cloud_data)\n",
    "\n",
    "            CTH=np.where(cloudy_bfs==ind)[0].max() #Cloud Top Height\n",
    "            CBH=np.where(cloudy_bfs==ind)[0].min()\n",
    "            # CH=CTH-CBH #Cloud Height\n",
    "            CH=data['zf'].values[CTH].item()-data['zf'].values[CBH].item() #Cloud Height\n",
    "            zlevs.append(CH)\n",
    "\n",
    "def average_clouds(t,variable):\n",
    "    global clouds,zlevs,cloudy_bfs\n",
    "    clouds,zlevs=[],[]\n",
    "    parcel_flag=cloudy_flag_array[t]\n",
    "    flag_plane=parcel_flag.copy()\n",
    "    if variable=='w':\n",
    "        cloudy_bfs=shape_search_3d(flag_plane) #only runs once the first time and then saved as global\n",
    "    all_clouds_single_layer(t,cloudy_bfs,variable)\n",
    "    return clouds,zlevs\n",
    "\n",
    "start_time = time.time();\n",
    "##############################\n",
    "\n",
    "# t=33\n",
    "# [clouds,CHs]=average_clouds(t,'w') #60s/1t==>140m/140t\n",
    "\n",
    "# # xlabel='w (m/s)'\n",
    "# fig, axs = plt.subplots(1, 1, figsize=(5, 5))\n",
    "# binned_scatter3d(clouds,CHs,axs)\n",
    "# fig.suptitle(f\"averaged cloudy parcel 3d-groups data by height\")\n",
    "# plt.ylabel('Cloud Height Grid Box');plt.xlabel(xlabel); ###  ***\n",
    "\n",
    "# ##############################\n",
    "# end_time = time.time(); elapsed_time = end_time - start_time\n",
    "# print(f\"Total Elapsed Time: {elapsed_time} seconds\")\n",
    "# print('done')\n",
    "\n",
    "\n",
    "#Plotting\n",
    "fig, axs = plt.subplots(2, 2, figsize=(8, 8))\n",
    "fig.suptitle(f\"averaged deep convective cloudy parcel groups data by height\")\n",
    "ax1, ax2, ax3, ax4 = axs.flatten()\n",
    "plt.tight_layout()\n",
    "\n",
    "start_time = time.time();\n",
    "##############################\n",
    "vars=['w','qv','qc','th'] \n",
    "xlabels=['w (m/s)','qv (g/kg)','qc (g/kg)','th (K)']\n",
    "axises=[ax1,ax2,ax3,ax4]\n",
    "# for t in np.arange(len(data['time'])):\n",
    "# for t in [11,12,13]: #TESTING\n",
    "for t in [33]: #TESTING\n",
    "    for variable,axis,xlabel in zip(vars,axises,xlabels):\n",
    "        print(f'working on {variable}')\n",
    "        [clouds,CHs]=average_clouds(t,variable) #60s/1t==>140m/140t\n",
    "        fig.subplots_adjust(wspace=0.5) \n",
    "        binned_scatter3d(clouds,CHs,axis) ###***\n",
    "        axis.set_ylabel('Cloud Height (km)');axis.set_xlabel(xlabel);\n",
    "##############################\n",
    "end_time = time.time(); elapsed_time = end_time - start_time\n",
    "print(f\"Total Elapsed Time: {elapsed_time} seconds\")\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1644b1-9100-4bd1-b7fa-3ec859c749cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ed68a9-3d00-46e4-b431-c60a7b669c44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c99e20-8b9e-40d5-b39e-30fb7c7a39b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data['zh'][CHs], bins=20, edgecolor='black')  # You can adjust the number of bins as needed\n",
    "plt.ylabel('Counts')\n",
    "plt.xlabel('Cloud Height Grid Box')\n",
    "plt.title('Histogram of Cloud Heights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8109bb4-7ed8-4803-be85-c5d4300c9119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TESTING\n",
    "# num_groups = 10\n",
    "# min_val, max_val = np.nanmin(values), np.nanmax(values)\n",
    "# bins = np.linspace(min_val, max_val, num_groups + 1)\n",
    "# group_indices = np.digitize(values, bins, right=False) - 1  # subtract 1 to make it zero-indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1f1bdd-e285-4bef-b3b7-493419a198d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # TESTING\n",
    "# #making contour matrix (does not take averages only adds last of each group currently)\n",
    "# ngroups=len(grouped_data)\n",
    "\n",
    "# maxrow=-1\n",
    "# for ind in np.arange(0,ngroups):\n",
    "#     length=len(grouped_data[ind])\n",
    "#     if length>maxrow:\n",
    "#         maxrow=length\n",
    "# matrix=np.zeros((maxrow,ngroups))\n",
    "\n",
    "# for group in np.arange(ngroups):\n",
    "#     current_group=grouped_data[group]\n",
    "#     for ind in np.arange(len(current_group)):\n",
    "#         val=current_group[ind][0]\n",
    "#         row=current_group[ind][1]\n",
    "#         matrix[row,group]=val\n",
    "\n",
    "\n",
    "# ######\n",
    "\n",
    "# matrix=matrix[0:4,:]\n",
    "# repeat_rows = 3\n",
    "# repeat_cols = 3\n",
    "# matrix = np.tile(matrix, (repeat_rows, repeat_cols))\n",
    "# ######\n",
    " \n",
    "# matrix_with_nan = np.where(matrix == 0, np.nan, matrix)\n",
    "\n",
    "# masked_matrix = np.ma.masked_invalid(matrix_with_nan)\n",
    "# cmap = plt.get_cmap('viridis').copy()\n",
    "# cmap.set_bad(\"white\")\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(6,4))\n",
    "# # contour = ax.contourf(masked_matrix, cmap=cmap)\n",
    "# contour = ax.contourf(matrix, cmap=cmap)\n",
    "# cbar = fig.colorbar(contour, ax=ax)\n",
    "# cbar.set_label('w (m/s)')\n",
    "# ax.set_ylabel('Height (CH Grid Diff)')\n",
    "# ax.set_xlabel('Group Number in Integers')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeba09a-d3ff-4643-82cc-9afe5f0ff647",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING Cloud BFS Averaging Algorithm\n",
    "# # #TESTING\n",
    "# # # one=np.array(w_values.copy())\n",
    "# # # two=np.array(qv_values.copy())\n",
    "# # # three=np.array(zlevels.copy())\n",
    "# # one2=np.array(w_values.copy())\n",
    "# # two2=np.array(qv_values.copy())\n",
    "# # three2=np.array(zlevels.copy())\n",
    "\n",
    "# types=['cloudy','updraft']\n",
    "# job_id=5\n",
    "\n",
    "# def readdata(type,job_id):\n",
    "#     with h5py.File(dir+f'tracking_algorithms/plots/h5/cloud_BFS_{type}_{job_id}.h5', 'r') as hdf:\n",
    "#         w_values = hdf['w_values'][:]\n",
    "#         qv_values = hdf['qv_values'][:]\n",
    "#         qc_values = hdf['qc_values'][:]\n",
    "#         th_values = hdf['th_values'][:]\n",
    "#         zlevels = hdf['zlevels'][:]\n",
    "#     zlevels=zlevels.astype(int)\n",
    "#     return w_values,qv_values,qc_values,th_values,zlevels\n",
    "\n",
    "\n",
    "# for type in types:\n",
    "#     [w_values,qv_values,qc_values,th_values,zlevels]=readdata(type,job_id)\n",
    "#     if type=='cloudy':\n",
    "#         one=np.array(w_values.copy())\n",
    "#         two=np.array(qv_values.copy())\n",
    "#         three=np.array(zlevels.copy())\n",
    "#     elif type=='updraft':\n",
    "#         one2=np.array(w_values.copy())\n",
    "#         two2=np.array(qv_values.copy())\n",
    "#         three2=np.array(zlevels.copy())\n",
    "\n",
    "# #TESTING TESTING *************\n",
    "# #reading data back in \n",
    "# def readdata(type,job_id):\n",
    "#     with h5py.File(dir+f'tracking_algorithms/plots/h5/cloud_BFS_{type}_{job_id}.h5', 'r') as hdf:\n",
    "#         w_values = hdf['w_values'][:]\n",
    "#         qv_values = hdf['qv_values'][:]\n",
    "#         qc_values = hdf['qc_values'][:]\n",
    "#         th_values = hdf['th_values'][:]\n",
    "#         zlevels = hdf['zlevels'][:]\n",
    "#     zlevels=zlevels.astype(int)\n",
    "#     return w_values,qv_values,qc_values,th_values,zlevels\n",
    "    \n",
    "\n",
    "# # type='cloudy'\n",
    "# # type='updraft'\n",
    "# type='cloudyupdraft'\n",
    "# num_jobs=30\n",
    "# job_id=1; [w_values,qv_values,qc_values,th_values,zlevels]=readdata(type,job_id)\n",
    "# vars = list(readdata(type, job_id))  # Store in a list\n",
    "\n",
    "# for job_id in range(2, num_jobs + 1):\n",
    "#     vars2 = list(readdata(type, job_id)) # Read the next job's data\n",
    "#     for i in range(len(vars)):\n",
    "#         vars[i] = np.concatenate((vars[i], vars2[i]))  # Update list elements\n",
    "# w_values, qv_values, qc_values, th_values, zlevels = vars\n",
    "# #************** this method has some issue\n",
    "\n",
    "\n",
    "# #TESTING TESTING *************\n",
    "# # type='cloudyupdraft'\n",
    "# # num_jobs=30\n",
    "# # job_id=4; [w_values,qv_values,qc_values,th_values,zlevels]=readdata(type,job_id)\n",
    "# # vars = list(readdata(type, job_id))  # Store in a list\n",
    "# # vars\n",
    "\n",
    "# # type='updraft'\n",
    "# # num_jobs=30\n",
    "# # job_id=4; [w_values,qv_values,qc_values,th_values,zlevels]=readdata(type,job_id)\n",
    "# # vars2 = list(readdata(type, job_id))  # Store in a list\n",
    "# # vars2\n",
    "\n",
    "# # test=vars[0]==vars2[0]\n",
    "# # plt.plot(test)\n",
    "\n",
    "# # one=w_values.copy()\n",
    "# # two=w_values.copy()\n",
    "# # three=w_values.copy()\n",
    "# test=one==two\n",
    "# plt.plot(test,color='k')\n",
    "# test=one==three\n",
    "# plt.plot(test+1,color='blue')\n",
    "# test=two==three\n",
    "# plt.plot(test+2,color='green')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa7a19b-6ecd-4f05-889f-1b4147e61606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# percents=np.arange(0, 101, 4)\n",
    "\n",
    "# for var in vars:\n",
    "#     values = globals()[f\"{var}_values\"]\n",
    "#     Nz = len(data['zf']) if var == 'w' else len(data['zh'])\n",
    "#     globals()[f\"{var}_percentile\"] = np.round(np.percentile(values, np.arange(0, 101, 4)),3)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # Ensure all arrays are of the same length\n",
    "# assert len(w_percentile) == len(percents) == len(qv_percentile)== len(qc_percentile)== len(th_percentile), \"Arrays must be of the same length.\"\n",
    "\n",
    "# # Create a figure\n",
    "# fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "# # Create key data for the table\n",
    "# key_data = np.vstack((percents, w_percentile, qv_percentile, qc_percentile, th_percentile)).T\n",
    "\n",
    "# # Create the table\n",
    "# table = ax.table(cellText=key_data, colLabels=['Percentile', 'w (m/s)', 'qv (g/kg)', 'qc (g/kg)', 'th (g/kg)'], \n",
    "#                  loc='center', cellLoc='center', colColours=['lightgrey', 'lightgrey', 'lightgrey', 'lightgrey', 'lightgrey'])\n",
    "# table.auto_set_font_size(False)\n",
    "# table.set_fontsize(10)\n",
    "# table.scale(1, 1.5)  # Adjust table height\n",
    "# ax.axis('off')\n",
    "\n",
    "\n",
    "# if type=='cloudy':\n",
    "#     fig.suptitle('Percentiles 2D Histogram of Averaged BFS Clouds', fontsize=14, y=1.4)\n",
    "# elif type=='updraft':\n",
    "#     fig.suptitle('Percentiles 2D Histogram of Averaged BFS Updrafts', fontsize=14, y=1.4)\n",
    "# elif type=='cloudyupdraft':\n",
    "#     fig.suptitle('Percentiles 2D Histogram of Averaged BFS Cloudy Updrafts', fontsize=14, y=1.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707a4c9f-a91a-42ab-ac13-6b87037817c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING\n",
    "\n",
    "# xlabels=['w (m/s)','qv (g/kg)','qc (g/kg)','th (K)']\n",
    "# values=z\n",
    "# num_groups = 25\n",
    "# min_val, max_val = np.nanmin(values), np.nanmax(values)\n",
    "# bins = np.linspace(min_val, max_val, num_groups + 1)\n",
    "# group_indices = np.digitize(values, bins, right=False) - 1  # subtract 1 to make it zero-indexed\n",
    "\n",
    "# from scipy.interpolate import griddata\n",
    "# x=group_indices\n",
    "# # Create a grid for contouring, adjusted to data ranges\n",
    "# min_x, max_x = np.nanmin(x), np.nanmax(x)\n",
    "# min_y, max_y = np.nanmin(y), np.nanmax(y)\n",
    "\n",
    "# grid_x, grid_y = np.mgrid[min_x:max_x:100j, min_y:max_y:100j]  # Create a grid for the contour\n",
    "# grid_z = griddata((x, y), z, (grid_x, grid_y), method='linear')  # Interpolate to grid\n",
    "\n",
    "# # Plotting\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.contourf(grid_x, grid_y, grid_z, levels=20, cmap='viridis')  # Create contour plot\n",
    "# plt.colorbar(label='w (m/s)')  # Optional: Add a colorbar\n",
    "# plt.title(\"Interpolated Scatter Data\")\n",
    "# plt.xlabel(\"Group Number\")\n",
    "# plt.ylabel(\"Z Level\")\n",
    "\n",
    "# plt.ylim(7,26)\n",
    "# plt.xticks(np.arange(min_x, max_x + 1, 1));\n",
    "\n",
    "# # current_y_ticks = plt.gca().get_yticks();\n",
    "# # current_y_tick_labels = plt.gca().get_yticklabels();\n",
    "# # new_y_tick_labels = [\n",
    "# #     str(round(data['zh'][int(float(label.get_text()))].values * 1, 1)) \n",
    "# #     for label in current_y_tick_labels\n",
    "# # ]\n",
    "# # # new_y_tick_labels = [\n",
    "# # #     str(round(data['zh'][int(float(label.get_text()))].values * 1000, 1)) \n",
    "# # #     for label in current_y_tick_labels\n",
    "# # # ]\n",
    "# # plt.gca().set_yticks(current_y_ticks);  # Ensure tick positions match\n",
    "# # plt.gca().set_yticklabels(new_y_tick_labels);\n",
    "\n",
    "\n",
    "# values=z\n",
    "# num_groups = 25\n",
    "# min_val, max_val = np.nanmin(values), np.nanmax(values)\n",
    "# bins = np.linspace(min_val, max_val, num_groups + 1)\n",
    "# group_indices = np.digitize(values, bins, right=False) - 1  # subtract 1 to make it zero-indexed\n",
    "\n",
    "# matrix=np.zeros((len(data['zh']),group_indices.max()+1))\n",
    "# for (group,yind,zind) in zip(group_indices,y,z):\n",
    "#     matrix[yind,group]=zind\n",
    "\n",
    "# num_levels=20\n",
    "# vmin=-5\n",
    "# vmax=15\n",
    "# # matrix = np.where(matrix == 0, np.nan, matrix)\n",
    "# plt.contourf(matrix,levels=num_levels,cmap='viridis',vmin=vmin, vmax=vmax)\n",
    "\n",
    "# # plt.xlim(0, 15)\n",
    "# # plt.ylim(5, 25)\n",
    "# plt.xlabel('group number')\n",
    "# plt.ylabel('z grid')\n",
    "# plt.colorbar(label='w (m/s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a56e583-3ce3-43a7-80cd-66beaeec46ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Function scatter plot colored by bins #not needed\n",
    "# def binned_scatter(values,zh,axis):\n",
    "#     num_groups = 10\n",
    "#     min_val, max_val = np.nanmin(values), np.nanmax(values)\n",
    "#     bins = np.linspace(min_val, max_val, num_groups + 1)\n",
    "#     group_indices = np.digitize(values, bins, right=False) - 1  # subtract 1 to make it zero-indexed\n",
    "#     # colors = cm.seismic(np.linspace(0, 1, num_groups))\n",
    "#     colors = cm.Spectral(np.linspace(0, 1, num_groups))\n",
    "    \n",
    "#     # ####### Optional\n",
    "#     # original_cmap = cm.Spectral\n",
    "    \n",
    "#     # # Function to darken higher values\n",
    "#     # def darken_colormap(cmap, factor):\n",
    "#     #     # Extract the RGB values from the colormap\n",
    "#     #     colors = cmap(np.linspace(0, 1, 256))\n",
    "#     #     # Darken the higher colors by scaling the RGB values\n",
    "#     #     colors[:, :3] *= np.linspace(1, 1-factor, 256)[:, np.newaxis]\n",
    "#     #     # Ensure alpha values are set to 1 (fully opaque)\n",
    "#     #     colors[:, -1] = 1.0\n",
    "#     #     # Create a new colormap from the modified colors\n",
    "#     #     return mcolors.ListedColormap(colors)\n",
    "    \n",
    "#     # # Create a darkened colormap\n",
    "#     # darkened_cmap = darken_colormap(original_cmap, factor=0.2) #what percent to darken the colors\n",
    "    \n",
    "#     # # Generate colors for the number of groups\n",
    "#     # colors = darkened_cmap(np.linspace(0, 1, num_groups))\n",
    "#     # ####### \n",
    "#     for i in range(num_groups):\n",
    "#         mask = group_indices == i\n",
    "#         axis.scatter(np.array(values)[mask], zh[mask], color=colors[i],alpha=1,label=f'> {bins[i]:.2f}')\n",
    "#     axis.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    " # fig.subplots_adjust(wspace=0.5) \n",
    " #        binned_scatter(clouds,data['zh'][zlevs],axis)\n",
    "\n",
    " #        #for contour plot\n",
    " #        z.extend(clouds)\n",
    " #        y.extend(zlevs)\n",
    " #        axis.set_ylabel('z (km)');axis.set_xlabel(xlabel);\n",
    "        \n",
    "\n",
    "\n",
    "# #TESTING Scatter plot of averaged values for deep convective clouds\n",
    "\n",
    "# def deep_clouds_single_layer(t,which_zh,cloudy_bfs,variable):\n",
    "#     max_ind=int(np.nanmax(cloudy_bfs))\n",
    "#     for ind in np.arange(1,max_ind+1):\n",
    "#         position=np.where(cloudy_bfs==ind)\n",
    "\n",
    "#         parcel_id=parcel_flag_array[t,which_zh][position] ######\n",
    "#         where = np.where(np.isin(out_nz[:, 0], parcel_id))\n",
    "#         if np.any(where)==True: \n",
    "#             if np.any(position)==True:\n",
    "#                 if variable=='w':\n",
    "#                     var_cloud_data=data[variable].isel(time=t).interp(zf=data['zh']).isel(zh=which_zh)[position].values\n",
    "#                 else:\n",
    "#                     var_cloud_data=data[variable].isel(time=t).isel(zh=which_zh)[position].values\n",
    "#                 if variable=='qv' or variable=='qc':\n",
    "#                     var_cloud_data*=1000\n",
    "#                 var_cloud_data=np.mean(var_cloud_data)\n",
    "#                 clouds.append(var_cloud_data)\n",
    "#                 zlevs.append(which_zh)\n",
    "        \n",
    "\n",
    "# def average_clouds(t,variable):\n",
    "#     global clouds,zlevs\n",
    "#     clouds,zlevs=[],[]\n",
    "#     parcel_flag=cloudy_flag_array[t]\n",
    "#     for which_zh in np.arange(len(data['zh'])):\n",
    "#         flag_plane=parcel_flag[which_zh]\n",
    "#         cloudy_bfs=shape_search_2d(flag_plane)\n",
    "#         deep_clouds_single_layer(t,which_zh,cloudy_bfs,variable)\n",
    "    \n",
    "#         # plt.contourf(cloudy_bfs) #TESTING\n",
    "#         # if which_zh==10: break #TESTING***\n",
    "#     return clouds,zlevs\n",
    "\n",
    "# #Plotting\n",
    "# fig, axs = plt.subplots(2, 2, figsize=(8, 8))\n",
    "# fig.suptitle(f\"averaged deep convective cloudy parcel groups data by height\")\n",
    "# ax1, ax2, ax3, ax4 = axs.flatten()\n",
    "# plt.tight_layout()\n",
    "\n",
    "# vars=['w','qv','qc','th']\n",
    "# xlabels=['w (m/s)','qv (g/kg)','qc (g/kg)','th (K)']\n",
    "# axises=[ax1,ax2,ax3,ax4]\n",
    "# # for t in np.arange(len(data['time'])):\n",
    "# # for t in [11,12,13]: #TESTING\n",
    "# for t in [11]: #TESTING\n",
    "#     for variable,axis,xlabel in zip(vars,axises,xlabels):\n",
    "#         print(f'working on {variable}')\n",
    "#         [clouds,zlevs]=average_clouds(t,variable)\n",
    "\n",
    "#         fig.subplots_adjust(wspace=0.5) \n",
    "#         binned_scatter(clouds,data['zh'][zlevs],axis)\n",
    "#         # axis.scatter(clouds,data['zh'][zlevs],color='b')\n",
    "        \n",
    "#         axis.set_ylabel('z (km)');axis.set_xlabel(xlabel);      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a776dd-8d5f-4b59-a94b-51f1460240de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TESTING\n",
    "# #If interested in connecting lagrangian BFS with neighboring non-lagrangian clouds \n",
    "\n",
    "# #Import Data\n",
    "# #####################################################################  \n",
    "# data2=np.array([[1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "#                 [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "#                 [0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0],\n",
    "#                 [0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0],\n",
    "#                 [0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0],\n",
    "#                 [1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "#                 [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "#                 [0,0,1,0,1,0,0,0,0,0,1,1,0,0,0,0,1,1,1,0,0],\n",
    "#                 [0,0,0,1,0,0,0,0,0,1,1,0,1,1,1,0,1,1,1,0,0],\n",
    "#                 [0,0,1,0,1,0,0,0,0,1,0,0,0,0,0,0,1,1,0,0,0],\n",
    "#                 [0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0],\n",
    "#                 [1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1]])  \n",
    "# array=data2.copy() #creates temporary array variable \n",
    "\n",
    "# #Running the Algorithm\n",
    "# ######################################################################  \n",
    "# start_time = time.time(); \n",
    "# final_array = shape_search_2d(array)\n",
    "# end_time = time.time(); elapsed_time = end_time - start_time\n",
    "# print(f\"Total Elapsed Time: {elapsed_time} seconds\")\n",
    "# ###################################################################### \n",
    "\n",
    "# #Plotting for Testing\n",
    "# ######################################################################  \n",
    "# plt.subplot(2, 1, 1)\n",
    "# plot_array=data2.copy(); \n",
    "# plt.imshow(plot_array,cmap='viridis')\n",
    "# plt.subplot(2, 1, 2)\n",
    "# plt.imshow(final_array,cmap='gist_ncar')\n",
    "# ######################################################################  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
