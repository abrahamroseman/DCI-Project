{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe11247d-70be-4b25-8409-3d6cce0c0f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS FUNCTION IS FOR RUNNING WITH SLURM JOB ARRAY\n",
    "#(SPLITS UP JOB_ARRAY BELOW INTO EVEN MORE TASKS)\n",
    "def StartSlurmJobArray(num_jobs,num_slurm_jobs, ISRUN):\n",
    "    job_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0)) #this is the current SBATCH job id\n",
    "    if job_id==0: job_id=1\n",
    "    if ISRUN==False:\n",
    "        start_job=1;end_job=num_jobs+1\n",
    "        return start_job,end_job\n",
    "    total_elements=num_jobs #total num of variables\n",
    "\n",
    "    job_range = total_elements // num_slurm_jobs  # Base size for each chunk\n",
    "    remaining = total_elements % num_slurm_jobs   # Number of chunks with 1 extra \n",
    "    \n",
    "    # Function to compute the start and end for each job_id\n",
    "    def get_job_range(job_id, num_slurm_jobs):\n",
    "        job_id-=1\n",
    "        # Add one extra element to the first 'remaining' chunks\n",
    "        start_job = job_id * job_range + min(job_id, remaining)\n",
    "        end_job = start_job + job_range + (1 if job_id < remaining else 0)\n",
    "    \n",
    "        if job_id == num_slurm_jobs - 1: \n",
    "            end_job = total_elements \n",
    "        return start_job, end_job\n",
    "    # def job_testing():\n",
    "    #     #TESTING\n",
    "    #     start=[];end=[]\n",
    "    #     for job_id in range(1,num_slurm_jobs+1):\n",
    "    #         start_job, end_job = get_job_range(job_id)\n",
    "    #         print(start_job,end_job)\n",
    "    #         start.append(start_job)\n",
    "    #         end.append(end_job)\n",
    "    #     print(np.all(start!=end))\n",
    "    #     print(len(np.unique(start))==len(start))\n",
    "    #     print(len(np.unique(end))==len(end))\n",
    "    # job_testing()\n",
    "    # if sbatch==True:\n",
    "        \n",
    "    start_job, end_job = get_job_range(job_id, num_slurm_jobs)\n",
    "    index_adjust=start_job\n",
    "    # print(f'start_job = {start_job}, end_job = {end_job}')\n",
    "    if start_job==0: start_job=1\n",
    "    if end_job==total_elements: end_job+=1\n",
    "    return start_job,end_job\n",
    "\n",
    "# job_id=1\n",
    "# [start_slurm_job,end_slurm_job,slurm_index_adjust]=StartSlurmJobArray(num_jobs,num_slurm_jobs,ISRUN)\n",
    "# parcel=parcel1.isel(xh=slice(start_job,end_job))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ead873a-cf3c-4145-b753-42776117eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in Packages and Data\n",
    "\n",
    "#Importing Packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import matplotlib.gridspec as gridspec\n",
    "import xarray as xr\n",
    "import os; import time\n",
    "import pickle\n",
    "import h5py\n",
    "###############################################################\n",
    "def coefs(coefficients,degree):\n",
    "    coef=coefficients\n",
    "    coefs=\"\"\n",
    "    for n in range(degree, -1, -1):\n",
    "        string=f\"({coefficients[len(coef)-(n+1)]:.1e})\"\n",
    "        coefs+=string + f\"x^{n}\"\n",
    "        if n != 0:\n",
    "            coefs+=\" + \"\n",
    "    return coefs\n",
    "###############################################################\n",
    "\n",
    "# Importing Model Data\n",
    "check=False\n",
    "dir='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "\n",
    "# dx = 1 km; Np = 1M; Nt = 5 min\n",
    "data1=xr.open_dataset(dir+'../cm1r20.3/run/cm1out_1km_5min.nc') #***\n",
    "parcel1=xr.open_dataset(dir+'../cm1r20.3/run/cm1out_pdata_1km_5min_1e6.nc') #***\n",
    "res='1km';t_res='5min'\n",
    "Np_str='1e6'\n",
    "\n",
    "# # dx = 1km; Np = 50M\n",
    "# #Importing Model Data\n",
    "# check=False\n",
    "# dir2='/home/air673/koa_scratch/'\n",
    "# data1=xr.open_dataset(dir2+'cm1out_1km_1min.nc') #***\n",
    "# parcel1=xr.open_dataset(dir2+'cm1out_pdata_1km_1min_50M.nc') #***\n",
    "# res='1km'; t_res='1min'; Np_str='50e6'\n",
    "\n",
    "# # dx = 1km; Np = 100M\n",
    "# #Importing Model Data\n",
    "# check=False\n",
    "# dir2='/home/air673/koa_scratch/'\n",
    "# data1=xr.open_dataset(dir2+'cm1out_1km_1min.nc') #***\n",
    "# parcel1=xr.open_dataset(dir2+'cm1out_pdata_1km_1min_100M.nc') #***\n",
    "# res='1km'; t_res='1min'; Np_str='100e6'\n",
    "\n",
    "# #uncomment if using 250m data\n",
    "# #Importing Model Data\n",
    "# check=False\n",
    "# dir2='/home/air673/koa_scratch/'\n",
    "# data1=xr.open_dataset(dir2+'cm1out_250m.nc') #***\n",
    "# # # parcel1=xr.open_dataset(dir2+'cm1out_pdata_250m.nc') #***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413ac9d3-0dbf-47dc-8d24-3b8d3af34dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "times=data1['time'].values/(1e9 * 60); times=times.astype(float);\n",
    "minutes=1/times[1] #1 / minutes per timestep = timesteps per minute\n",
    "kms=np.argmax(data1['xh'].values-data1['xh'][0].values >= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309327a9-a931-4dee-b35f-7b51bae815c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "dir2='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "path=dir2+'../Functions/'\n",
    "sys.path.append(path)\n",
    "\n",
    "import NumericalFunctions\n",
    "from NumericalFunctions import * # import NumericalFunctions \n",
    "import PlottingFunctions\n",
    "from PlottingFunctions import * # import PlottingFunctions\n",
    "\n",
    "\n",
    "# # Get all functions in NumericalFunctions\n",
    "# import inspect\n",
    "# functions = [f[0] for f in inspect.getmembers(NumericalFunctions, inspect.isfunction)]\n",
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaff6515-c7d8-4875-86f1-dbc073e3e4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "#SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4eb8a8-ecb3-42c6-815b-906f3f684936",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "#JOB ARRAY SETUP\n",
    "################################\n",
    "#*#*\n",
    "# how many total jobs are being run? i.e. array=1-100 ==> num_jobs=100\n",
    "num_jobs=60 #1M parcels\n",
    "# num_jobs=200 #50M parcels\n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12326467-46ec-4fa8-a61a-2eb25a6a938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "#DATA LOADING FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06198640-e69b-47b7-b71a-53bac4e4db1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JOB ARRAY SETUP\n",
    "def StartJobArray(job_id,num_jobs):\n",
    "    total_elements=len(data1['time']) #total num of variables\n",
    "\n",
    "    if num_jobs >= total_elements:\n",
    "        raise ValueError(\"Number of jobs cannot be greater than or equal to total elements.\")\n",
    "    \n",
    "    job_range = total_elements // num_jobs  # Base size for each chunk\n",
    "    remaining = total_elements % num_jobs   # Number of chunks with 1 extra \n",
    "    \n",
    "    # Function to compute the start and end for each job_id\n",
    "    def get_job_range(job_id, num_jobs):\n",
    "        job_id-=1\n",
    "        # Add one extra element to the first 'remaining' chunks\n",
    "        start_job = job_id * job_range + min(job_id, remaining)\n",
    "        end_job = start_job + job_range + (1 if job_id < remaining else 0)\n",
    "    \n",
    "        if job_id == num_jobs - 1: \n",
    "            end_job = total_elements #- 1\n",
    "        return start_job, end_job\n",
    "    # def job_testing():\n",
    "    #     #TESTING\n",
    "    #     start=[];end=[]\n",
    "    #     for job_id in range(1,num_jobs+1):\n",
    "    #         start_job, end_job = get_job_range(job_id)\n",
    "    #         print(start_job,end_job)\n",
    "    #         start.append(start_job)\n",
    "    #         end.append(end_job)\n",
    "    #     print(np.all(start!=end))\n",
    "    #     print(len(np.unique(start))==len(start))\n",
    "    #     print(len(np.unique(end))==len(end))\n",
    "    # job_testing()\n",
    "\n",
    "    # if sbatch==True:\n",
    "    #     job_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0)) #this is the current SBATCH job id\n",
    "    #     if job_id==0: job_id=1\n",
    "        \n",
    "    start_job, end_job = get_job_range(job_id, num_jobs)\n",
    "    index_adjust=start_job\n",
    "    # print(f'start_job = {start_job}, end_job = {end_job}')\n",
    "    return start_job,end_job,index_adjust\n",
    "job_id=1\n",
    "[start_job,end_job,index_adjust]=StartJobArray(job_id,num_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b978e822-2989-4841-a37a-78b85897d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Back Data Later\n",
    "##############\n",
    "\n",
    "def make_data_dict(in_file,var_names,read_type,start_job,end_job):\n",
    "    if read_type=='h5py':\n",
    "        with h5py.File(in_file, 'r') as f:\n",
    "            data_dict = {var_name: f[var_name][start_job:end_job] for var_name in var_names}\n",
    "            \n",
    "    elif read_type=='xarray':\n",
    "        in_data = xr.open_dataset(\n",
    "            in_file,\n",
    "            engine='h5netcdf',\n",
    "            phony_dims='sort',\n",
    "            chunks={'phony_dim_0': 100, 'phony_dim_1': 100_000} \n",
    "        )\n",
    "        data_dict = {k: in_data[k][start_job:end_job].compute().data for k in var_names}\n",
    "    return data_dict\n",
    "\n",
    "# read_type='xarray'\n",
    "read_type='h5py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c77d903-482a-4465-b76e-9ea1ee590fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeDictionary(**vars):\n",
    "    return vars\n",
    "    \n",
    "def GetData(start_job,end_job):\n",
    "    dir2=dir+'Project_Algorithms/Lagrangian_Arrays/'\n",
    "    in_file=dir2+f'lagrangian_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "    \n",
    "    var_names = ['W', 'QCQI']\n",
    "    data_dict = make_data_dict(in_file,var_names,read_type,start_job,end_job)\n",
    "    W, QCQI = (data_dict[k] for k in var_names)\n",
    "    \n",
    "    # #Making Time Matrix\n",
    "    # rows, cols = A.shape[0], A.shape[1]\n",
    "    # T = np.arange(rows).reshape(-1, 1) * np.ones((1, cols), dtype=int)\n",
    "    \n",
    "    dir2=dir+'Project_Algorithms/Lagrangian_Arrays/'\n",
    "    in_file=dir2+f'VARS_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "    \n",
    "    var_names = ['QV','TH','TH_E','BUOYANCY','HMC']#,'QI','QR']\n",
    "    data_dict = make_data_dict(in_file,var_names,read_type,start_job,end_job)\n",
    "    QV, TH, TH_E, BUOYANCY, HMC = [data_dict[k] for k in var_names] #, QI, QR\n",
    "\n",
    "    dir2=dir+'Project_Algorithms/Lagrangian_Arrays/'\n",
    "    in_file=dir2+f'VARS_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "    \n",
    "    var_names = ['VMF_c','VMF_g']\n",
    "    data_dict = make_data_dict(in_file,var_names,read_type,start_job,end_job)\n",
    "    VMF_c, VMF_g = [data_dict[k] for k in var_names]\n",
    "\n",
    "\n",
    "    VARs=MakeDictionary(W=W, QCQI=QCQI, QV=QV, \n",
    "                        TH=TH, TH_E=TH_E, \n",
    "                        BUOYANCY=BUOYANCY, HMC=HMC,\n",
    "                        VMF_g=VMF_g,VMF_c=VMF_c)\n",
    "    return VARs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bb6de4-45a0-40f3-8104-b7544c9c8aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetSpatialData(start_job,end_job):\n",
    "    dir2=dir+'Project_Algorithms/Lagrangian_Arrays/'\n",
    "    in_file=dir2+f'lagrangian_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "    \n",
    "    var_names = ['A_g','A_c','Z', 'Y', 'X', 'z']\n",
    "    data_dict = make_data_dict(in_file,var_names,read_type,start_job,end_job)\n",
    "    A_g,A_c, Z,Y,X,parcel_z = (data_dict[k] for k in var_names)\n",
    "    \n",
    "    # #Making Time Matrix\n",
    "    # rows, cols = A.shape[0], A.shape[1]\n",
    "    # T = np.arange(rows).reshape(-1, 1) * np.ones((1, cols), dtype=int)\n",
    "    \n",
    "    return A_g,A_c, Z,Y,X,parcel_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcafadf7-a5da-4898-8767-8314d2fbe4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DomainSubsetting(A_g,A_c):\n",
    "    #DOMAIN SUBSETTING\n",
    "    ############################################################\n",
    "    ocean_percent=2/8\n",
    "    \n",
    "    left_to_coast=data1['xh'][0]+(data1['xh'][-1]-data1['xh'][0])*ocean_percent\n",
    "    where_coast_xh=np.where(data1['xh']>=left_to_coast)[0][0]#-25\n",
    "    where_coast_xf=np.where(data1['xf']>=left_to_coast)[0][0]#-25\n",
    "    end_xh=len(data1['xh'])-1-50\n",
    "    end_xf=len(data1['xf'])-1-50\n",
    "    \n",
    "    # print(f'x in {0}:{where_coast_xh-1} FOR SEA')\n",
    "    # print(f'x in {where_coast_xh}:{end_xh} FOR LAND')\n",
    "    # t_end=78 \n",
    "    # if res=='250m':t_end=410\n",
    "    # print(f't in {0}:{t_end} (6.5 hours)')\n",
    "    if t_res==\"5min\":\n",
    "        t_start=36\n",
    "    elif t_res==\"1min\":\n",
    "        t_start=36*5\n",
    "    # print(f't in {t_start}:end (8 hours)')\n",
    "    \n",
    "    #####################################################################\n",
    "    \n",
    "    #SUBSETTING CODE\n",
    "    A_g[(X<where_coast_xh)|(X>end_xh)]=0\n",
    "    A_c[(X<where_coast_xh)|(X>end_xh)]=0\n",
    "    \n",
    "    #SUBSETTING TIME FOR JOB ARRAY\n",
    "    if end_job<=t_start:\n",
    "        A_g[0:t_start]=0\n",
    "        A_c[0:t_start]=0\n",
    "    # elif job_array==False:\n",
    "    #     A_g[0:t_start]=0\n",
    "    #     A_c[0:t_start]=0\n",
    "    return A_g,A_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1164b8e9-0d2b-463a-9aec-357604c8af2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "#MAKE PROFILES FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1a0046-2889-49fe-8cc4-3a2521e1934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir2=dir+'Project_Algorithms/Domain_Profiles/'\n",
    "\n",
    "def LagrangianProfiles_Simultaneous(VARs,Z,data_type):\n",
    "    # print(f'currently working on type {data_type}')\n",
    "\n",
    "    #MAKE PROFILE_ARRAYS TO HOLD THE RESULT\n",
    "    profile_dict = {}\n",
    "    zhs = data1['zh'].values  # outside the loop for efficiency\n",
    "    for var in VARs:\n",
    "        profile_dict[f\"profile_{var}\"]=np.zeros((len(zhs), 3))\n",
    "        profile_dict[f\"profile_{var}\"][:,2]=zhs\n",
    "\n",
    "\n",
    "    Nt=len(data['time'])\n",
    "    for t in np.arange(Nt):\n",
    "    \n",
    "        #GET THE LOCATIONS OF PARCELS SUCCEEDING THRESHOLD\n",
    "        if data_type=='general':\n",
    "            where=np.where(A_g[t]==1)\n",
    "            zs=Z[t,where[0]]\n",
    "        elif data_type=='cloudy':\n",
    "            where=np.where(A_c[t]==1)\n",
    "            zs=Z[t,where[0]]\n",
    "\n",
    "        #For Each Variable, Link profile_array to profile_{var} and Add Profile Elements to Storage Array\n",
    "        for var in VARs:\n",
    "            var_data = VARs[var][t,where[0]]\n",
    "            np.add.at(profile_dict[f\"profile_{var}\"][:, 0], zs, var_data)\n",
    "            np.add.at(profile_dict[f\"profile_{var}\"][:, 1], zs, 1)\n",
    "    return profile_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5d76e2-a375-42a8-88ff-f5ef7d7f3d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveProfile(Dictionary,data_type):\n",
    "    # print(\"Saving all profiles...\")\n",
    "\n",
    "    #OUTPUTTING RESULTS    \n",
    "    dir2=dir+'Project_Algorithms/Domain_Profiles/'\n",
    "    if data_type=='general':\n",
    "        output_file=dir2+f'job_out/general_lagrangian_profiles_{res}_{t_res}_{Np_str}'\n",
    "    elif data_type=='cloudy':\n",
    "        output_file=dir2+f'job_out/cloudy_lagrangian_profiles_{res}_{t_res}_{Np_str}'\n",
    "    output_file+=f'_{job_id}.h5'\n",
    "\n",
    "    # Write the entire dictionary to HDF5\n",
    "    with h5py.File(output_file, \"w\") as h5f:\n",
    "        for key, profile_data in Dictionary.items():\n",
    "            h5f.create_dataset(key, data=profile_data)\n",
    "            \n",
    "    # print(\"Done saving.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa32080-e0c9-452c-baea-ef961c7ec692",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "#RUNNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476f03cd-f177-4c24-83d8-c26bcd086b53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_slurm_jobs=30\n",
    "[start_slurm_job,end_slurm_job]=StartSlurmJobArray(num_jobs=num_jobs,num_slurm_jobs=num_slurm_jobs,ISRUN=False) #if ISRUN is False, then will not run using slurm_job_array\n",
    "\n",
    "print(f\"Running on Slurm_Jobs for Slurm_Job_Ids: {(start_slurm_job,end_slurm_job-1)}\")\n",
    "\n",
    "job_id_list=np.arange(start_slurm_job,end_slurm_job)\n",
    "for job_id in job_id_list:\n",
    "    # job_id=20 #TESTING\n",
    "    if job_id % 1 == 0: print(f'current job_id = {job_id}')\n",
    "    [start_job,end_job,index_adjust]=StartJobArray(job_id,num_jobs)\n",
    "\n",
    "    #SLICING DATA\n",
    "    data=data1.isel(time=slice(start_job,end_job))\n",
    "    parcel=parcel1.isel(xh=slice(start_job,end_job))\n",
    "\n",
    "    #GETTING DATA AND PUTTING IN A DICTIONARY\n",
    "    [A_g,A_c, Z,Y,X,parcel_z] = GetSpatialData(start_job,end_job) \n",
    "    [A_g,A_c]=DomainSubsetting(A_g,A_c)\n",
    "    VARs=GetData(start_job,end_job)\n",
    "\n",
    "    #CALCULATING AND SAVING\n",
    "    for data_type in ['general','cloudy']:\n",
    "        Dictionary=LagrangianProfiles_Simultaneous(VARs, Z, data_type) #VERSION2: EACH VARIABLE SIMULTANEOUSLY (RECOMMENDED)\n",
    "        SaveProfile(Dictionary,data_type)\n",
    "        del Dictionary\n",
    "    #check_memory(globals())\n",
    "    del VARs\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0247d7a5-545f-4acb-ade8-f8b11f76b2a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eace57-c1f5-427e-ab99-86c39b821afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "#RECOMBINE SEPERATE JOB_ARRAYS AFTER\n",
    "recombine=False #KEEP FALSE WHEN JOB ARRAY IS RUNNING\n",
    "recombine=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58196976-18fc-4259-90c0-4a173c66a2a0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dir2=dir+'Project_Algorithms/Domain_Profiles/'\n",
    "def GetInputFileName(data_type,job_id):\n",
    "    #CALLING IN DATA\n",
    "    if data_type == \"general\":\n",
    "        input_file = dir2+f'job_out/general_lagrangian_profiles_{res}_{t_res}_{Np_str}_{job_id}.h5' \n",
    "    elif data_type == \"cloudy\":\n",
    "        input_file = dir2+f'job_out/cloudy_lagrangian_profiles_{res}_{t_res}_{Np_str}_{job_id}.h5'\n",
    "    return input_file\n",
    "def GetOutputFileName(data_type):\n",
    "    #CALLING IN DATA\n",
    "    if data_type == \"general\":\n",
    "        output_file = dir2+f'job_out/general_lagrangian_profiles_{res}_{t_res}_{Np_str}.h5' \n",
    "    elif data_type == \"cloudy\":\n",
    "        output_file = dir2+f'job_out/cloudy_lagrangian_profiles_{res}_{t_res}_{Np_str}.h5'\n",
    "    return output_file\n",
    "def Recombine(num_jobs):\n",
    "    data_types=[\"general\",\"cloudy\"]\n",
    "    for data_type in data_types:\n",
    "        \n",
    "        #MAKING OUTPUT FILE PATH\n",
    "        output_file=GetOutputFileName(data_type)\n",
    "        \n",
    "        #MAKING PROFILES DICTIONARY\n",
    "        profiles = {}  # Store profiles for all variables\n",
    "        job_id=1; input_file=GetInputFileName(data_type,job_id)\n",
    "        with h5py.File(input_file, 'r') as f:\n",
    "            for var in f:\n",
    "                profiles[var]=f[f\"{var}\"][:]\n",
    "\n",
    "        #RECOMBINING\n",
    "        for job_id in np.arange(2,num_jobs+1):\n",
    "            if np.mod(job_id,10)==0: print(f\"job_id = {job_id}\")\n",
    "            #CALLING IN DATA and COMPILING PROFILES\n",
    "            input_file=GetInputFileName(data_type,job_id)\n",
    "            with h5py.File(input_file, 'r') as f:\n",
    "                for var in f:  \n",
    "\n",
    "                    profiles[var][:,0:1+1]+=f[f'{var}'][:,0:1+1]\n",
    "        \n",
    "        #SAVING INTO FINAL FORM\n",
    "        with h5py.File(output_file, 'w') as f:\n",
    "            for var in profiles:\n",
    "                profile_var = profiles[var]\n",
    "                f.create_dataset(f'{var}', data=profile_var, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3e861d-0c34-4363-9b5b-1f7575e0fe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "if recombine==True:\n",
    "    Recombine(num_jobs=num_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651c2df5-578c-46a1-b7e3-b2e6998d6288",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcd174b-4eb2-4e10-8e6b-d5eb4df32b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24051cdc-cc78-49ba-a203-24e939ae5a14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d18473b-ac89-4dc3-a905-0c66aadb0b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TESTING\n",
    "# def averaged_profiles(profile):\n",
    "#     out_var = profile[(profile[:, 1] != 0)]  # gets rid of rows that have no data\n",
    "#     out_var = np.array([out_var[:, 0] / out_var[:, 1], out_var[:, 2]]).T  # divides the data column by the counter column\n",
    "#     return out_var\n",
    "# job_id=20; input_file=GetInputFileName(data_type,job_id)\n",
    "# with h5py.File(input_file, 'r') as f:\n",
    "#     profile=f['profile_W'][:]\n",
    "# out=averaged_profiles(profile)\n",
    "# plt.plot(out[:,0],out[:,1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
