{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686337d8-2e00-4e03-979c-a2f2b64cfa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in Packages and Data\n",
    "\n",
    "#Importing Packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import matplotlib.gridspec as gridspec\n",
    "import xarray as xr\n",
    "import os; import time\n",
    "import pickle\n",
    "import h5py\n",
    "###############################################################\n",
    "def coefs(coefficients,degree):\n",
    "    coef=coefficients\n",
    "    coefs=\"\"\n",
    "    for n in range(degree, -1, -1):\n",
    "        string=f\"({coefficients[len(coef)-(n+1)]:.1e})\"\n",
    "        coefs+=string + f\"x^{n}\"\n",
    "        if n != 0:\n",
    "            coefs+=\" + \"\n",
    "    return coefs\n",
    "###############################################################\n",
    "\n",
    "# Importing Model Data\n",
    "check=False\n",
    "dir='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "###############################################################\n",
    "\n",
    "# # dx = 1 km; Np = 1M; Nt = 5 min\n",
    "# data=xr.open_dataset(dir+'../cm1r20.3/run/cm1out_1km_5min.nc') #***\n",
    "# parcel=xr.open_dataset(dir+'../cm1r20.3/run/cm1out_pdata_1km_5min_1e6.nc') #***\n",
    "# res='1km';t_res='5min'\n",
    "# Np_str='1e6'\n",
    "\n",
    "# dx = 1km; Np = 50M\n",
    "#Importing Model Data\n",
    "check=False\n",
    "dir2='/home/air673/koa_scratch/'\n",
    "data=xr.open_dataset(dir2+'cm1out_1km_1min.nc') #***\n",
    "parcel=xr.open_dataset(dir2+'cm1out_pdata_1km_1min_50M.nc') #***\n",
    "res='1km'; t_res='1min'; Np_str='50e6'\n",
    "\n",
    "# # dx = 1km; Np = 100M\n",
    "# #Importing Model Data\n",
    "# check=False\n",
    "# dir2='/home/air673/koa_scratch/'\n",
    "# data=xr.open_dataset(dir2+'cm1out_1km_1min.nc') #***\n",
    "# parcel=xr.open_dataset(dir2+'cm1out_pdata_1km_1min_100M.nc') #***\n",
    "# res='1km'; t_res='1min'; Np_str='100e6'\n",
    "\n",
    "# #uncomment if using 250m data\n",
    "# #Importing Model Data\n",
    "# check=False\n",
    "# dir2='/home/air673/koa_scratch/'\n",
    "# data=xr.open_dataset(dir2+'cm1out_250m.nc') #***\n",
    "# # # parcel=xr.open_dataset(dir2+'cm1out_pdata_250m.nc') #***\n",
    "\n",
    "# # Restricts the timesteps of the data from timesteps0 to 140\n",
    "# data=data.isel(time=np.arange(0,400+1))\n",
    "# # # parcel=parcel.isel(time=np.arange(0,400+1))\n",
    "# res='250m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2278b9ae-e490-422e-8614-cb0f6428a989",
   "metadata": {},
   "outputs": [],
   "source": [
    "times=data['time'].values/(1e9 * 60); times=times.astype(float);\n",
    "minutes=1/times[1] #1 / minutes per timestep = timesteps per minute\n",
    "kms=np.argmax(data['xh'].values-data['xh'][0].values >= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a91fd70-216c-453d-ae84-dcc08a1e0106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "dir2='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "path=dir2+'../Functions/'\n",
    "sys.path.append(path)\n",
    "\n",
    "import NumericalFunctions\n",
    "from NumericalFunctions import * # import NumericalFunctions \n",
    "import PlottingFunctions\n",
    "from PlottingFunctions import * # import PlottingFunctions\n",
    "\n",
    "\n",
    "# # Get all functions in NumericalFunctions\n",
    "# import inspect\n",
    "# functions = [f[0] for f in inspect.getmembers(NumericalFunctions, inspect.isfunction)]\n",
    "# functions\n",
    "\n",
    "# # Get all functions in NumericalFunctions\n",
    "# import inspect\n",
    "# functions = [f[0] for f in inspect.getmembers(PlottingFunctions, inspect.isfunction)]\n",
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c90f210-53db-47ab-b03e-616a985f9cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JOB ARRAY SETUP\n",
    "job_array=False;index_adjust=0\n",
    "job_array=True\n",
    "##############################*#*\n",
    "\n",
    "if job_array==True:\n",
    "    num_jobs=300 #how many total jobs are being run? i.e. array=1-100 ==> num_jobs=100 #***\n",
    "    total_elements=len(parcel['xh']) #total num of variables\n",
    "\n",
    "    if num_jobs >= total_elements:\n",
    "        raise ValueError(\"Number of jobs cannot be greater than or equal to total elements.\")\n",
    "    \n",
    "    job_range = total_elements // num_jobs  # Base size for each chunk\n",
    "    remaining = total_elements % num_jobs   # Number of chunks with 1 extra \n",
    "    \n",
    "    # Function to compute the start and end for each job_id\n",
    "    def get_job_range(job_id, num_jobs):\n",
    "        job_id-=1\n",
    "        # Add one extra element to the first 'remaining' chunks\n",
    "        start_job = job_id * job_range + min(job_id, remaining)\n",
    "        end_job = start_job + job_range + (1 if job_id < remaining else 0)\n",
    "    \n",
    "        if job_id == num_jobs - 1: \n",
    "            end_job = total_elements #- 1\n",
    "        return start_job, end_job\n",
    "    # def job_testing():\n",
    "    #     #TESTING\n",
    "    #     start=[];end=[]\n",
    "    #     for job_id in range(1,num_jobs+1):\n",
    "    #         start_job, end_job = get_job_range(job_id)\n",
    "    #         print(start_job,end_job)\n",
    "    #         start.append(start_job)\n",
    "    #         end.append(end_job)\n",
    "    #     print(np.all(start!=end))\n",
    "    #     print(len(np.unique(start))==len(start))\n",
    "    #     print(len(np.unique(end))==len(end))\n",
    "    # job_testing()\n",
    "    \n",
    "    job_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0)) #this is the current SBATCH job id\n",
    "    if job_id==0: job_id=3\n",
    "    start_job, end_job = get_job_range(job_id, num_jobs)\n",
    "    index_adjust=start_job\n",
    "    print(f'start_job = {start_job}, end_job = {end_job}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73802ad2-f1b4-4535-867a-90fee9e86481",
   "metadata": {},
   "outputs": [],
   "source": [
    "if job_array==True:\n",
    "    #Indexing Array with JobArray\n",
    "    parcel=parcel.isel(xh=slice(start_job,end_job))\n",
    "    #(for 150_000_000 parcels use 500-1000 jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498c4228-bc42-4478-8914-b43de8df388c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Back Data Later\n",
    "##############\n",
    "def make_data_dict(in_file,var_names,read_type):\n",
    "    if read_type=='h5py':\n",
    "        with h5py.File(in_file, 'r') as f:\n",
    "            if job_array==True:\n",
    "                data_dict = {var_name: f[var_name][:,start_job:end_job] for var_name in var_names}\n",
    "            elif job_array==False:\n",
    "                data_dict = {var_name: f[var_name][:] for var_name in var_names}\n",
    "            \n",
    "    elif read_type=='xarray':\n",
    "        in_data = xr.open_dataset(\n",
    "            in_file,\n",
    "            engine='h5netcdf',\n",
    "            phony_dims='sort',\n",
    "            chunks={'phony_dim_0': 100, 'phony_dim_1': 1_000_000} \n",
    "        )\n",
    "        if job_array==True:\n",
    "            data_dict = {k: in_data[k][:,start_job:end_job].compute().data for k in var_names}\n",
    "        elif job_array==False:\n",
    "            data_dict = {k: in_data[k][:].compute().data for k in var_names}\n",
    "    return data_dict\n",
    "\n",
    "# read_type='xarray'\n",
    "read_type='h5py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc377480-dbae-4b52-9466-8fc0cb4e008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "dir2=dir+'Project_Algorithms/Lagrangian_Arrays/'\n",
    "in_file=dir2+f'lagrangian_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "\n",
    "var_names = ['A_g', 'A_c', 'W', 'QCQI', 'Z', 'Y', 'X', 'z']\n",
    "data_dict = make_data_dict(in_file,var_names,read_type)\n",
    "A_g, A_c, W, QCQI, Z, Y, X, parcel_z = (data_dict[k] for k in var_names)\n",
    "\n",
    "# #Making Time Matrix\n",
    "# rows, cols = A.shape[0], A.shape[1]\n",
    "# T = np.arange(rows).reshape(-1, 1) * np.ones((1, cols), dtype=int)\n",
    "check_memory(globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78145720-d9d1-4faa-b6c1-d3c069d4ef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Back Data Later\n",
    "##############\n",
    "import h5py\n",
    "dir2=dir+'Project_Algorithms/Lagrangian_Arrays/'\n",
    "in_file=dir2+f'VARS_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "\n",
    "var_names = ['QV','TH','TH_E','BUOYANCY','HMC']\n",
    "data_dict = make_data_dict(in_file,var_names,read_type)\n",
    "QV, TH, TH_E, BUOYANCY, HMC = (data_dict[k] for k in var_names)\n",
    "check_memory(globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8459e060-9d7d-4e5b-bec0-f0617aa1f916",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b804e9-f03f-4459-9bc6-a7d09b0d5e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "#READING BACK IN\n",
    "def LoadFinalData(in_file):\n",
    "    dict = {}\n",
    "    with h5py.File(in_file, 'r') as f:\n",
    "        for key in f.keys():\n",
    "            dict[key] = f[key][:]\n",
    "    return dict\n",
    "\n",
    "def LoadAllCloudBase():\n",
    "    dir2 = dir + f'Project_Algorithms/Tracking_Algorithms/'\n",
    "    in_file = dir2 + f\"all_cloudbase_{res}_{t_res}_{Np_str}.pkl\"\n",
    "    with open(in_file, 'rb') as f:\n",
    "        all_cloudbase = pickle.load(f)\n",
    "    return(all_cloudbase)\n",
    "min_all_cloudbase=np.nanmin(LoadAllCloudBase())\n",
    "print(f\"Minimum Cloudbase is: {min_all_cloudbase}\\n\")\n",
    "\n",
    "dir2 = dir + f'Project_Algorithms/Tracking_Algorithms/'\n",
    "in_file=dir2+f\"parcel_tracking_SUBSET_{res}_{t_res}_{Np_str}\"\n",
    "final_dict=LoadFinalData(in_file)\n",
    "\n",
    "\n",
    "#DYNAMICALLY CREATING VARIABLES\n",
    "for key, value in final_dict.items():\n",
    "    globals()[key] = value\n",
    "\n",
    "# #DYNAMICALLY PRINTING VARIABLE SIZES\n",
    "# for key in final_dict:\n",
    "#     print(f\"{key} has {final_dict[key].shape[0]} parcels\")\n",
    "\n",
    "# PRINTING VARIABLE SIZES (ONE BY ONE)\n",
    "print(f'ALL: {len(CL_ALL_out_arr)} CL parcels and {len(nonCL_ALL_out_arr)} nonCL parcels')\n",
    "print(f'SHALLOW: {len(CL_SHALLOW_out_arr)} CL parcels and {len(nonCL_SHALLOW_out_arr)} nonCL parcels')\n",
    "print(f'DEEP: {len(CL_DEEP_out_arr)} CL parcels and {len(nonCL_DEEP_out_arr)} nonCL parcels')\n",
    "print('\\n')\n",
    "print(f'ALL: {len(SBZ_ALL_out_arr)} SBZ parcels and {len(nonSBZ_ALL_out_arr)} nonSBZ parcels')\n",
    "print(f'SHALLOW: {len(SBZ_SHALLOW_out_arr)} SBZ parcels and {len(nonSBZ_SHALLOW_out_arr)} nonSBZ parcels')\n",
    "print(f'DEEP: {len(SBZ_DEEP_out_arr)} SBZ parcels and {len(nonSBZ_DEEP_out_arr)} nonSBZ parcels')\n",
    "print('\\n')\n",
    "print(f'ALL: {len(ColdPool_ALL_out_arr)} ColdPool parcels')\n",
    "print(f'SHALLOW: {len(ColdPool_SHALLOW_out_arr)} ColdPool parcels')\n",
    "print(f'DEEP: {len(ColdPool_DEEP_out_arr)} ColdPool parcels')\n",
    "\n",
    "\n",
    "#APPLYING JOB ARRAY\n",
    "if job_array==True:\n",
    "    print('APPLYING JOB ARRAY')\n",
    "    def job_filter(arr):\n",
    "        return arr[(arr[:,0]>=start_job)&(arr[:,0]<end_job)]\n",
    "    for name in [\n",
    "        'CL_ALL_out_arr', 'nonCL_ALL_out_arr',\n",
    "        'CL_SHALLOW_out_arr', 'nonCL_SHALLOW_out_arr',\n",
    "        'CL_DEEP_out_arr', 'nonCL_DEEP_out_arr',\n",
    "        'SBZ_ALL_out_arr', 'nonSBZ_ALL_out_arr',\n",
    "        'SBZ_SHALLOW_out_arr', 'nonSBZ_SHALLOW_out_arr',\n",
    "        'SBZ_DEEP_out_arr', 'nonSBZ_DEEP_out_arr',\n",
    "        'ColdPool_ALL_out_arr', 'ColdPool_SHALLOW_out_arr', 'ColdPool_DEEP_out_arr'\n",
    "    ]:\n",
    "        globals()[name] = job_filter(globals()[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62fa092-a450-4506-9ef2-51e20ee875ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALL/DEEP/SHALLOW CL vs non-CL Tracked Parcel Plots\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1bc71d-388c-4dd2-a6a4-89622e2374ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tracked_profile(var_data,type1,type2):\n",
    "\n",
    "    out_arr=globals()[f\"{type2}_{type1.upper()}_out_arr\"].copy() \n",
    "\n",
    "    \n",
    "    zhs=data['zh'].values\n",
    "    profile_array =np.zeros((len(zhs), 3)) #column 1: var, column 2: counter, column 3: list of zhs\n",
    "    profile_array[:,2]=zhs;\n",
    "\n",
    "    for row in range(out_arr.shape[0]):\n",
    "        after=out_arr[row,3]\n",
    "        # if np.mod(row,3000)==0: print(f'{row}/{out_arr.shape[0]}')\n",
    "        p=out_arr[row,0]\n",
    "        \n",
    "        # ts=np.arange(out_arr[row,4],out_arr[row,5]+1 + after)\n",
    "        ts_end = min(out_arr[row, 2] + 1 + after, len(data['time'])) #this takes care of exceeding buffers\n",
    "        ts = np.arange(out_arr[row, 1], ts_end)\n",
    "        \n",
    "        zs=Z[ts,p-index_adjust]\n",
    "        ys=Y[ts,p-index_adjust]\n",
    "        xs=X[ts,p-index_adjust]\n",
    "        \n",
    "        vars=var_data[ts,p-index_adjust]\n",
    "        np.add.at(profile_array[:, 0], zs, vars)\n",
    "        np.add.at(profile_array[:, 1], zs, 1)\n",
    "    return profile_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fa8155-d7d3-4c54-8248-0d61f3693407",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def tracked_profile_SE(profile_data,var_data,type1,type2):  \n",
    "    global test\n",
    "    out_arr=globals()[f\"{type2}_{type1.upper()}_out_arr\"].copy() \n",
    "\n",
    "    \n",
    "    zhs=data['zh'].values\n",
    "    profile_array =np.zeros((len(zhs), 3)) #column 1: var, column 2: counter, column 3: list of zhs\n",
    "    profile_array[:,2]=zhs;\n",
    "\n",
    "    # test=[] #TESTING\n",
    "    for row in range(out_arr.shape[0]):\n",
    "        after=out_arr[row,3]\n",
    "        # if np.mod(row,3000)==0: print(f'{row}/{out_arr.shape[0]}')\n",
    "        p=out_arr[row,0]\n",
    "        \n",
    "        # ts=np.arange(out_arr[row,4],out_arr[row,5]+1 + after)\n",
    "        ts_end = min(out_arr[row, 2] + 1 + after, len(data['time'])) #this takes care of exceeding buffers\n",
    "        ts = np.arange(out_arr[row, 1], ts_end)\n",
    "        \n",
    "        zs=Z[ts,p-index_adjust]\n",
    "        ys=Y[ts,p-index_adjust]\n",
    "        xs=X[ts,p-index_adjust]\n",
    "        \n",
    "        vars=var_data[ts,p-index_adjust]\n",
    "        mean_mu=profile_data[zs,0]/profile_data[zs,1]\n",
    "\n",
    "        # for count,z in enumerate(zs): #TESTING\n",
    "        #     if z==28:\n",
    "        #         test.append(np.array(vars)[count])\n",
    "        np.add.at(profile_array[:, 0], zs,  (vars-mean_mu)**2) #SUMMING UP THE SQUARES\n",
    "        np.add.at(profile_array[:, 1], zs,  1) #SUMMING UP THE SQUARES\n",
    "    return profile_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e962cd8f-5764-4719-b1de-b2fd37cc605e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "#RUNNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e8126b-7e03-4c46-8fcb-94101424d3b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "var_names = ['W', 'QV', 'QCQI', 'TH', 'TH_E', 'BUOYANCY', 'HMC']\n",
    "types = ['all', 'shallow', 'deep']\n",
    "for t in types:\n",
    "    print(t.upper())\n",
    "    for var_name in var_names:\n",
    "        globals()[f'CL_{t.upper()}_profile_array_{var_name.lower()}'] = tracked_profile(globals()[var_name], type1=t, type2='CL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adfe8fc-18b1-42d0-abc6-c4a491b215ee",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #TESTING#*#*\n",
    "# def averaged_profiles(profile):\n",
    "#     out_var=profile[ (profile[:, 1] != 0)]; #gets rid of rows that have no data\\n\"\n",
    "#     out_var=np.array([out_var[:, 0] / out_var[:, 1], out_var[:, 2]]).T #divides the data column by the counter column\n",
    "#     return out_var\n",
    "\n",
    "# # out=averaged_profiles(CL_ALL_profile_array_w)\n",
    "# # out=averaged_profiles(CL_ALL_profile_array_qv)\n",
    "# out=averaged_profiles(CL_ALL_profile_array_th)\n",
    "# plt.plot(out[:,0],out[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e6b6dc-2ad7-4971-a132-b58dafd778f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names = ['W', 'QV', 'QCQI', 'TH', 'TH_E', 'BUOYANCY', 'HMC']\n",
    "types = ['all', 'shallow', 'deep']\n",
    "for t in types:\n",
    "    print(t.upper())\n",
    "    for var_name in var_names:\n",
    "        globals()[f'nonCL_{t.upper()}_profile_array_{var_name.lower()}'] = tracked_profile(globals()[var_name], type1=t,type2='nonCL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1217e54b-1c4a-42a6-857d-ba239684a75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVING\n",
    "\n",
    "# Define categories and variables\n",
    "types = [\"ALL\", \"SHALLOW\", \"DEEP\"]\n",
    "variables = [\"W\", \"QV\", \"QCQI\", \"TH\", \"TH_E\", \"BUOYANCY\", \"HMC\"]\n",
    "\n",
    "# Dictionary to store all variables\n",
    "tracked_profiles = {}\n",
    "\n",
    "# Populate all profiles in one loop\n",
    "for type in types:\n",
    "    for var in variables:\n",
    "        tracked_profiles[f\"CL_{type}_profile_array_{var.lower()}\"] = eval(f\"CL_{type}_profile_array_{var.lower()}\")\n",
    "        tracked_profiles[f\"nonCL_{type}_profile_array_{var.lower()}\"] = eval(f\"nonCL_{type}_profile_array_{var.lower()}\")\n",
    "\n",
    "# Save all variables in an HDF5 file\n",
    "dir2=dir+'Project_Algorithms/Tracked_Profiles/job_out2/'\n",
    "output_file=dir2+f\"CL_nonCL_tracked_profiles_{res}_{t_res}_{Np_str}\"\n",
    "if job_array==True:\n",
    "    output_file+=f\"_{job_id}.h5\"\n",
    "elif job_array==False:\n",
    "    output_file+=f\".h5\"\n",
    "\n",
    "with h5py.File(output_file, \"w\") as h5f:\n",
    "    for name, profile_data in tracked_profiles.items():\n",
    "        h5f.create_dataset(name, data=profile_data)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34584d90-7fb9-43f4-a36e-a37513f715c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SBZ vs nonSBZ\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d3b8ad-e287-4dc3-bab5-9cdecf30e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names = ['W', 'QV', 'QCQI', 'TH', 'TH_E', 'BUOYANCY', 'HMC']\n",
    "types = ['all', 'shallow', 'deep']\n",
    "for t in types:\n",
    "    print(t.upper())\n",
    "    for var_name in var_names:\n",
    "        globals()[f'SBZ_{t.upper()}_profile_array_{var_name.lower()}'] = tracked_profile(globals()[var_name], type1=t, type2='SBZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c4ebb8-6927-4c3e-b64e-899b9f6a3ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names = ['W', 'QV', 'QCQI', 'TH', 'TH_E', 'BUOYANCY', 'HMC']\n",
    "types = ['all', 'shallow', 'deep']\n",
    "for t in types:\n",
    "    print(t.upper())\n",
    "    for var_name in var_names:\n",
    "        globals()[f'nonSBZ_{t.upper()}_profile_array_{var_name.lower()}'] = tracked_profile(globals()[var_name], type1=t, type2='nonSBZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64677b75-51ea-44c0-a463-104b111a86ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVING\n",
    "\n",
    "# Define categories and variables\n",
    "types = [\"ALL\", \"SHALLOW\", \"DEEP\"]\n",
    "variables = [\"W\", \"QV\", \"QCQI\", \"TH\", \"TH_E\", \"BUOYANCY\", \"HMC\"]\n",
    "\n",
    "# Dictionary to store all variables\n",
    "tracked_profiles = {}\n",
    "\n",
    "# Populate all profiles in one loop\n",
    "for type in types:\n",
    "    for var in variables:\n",
    "        tracked_profiles[f\"SBZ_{type}_profile_array_{var.lower()}\"] = eval(f\"SBZ_{type}_profile_array_{var.lower()}\")\n",
    "        tracked_profiles[f\"nonSBZ_{type}_profile_array_{var.lower()}\"] = eval(f\"nonSBZ_{type}_profile_array_{var.lower()}\")\n",
    "\n",
    "# Save all variables in an HDF5 file\n",
    "dir2=dir+'Project_Algorithms/Tracked_Profiles/job_out2/'\n",
    "output_file=dir2+f\"SBZ_nonSBZ_tracked_profiles_{res}_{t_res}_{Np_str}\"\n",
    "if job_array==True:\n",
    "    output_file+=f\"_{job_id}.h5\"\n",
    "elif job_array==False:\n",
    "    output_file+=f\".h5\"\n",
    "with h5py.File(output_file, \"w\") as h5f:\n",
    "    for name, profile_data in tracked_profiles.items():\n",
    "        h5f.create_dataset(name, data=profile_data)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84d7dc0-5374-4cca-a0d8-a5bd9bf52773",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ColdPool\n",
    "################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2665b72a-b833-4971-a665-49010e57de4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names = ['W', 'QV', 'QCQI', 'TH', 'TH_E', 'BUOYANCY', 'HMC']\n",
    "types = ['all', 'shallow', 'deep']\n",
    "for t in types:\n",
    "    print(t.upper())\n",
    "    for var_name in var_names:\n",
    "        globals()[f'ColdPool_{t.upper()}_profile_array_{var_name.lower()}'] = tracked_profile(globals()[var_name], type1=t, type2='ColdPool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65427607-3a24-451f-aadc-1721e3b56009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVING\n",
    "\n",
    "# Define categories and variables\n",
    "types = [\"ALL\", \"SHALLOW\", \"DEEP\"]\n",
    "variables = [\"W\", \"QV\", \"QCQI\", \"TH\", \"TH_E\", \"BUOYANCY\", \"HMC\"]\n",
    "\n",
    "# Dictionary to store all variables\n",
    "tracked_profiles = {}\n",
    "\n",
    "# Populate all profiles in one loop\n",
    "for type in types:\n",
    "    for var in variables:\n",
    "        tracked_profiles[f\"ColdPool_{type}_profile_array_{var.lower()}\"] = eval(f\"ColdPool_{type}_profile_array_{var.lower()}\")\n",
    "\n",
    "# Save all variables in an HDF5 file\n",
    "dir2=dir+'Project_Algorithms/Tracked_Profiles/job_out2/'\n",
    "output_file=dir2+f\"ColdPool_tracked_profiles_{res}_{t_res}_{Np_str}\"\n",
    "if job_array==True:\n",
    "    output_file+=f\"_{job_id}.h5\"\n",
    "elif job_array==False:\n",
    "    output_file+=f\".h5\"\n",
    "with h5py.File(output_file, \"w\") as h5f:\n",
    "    for name, profile_data in tracked_profiles.items():\n",
    "        h5f.create_dataset(name, data=profile_data)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d08873-1402-4842-b62e-85a207036dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_memory(globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228b132f-d523-406c-b2fe-e81877494f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e215e4-eaba-4ad8-8857-405984abb9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#RECOMBINING\n",
    "recombine=False #KEEP FALSE WHEN JOB_ARRAY IS RUNNING\n",
    "# recombine=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7523a4-c1f9-4cab-834b-7ae5bfa45e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "if recombine==True:\n",
    "    types = [\"ALL\", \"SHALLOW\", \"DEEP\"]\n",
    "    variables = [\"W\", \"QV\", \"QCQI\", \"TH\", \"TH_E\", \"BUOYANCY\", \"HMC\"]\n",
    "    \n",
    "    vars_list = []\n",
    "    \n",
    "    for t in types:\n",
    "        for var in variables:\n",
    "            vars_list.append(f\"CL_{t}_profile_array_{var.lower()}\")\n",
    "            vars_list.append(f\"nonCL_{t}_profile_array_{var.lower()}\")\n",
    "            \n",
    "    dir2=dir+'Project_Algorithms/Tracked_Profiles/job_out2/'\n",
    "    dir3=dir+'Project_Algorithms/Tracked_Profiles/OUTPUT_FILES/'\n",
    "    \n",
    "    #MAKING OUTPUT FILE PATH\n",
    "    output_file=dir3+f\"CL_nonCL_tracked_profiles_{res}_{t_res}_{Np_str}.h5\"\n",
    "    \n",
    "    #MAKING PROFILES DICTIONARY\n",
    "    zhs = data['zh'].values\n",
    "    profiles = {}  # Store profiles for all variables\n",
    "    for var in vars_list:\n",
    "        profiles[var] = np.zeros((len(zhs), 3))  # column 1: var, column 2: counter, column 3: list of zhs\n",
    "        profiles[var][:, 2] = zhs \n",
    "\n",
    "    num_jobs=300\n",
    "    for job_id in np.arange(1,num_jobs+1):\n",
    "        if np.mod(job_id,10)==0: print(f\"job_id = {job_id}\")\n",
    "    \n",
    "        #CALLING IN DATA\n",
    "        input_file=dir2+f\"CL_nonCL_tracked_profiles_{res}_{t_res}_{Np_str}_{job_id}.h5\"\n",
    "    \n",
    "        #COMPILING PROFILES\n",
    "        with h5py.File(input_file, 'r') as f:\n",
    "            for var in vars_list:\n",
    "                profiles[var][:,0:1+1]+=f[f'{var}'][:,0:1+1]\n",
    "    \n",
    "    #SAVING INTO FINAL FORM\n",
    "    with h5py.File(output_file, 'w') as f:\n",
    "        for var in profiles:\n",
    "            profile_var = profiles[var]\n",
    "            f.create_dataset(f'{var}', data=profile_var, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f622301a-e644-49b2-8b99-370a14b4343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if recombine==True:\n",
    "\n",
    "    types = [\"ALL\", \"SHALLOW\", \"DEEP\"]\n",
    "    variables = [\"W\", \"QV\", \"QCQI\", \"TH\", \"TH_E\", \"BUOYANCY\", \"HMC\"]\n",
    "    \n",
    "    vars_list = []\n",
    "    \n",
    "    for t in types:\n",
    "        for var in variables:\n",
    "            vars_list.append(f\"SBZ_{t}_profile_array_{var.lower()}\")\n",
    "            vars_list.append(f\"nonSBZ_{t}_profile_array_{var.lower()}\")\n",
    "            \n",
    "    dir2=dir+'Project_Algorithms/Tracked_Profiles/job_out2/'\n",
    "    dir3=dir+'Project_Algorithms/Tracked_Profiles/OUTPUT_FILES/'\n",
    "    \n",
    "    #MAKING OUTPUT FILE PATH\n",
    "    output_file=dir3+f\"SBZ_nonSBZ_tracked_profiles_{res}_{t_res}_{Np_str}.h5\"\n",
    "    \n",
    "    #MAKING PROFILES DICTIONARY\n",
    "    zhs = data['zh'].values\n",
    "    profiles = {}  # Store profiles for all variables\n",
    "    for var in vars_list:\n",
    "        profiles[var] = np.zeros((len(zhs), 3))  # column 1: var, column 2: counter, column 3: list of zhs\n",
    "        profiles[var][:, 2] = zhs \n",
    "    \n",
    "    num_jobs=300\n",
    "    for job_id in np.arange(1,num_jobs+1):\n",
    "        if np.mod(job_id,10)==0: print(f\"job_id = {job_id}\")\n",
    "    \n",
    "        #CALLING IN DATA\n",
    "        input_file=dir2+f\"SBZ_nonSBZ_tracked_profiles_{res}_{t_res}_{Np_str}_{job_id}.h5\"\n",
    "    \n",
    "        #COMPILING PROFILES\n",
    "        with h5py.File(input_file, 'r') as f:\n",
    "            for var in vars_list:\n",
    "                profiles[var][:,0:1+1]+=f[f'{var}'][:,0:1+1]\n",
    "    \n",
    "    #SAVING INTO FINAL FORM\n",
    "    with h5py.File(output_file, 'w') as f:\n",
    "        for var in profiles:\n",
    "            profile_var = profiles[var]\n",
    "            f.create_dataset(f'{var}', data=profile_var, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54e0a2b-cbc1-4bf5-be56-07598ba5c45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if recombine==True:\n",
    "\n",
    "    types = [\"ALL\", \"SHALLOW\", \"DEEP\"]\n",
    "    variables = [\"W\", \"QV\", \"QCQI\", \"TH\", \"TH_E\", \"BUOYANCY\", \"HMC\"]\n",
    "    \n",
    "    vars_list = []\n",
    "    \n",
    "    for t in types:\n",
    "        for var in variables:\n",
    "            vars_list.append(f\"ColdPool_{t}_profile_array_{var.lower()}\")\n",
    "            \n",
    "    dir2=dir+'Project_Algorithms/Tracked_Profiles/job_out2/'\n",
    "    dir3=dir+'Project_Algorithms/Tracked_Profiles/OUTPUT_FILES/'\n",
    "    \n",
    "    \n",
    "    #MAKING OUTPUT FILE PATH\n",
    "    output_file=dir3+f\"ColdPool_tracked_profiles_{res}_{t_res}_{Np_str}.h5\"\n",
    "    \n",
    "    #MAKING PROFILES DICTIONARY\n",
    "    zhs = data['zh'].values\n",
    "    profiles = {}  # Store profiles for all variables\n",
    "    for var in vars_list:\n",
    "        profiles[var] = np.zeros((len(zhs), 3))  # column 1: var, column 2: counter, column 3: list of zhs\n",
    "        profiles[var][:, 2] = zhs \n",
    "    \n",
    "    num_jobs=300\n",
    "    for job_id in np.arange(1,num_jobs+1):\n",
    "        if np.mod(job_id,10)==0: print(f\"job_id = {job_id}\")\n",
    "    \n",
    "        #CALLING IN DATA\n",
    "        input_file=dir2+f\"ColdPool_tracked_profiles_{res}_{t_res}_{Np_str}_{job_id}.h5\"\n",
    "    \n",
    "        #COMPILING PROFILES\n",
    "        with h5py.File(input_file, 'r') as f:\n",
    "            for var in vars_list:\n",
    "                profiles[var][:,0:1+1]+=f[f'{var}'][:,0:1+1]\n",
    "    \n",
    "    #SAVING INTO FINAL FORM\n",
    "    with h5py.File(output_file, 'w') as f:\n",
    "        for var in profiles:\n",
    "            profile_var = profiles[var]\n",
    "            f.create_dataset(f'{var}', data=profile_var, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c912a1-c1d4-47b4-8687-e90d54c06778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39786fdd-1819-4ed3-bfdc-f8de7af310a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d361c3-d930-4dcf-87f9-9aab2d6b3864",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffa8a55-24a4-49e5-9dd4-8e48aa0b639c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "#RUNNING FOR STANDARD ERROR\n",
    "POST_JOB_ARRAY=False #KEEP FALSE FOR FIRST RUN\n",
    "# POST_JOB_ARRAY=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49137517-8d8c-4dc3-a8ae-6f19f43309ae",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#LOADING BACK IN\n",
    "if POST_JOB_ARRAY==True:\n",
    "    \n",
    "    types = [\"ALL\", \"SHALLOW\", \"DEEP\"]\n",
    "    variables = [\"W\", \"QV\", \"QCQI\", \"TH\", \"TH_E\", \"BUOYANCY\", \"HMC\"]\n",
    "    \n",
    "    vars_list = []\n",
    "    \n",
    "    for t in types:\n",
    "        for var in variables:\n",
    "            vars_list.append(f\"CL_{t}_profile_array_{var.lower()}\")\n",
    "            vars_list.append(f\"nonCL_{t}_profile_array_{var.lower()}\")\n",
    "    \n",
    "    # Define directory and output file path\n",
    "    dir2 = dir + 'Project_Algorithms/Tracked_Profiles/job_out2/'\n",
    "    output_file = dir2 + f\"CL_nonCL_tracked_profiles_{res}_{t_res}_{Np_str}.h5\"\n",
    "    \n",
    "    # Open the HDF5 file and read the stored datasets into dynamically named variables\n",
    "    with h5py.File(output_file, 'r') as f:\n",
    "        for var in vars_list:\n",
    "            globals()[var] = f[f'{var}'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc90b7b-c871-425c-83ef-9c75a3e6c9d9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#LOADING BACK IN\n",
    "if POST_JOB_ARRAY==True:\n",
    "    \n",
    "    types = [\"ALL\", \"SHALLOW\", \"DEEP\"]\n",
    "    variables = [\"W\", \"QV\", \"QCQI\", \"TH\", \"TH_E\", \"BUOYANCY\", \"HMC\"]\n",
    "    \n",
    "    vars_list = []\n",
    "    SE_list = []\n",
    "    \n",
    "    for t in types:\n",
    "        for var in variables:\n",
    "            vars_list.append(f\"SBZ_{t}_profile_array_{var.lower()}\")\n",
    "            vars_list.append(f\"nonSBZ_{t}_profile_array_{var.lower()}\")\n",
    "    \n",
    "    # Define directory and output file path\n",
    "    dir2 = dir + 'Project_Algorithms/Tracked_Profiles/job_out2/'\n",
    "    output_file = dir2 + f\"SBZ_nonSBZ_tracked_profiles_{res}_{t_res}_{Np_str}.h5\"\n",
    "    \n",
    "    # Open the HDF5 file and read the stored datasets into dynamically named variables\n",
    "    with h5py.File(output_file, 'r') as f:\n",
    "        for var in vars_list:\n",
    "            globals()[var] = f[f'{var}'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b14d5af-5402-40e1-830f-ac38bed30377",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#LOADING BACK IN\n",
    "if POST_JOB_ARRAY==True:\n",
    "    \n",
    "    types = [\"ALL\", \"SHALLOW\", \"DEEP\"]\n",
    "    variables = [\"W\", \"QV\", \"QCQI\", \"TH\", \"TH_E\", \"BUOYANCY\", \"HMC\"]\n",
    "    \n",
    "    vars_list = []\n",
    "    SE_list = []\n",
    "    \n",
    "    for t in types:\n",
    "        for var in variables:\n",
    "            vars_list.append(f\"ColdPool_{t}_profile_array_{var.lower()}\")\n",
    "    \n",
    "    # Define directory and output file path\n",
    "    dir2 = dir + 'Project_Algorithms/Tracked_Profiles/job_out2/'\n",
    "    output_file = dir2 + f\"ColdPool_tracked_profiles_{res}_{t_res}_{Np_str}.h5\"\n",
    "    \n",
    "    # Open the HDF5 file and read the stored datasets into dynamically named variables\n",
    "    with h5py.File(output_file, 'r') as f:\n",
    "        for var in vars_list:\n",
    "            globals()[var] = f[f'{var}'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab5e618-03b7-4e81-b209-3d5c16321808",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if POST_JOB_ARRAY==True:\n",
    "    var_names = ['W', 'QV', 'QCQI', 'TH', 'TH_E', 'BUOYANCY', 'HMC']\n",
    "    types = ['all', 'shallow', 'deep']\n",
    "    \n",
    "    for t in types:\n",
    "        print(t.upper())\n",
    "        for var_name in var_names:\n",
    "            globals()[f'CL_{t.upper()}_SE_{var_name.lower()}'] = tracked_profile_SE(globals()[f'CL_{t.upper()}_profile_array_{var_name.lower()}'],globals()[var_name], type1=t, type2='CL')\n",
    "            \n",
    "if POST_JOB_ARRAY==True:\n",
    "    \n",
    "    var_names = ['W', 'QV', 'QCQI', 'TH', 'TH_E', 'BUOYANCY', 'HMC']\n",
    "    types = ['all', 'shallow', 'deep']\n",
    "    \n",
    "    for t in types:\n",
    "        print(t.upper())\n",
    "        for var_name in var_names:\n",
    "            globals()[f'nonCL_{t.upper()}_SE_{var_name.lower()}'] = tracked_profile_SE(globals()[f'nonCL_{t.upper()}_profile_array_{var_name.lower()}'],globals()[var_name], type1=t, type2='nonCL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0c3e1d-fdb9-4403-995a-8874ef824de2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if POST_JOB_ARRAY==True:\n",
    "    var_names = ['W', 'QV', 'QCQI', 'TH', 'TH_E', 'BUOYANCY', 'HMC']\n",
    "    types = ['all', 'shallow', 'deep']\n",
    "    \n",
    "    for t in types:\n",
    "        print(t.upper())\n",
    "        for var_name in var_names:\n",
    "            globals()[f'SBZ_{t.upper()}_SE_{var_name.lower()}'] = tracked_profile_SE(globals()[f'SBZ_{t.upper()}_profile_array_{var_name.lower()}'],globals()[var_name], type1=t, type2='SBZ')\n",
    "            \n",
    "if POST_JOB_ARRAY==True:\n",
    "    \n",
    "    var_names = ['W', 'QV', 'QCQI', 'TH', 'TH_E', 'BUOYANCY', 'HMC']\n",
    "    types = ['all', 'shallow', 'deep']\n",
    "    \n",
    "    for t in types:\n",
    "        print(t.upper())\n",
    "        for var_name in var_names:\n",
    "            globals()[f'nonSBZ_{t.upper()}_SE_{var_name.lower()}'] = tracked_profile_SE(globals()[f'nonSBZ_{t.upper()}_profile_array_{var_name.lower()}'],globals()[var_name], type1=t, type2='nonSBZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c0c139-4d08-43d6-bdcf-91fe7f78b7fe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if POST_JOB_ARRAY==True:\n",
    "    var_names = ['W', 'QV', 'QCQI', 'TH', 'TH_E', 'BUOYANCY', 'HMC']\n",
    "    types = ['all', 'shallow', 'deep']\n",
    "    \n",
    "    for t in types:\n",
    "        print(t.upper())\n",
    "        for var_name in var_names:\n",
    "            globals()[f'ColdPool_{t.upper()}_SE_{var_name.lower()}'] = tracked_profile_SE(globals()[f'ColdPool_{t.upper()}_profile_array_{var_name.lower()}'],globals()[var_name], type1=t, type2='ColdPool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8d067f-2150-4d5b-96fc-c1abec8475c5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#SAVING\n",
    "if POST_JOB_ARRAY==True:\n",
    "    # Define categories and variables\n",
    "    types = [\"ALL\", \"SHALLOW\", \"DEEP\"]\n",
    "    variables = [\"W\", \"QV\", \"QCQI\", \"TH\", \"TH_E\", \"BUOYANCY\", \"HMC\"]\n",
    "    \n",
    "    # Dictionary to store all variables\n",
    "    tracked_profiles = {}\n",
    "    \n",
    "    # Populate all profiles in one loop\n",
    "    for type in types:\n",
    "        for var in variables:\n",
    "            tracked_profiles[f\"CL_{type}_profile_array_{var.lower()}\"] = eval(f\"CL_{type}_profile_array_{var.lower()}\")\n",
    "            tracked_profiles[f\"nonCL_{type}_profile_array_{var.lower()}\"] = eval(f\"nonCL_{type}_profile_array_{var.lower()}\")\n",
    "            \n",
    "            tracked_profiles[f\"CL_{type}_SE_{var.lower()}\"] = eval(f\"CL_{type}_SE_{var.lower()}\")\n",
    "            tracked_profiles[f\"nonCL_{type}_SE_{var.lower()}\"] = eval(f\"nonCL_{type}_SE_{var.lower()}\")\n",
    "    \n",
    "    # Save all variables in an HDF5 file\n",
    "    dir2=dir+'Project_Algorithms/Tracked_Profiles/job_out2/'\n",
    "    output_file=dir2+f\"CL_nonCL_tracked_profiles_SE_{res}_{t_res}_{Np_str}\"\n",
    "    if job_array==True:\n",
    "        output_file+=f\"_{job_id}.h5\"\n",
    "    elif job_array==False:\n",
    "        output_file+=f\".h5\"\n",
    "    with h5py.File(output_file, \"w\") as h5f:\n",
    "        for name, profile_data in tracked_profiles.items():\n",
    "            h5f.create_dataset(name, data=profile_data)\n",
    "    print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45fb86c-a583-411e-a891-e5a285808a7c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#SAVING\n",
    "if POST_JOB_ARRAY==True:\n",
    "# Define categories and variables\n",
    "    types = [\"ALL\", \"SHALLOW\", \"DEEP\"]\n",
    "    variables = [\"W\", \"QV\", \"QCQI\", \"TH\", \"TH_E\", \"BUOYANCY\", \"HMC\"]\n",
    "    \n",
    "    # Dictionary to store all variables\n",
    "    tracked_profiles = {}\n",
    "    \n",
    "    # Populate all profiles in one loop\n",
    "    for type in types:\n",
    "        for var in variables:\n",
    "            tracked_profiles[f\"SBZ_{type}_profile_array_{var.lower()}\"] = eval(f\"SBZ_{type}_profile_array_{var.lower()}\")\n",
    "            tracked_profiles[f\"nonSBZ_{type}_profile_array_{var.lower()}\"] = eval(f\"nonSBZ_{type}_profile_array_{var.lower()}\")\n",
    "            \n",
    "            tracked_profiles[f\"SBZ_{type}_SE_{var.lower()}\"] = eval(f\"SBZ_{type}_SE_{var.lower()}\")\n",
    "            tracked_profiles[f\"nonSBZ_{type}_SE_{var.lower()}\"] = eval(f\"nonSBZ_{type}_SE_{var.lower()}\")\n",
    "    \n",
    "    # Save all variables in an HDF5 file\n",
    "    dir2=dir+'Project_Algorithms/Tracked_Profiles/job_out2/'\n",
    "    output_file=dir2+f\"SBZ_nonSBZ_tracked_profiles_SE_{res}_{t_res}_{Np_str}\"\n",
    "    if job_array==True:\n",
    "        output_file+=f\"_{job_id}.h5\"\n",
    "    elif job_array==False:\n",
    "        output_file+=f\".h5\"\n",
    "    with h5py.File(output_file, \"w\") as h5f:\n",
    "        for name, profile_data in tracked_profiles.items():\n",
    "            h5f.create_dataset(name, data=profile_data)\n",
    "    print('done')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8980053-67e6-4336-b27f-6ab1c7fb8c72",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#SAVING\n",
    "if POST_JOB_ARRAY==True:\n",
    "    # Define categories and variables\n",
    "    types = [\"ALL\", \"SHALLOW\", \"DEEP\"]\n",
    "    variables = [\"W\", \"QV\", \"QCQI\", \"TH\", \"TH_E\", \"BUOYANCY\", \"HMC\"]\n",
    "    \n",
    "    # Dictionary to store all variables\n",
    "    tracked_profiles = {}\n",
    "    \n",
    "    # Populate all profiles in one loop\n",
    "    for type in types:\n",
    "        for var in variables:\n",
    "            tracked_profiles[f\"ColdPool_{type}_profile_array_{var.lower()}\"] = eval(f\"ColdPool_{type}_profile_array_{var.lower()}\")\n",
    "            \n",
    "            tracked_profiles[f\"ColdPool_{type}_SE_{var.lower()}\"] = eval(f\"ColdPool_{type}_SE_{var.lower()}\")\n",
    "    \n",
    "    # Save all variables in an HDF5 file\n",
    "    dir2=dir+'Project_Algorithms/Tracked_Profiles/job_out2/'\n",
    "    output_file=dir2+f\"ColdPool_tracked_profiles_SE_{res}_{t_res}_{Np_str}\"\n",
    "    if job_array==True:\n",
    "        output_file+=f\"_{job_id}.h5\"\n",
    "    elif job_array==False:\n",
    "        output_file+=f\".h5\"\n",
    "    with h5py.File(output_file, \"w\") as h5f:\n",
    "        for name, profile_data in tracked_profiles.items():\n",
    "            h5f.create_dataset(name, data=profile_data)\n",
    "    print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdf56ff-e0d5-4988-b95c-df33163c4333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#RECOMBINING\n",
    "recombine=False #KEEP FALSE WHEN JOB_ARRAY IS RUNNING\n",
    "# recombine=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e59327-1992-47d6-9750-090f24ae6485",
   "metadata": {},
   "outputs": [],
   "source": [
    "if recombine==True:\n",
    "    types = [\"ALL\", \"SHALLOW\", \"DEEP\"]\n",
    "    variables = [\"W\", \"QV\", \"QCQI\", \"TH\", \"TH_E\", \"BUOYANCY\", \"HMC\"]\n",
    "    \n",
    "    vars_list = []\n",
    "    \n",
    "    for t in types:\n",
    "        for var in variables:\n",
    "            vars_list.append(f\"CL_{t}_profile_array_SE_{var}\")\n",
    "            vars_list.append(f\"nonCL_{t}_profile_array_SE_{var}\")\n",
    "            \n",
    "    dir2=dir+'Project_Algorithms/Tracked_Profiles/job_out2/'\n",
    "    dir3=dir+'Project_Algorithms/Tracked_Profiles/OUTPUT_FILES/'\n",
    "    \n",
    "    #MAKING OUTPUT FILE PATH\n",
    "    output_file=dir3+f\"CL_nonCL_tracked_profiles_SE_{res}_{t_res}_{Np_str}.h5\"\n",
    "    \n",
    "    #MAKING PROFILES DICTIONARY\n",
    "    zhs = data['zh'].values\n",
    "    profiles = {}  # Store profiles for all variables\n",
    "    for var in vars_list:\n",
    "        profiles[var] = np.zeros((len(zhs), 3))  # column 1: var, column 2: counter, column 3: list of zhs\n",
    "        profiles[var][:, 2] = zhs \n",
    "    \n",
    "    num_jobs=300\n",
    "    for job_id in np.arange(1,num_jobs+1):\n",
    "        if np.mod(job_id,10)==0: print(f\"job_id = {job_id}\")\n",
    "    \n",
    "        #CALLING IN DATA\n",
    "        input_file=dir2+f\"CL_nonCL_tracked_profiles_SE_{res}_{t_res}_{Np_str}_{job_id}.h5\"\n",
    "    \n",
    "        #COMPILING PROFILES\n",
    "        with h5py.File(input_file, 'r') as f:\n",
    "            for var in vars_list:\n",
    "                profiles[var][:,0:1+1]+=f[f'{var}'][:,0:1+1]\n",
    "    \n",
    "    #SAVING INTO FINAL FORM\n",
    "    with h5py.File(output_file, 'w') as f:\n",
    "        for var in profiles:\n",
    "            profile_var = profiles[var]\n",
    "            f.create_dataset(f'{var}', data=profile_var, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06401a47-815a-4078-9aaa-76b3ffd9a391",
   "metadata": {},
   "outputs": [],
   "source": [
    "if recombine==True:\n",
    "    types = [\"ALL\", \"SHALLOW\", \"DEEP\"]\n",
    "    variables = [\"W\", \"QV\", \"QCQI\", \"TH\", \"TH_E\", \"BUOYANCY\", \"HMC\"]\n",
    "    \n",
    "    vars_list = []\n",
    "    \n",
    "    for t in types:\n",
    "        for var in variables:\n",
    "            vars_list.append(f\"SBZ_{t}_profile_array_SE_{var}\")\n",
    "            vars_list.append(f\"nonSBZ_{t}_profile_array_SE_{var}\")\n",
    "            \n",
    "    dir2=dir+'Project_Algorithms/Tracked_Profiles/job_out2/'\n",
    "    dir3=dir+'Project_Algorithms/Tracked_Profiles/OUTPUT_FILES/'\n",
    "    \n",
    "    #MAKING OUTPUT FILE PATH\n",
    "    output_file=dir3+f\"SBZ_nonSBZ_tracked_profiles_SE_{res}_{t_res}_{Np_str}.h5\"\n",
    "    \n",
    "    #MAKING PROFILES DICTIONARY\n",
    "    zhs = data['zh'].values\n",
    "    profiles = {}  # Store profiles for all variables\n",
    "    for var in vars_list:\n",
    "        profiles[var] = np.zeros((len(zhs), 3))  # column 1: var, column 2: counter, column 3: list of zhs\n",
    "        profiles[var][:, 2] = zhs \n",
    "    \n",
    "    num_jobs=300\n",
    "    for job_id in np.arange(1,num_jobs+1):\n",
    "        if np.mod(job_id,10)==0: print(f\"job_id = {job_id}\")\n",
    "    \n",
    "        #CALLING IN DATA\n",
    "        input_file=dir2+f\"SBZ_nonSBZ_tracked_profiles_SE_{res}_{t_res}_{Np_str}_{job_id}.h5\"\n",
    "    \n",
    "        #COMPILING PROFILES\n",
    "        with h5py.File(input_file, 'r') as f:\n",
    "            for var in vars_list:\n",
    "                profiles[var][:,0:1+1]+=f[f'{var}'][:,0:1+1]\n",
    "    \n",
    "    #SAVING INTO FINAL FORM\n",
    "    with h5py.File(output_file, 'w') as f:\n",
    "        for var in profiles:\n",
    "            profile_var = profiles[var]\n",
    "            f.create_dataset(f'{var}', data=profile_var, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbf7ca7-906c-4e34-bd2c-916b2d76e063",
   "metadata": {},
   "outputs": [],
   "source": [
    "if recombine==True:\n",
    "    types = [\"ALL\", \"SHALLOW\", \"DEEP\"]\n",
    "    variables = [\"W\", \"QV\", \"QCQI\", \"TH\", \"TH_E\", \"BUOYANCY\", \"HMC\"]\n",
    "    \n",
    "    vars_list = []\n",
    "    \n",
    "    for t in types:\n",
    "        for var in variables:\n",
    "            vars_list.append(f\"ColdPool_{t}_profile_array_SE_{var}\")\n",
    "            \n",
    "    dir2=dir+'Project_Algorithms/Tracked_Profiles/job_out2/'\n",
    "    dir3=dir+'Project_Algorithms/Tracked_Profiles/OUTPUT_FILES/'\n",
    "    \n",
    "    #MAKING OUTPUT FILE PATH\n",
    "    output_file=dir3+f\"ColdPool_tracked_profiles_SE_{res}_{t_res}_{Np_str}.h5\"\n",
    "    \n",
    "    #MAKING PROFILES DICTIONARY\n",
    "    zhs = data['zh'].values\n",
    "    profiles = {}  # Store profiles for all variables\n",
    "    for var in vars_list:\n",
    "        profiles[var] = np.zeros((len(zhs), 3))  # column 1: var, column 2: counter, column 3: list of zhs\n",
    "        profiles[var][:, 2] = zhs \n",
    "    \n",
    "    num_jobs=300\n",
    "    for job_id in np.arange(1,num_jobs+1):\n",
    "        if np.mod(job_id,10)==0: print(f\"job_id = {job_id}\")\n",
    "    \n",
    "        #CALLING IN DATA\n",
    "        input_file=dir2+f\"ColdPool_tracked_profiles_SE_{res}_{t_res}_{Np_str}_{job_id}.h5\"\n",
    "    \n",
    "        #COMPILING PROFILES\n",
    "        with h5py.File(input_file, 'r') as f:\n",
    "            for var in vars_list:\n",
    "                profiles[var][:,0:1+1]+=f[f'{var}'][:,0:1+1]\n",
    "    \n",
    "    #SAVING INTO FINAL FORM\n",
    "    with h5py.File(output_file, 'w') as f:\n",
    "        for var in profiles:\n",
    "            profile_var = profiles[var]\n",
    "            f.create_dataset(f'{var}', data=profile_var, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e95831e-8a43-479b-92bb-3a2e2d785574",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cdd687-a532-432d-9b0a-bf6bf3bf4628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef71468-4b13-4c92-ad72-ad77d7dab64e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaabb538-f0ad-43f6-a85f-490973cc3a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c700af6-dda2-4415-bebf-062188a812c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#READING BACK IN\n",
    "\n",
    "types = [\"ALL\", \"SHALLOW\", \"DEEP\"]\n",
    "variables = [\"W\", \"QV\", \"QCQI\", \"TH\", \"TH_E\", \"BUOYANCY\", \"HMC\"]\n",
    "\n",
    "vars_list = []\n",
    "SE_list = []\n",
    "\n",
    "for t in types:\n",
    "    for var in variables:\n",
    "        vars_list.append(f\"CL_{t}_profile_array_{var.lower()}\")\n",
    "        vars_list.append(f\"nonCL_{t}_profile_array_{var.lower()}\")\n",
    "for t in types:\n",
    "    for var in variables:\n",
    "        SE_list.append(f\"CL_{t}_SE_{var.lower()}\")\n",
    "        SE_list.append(f\"nonCL_{t}_SE_{var.lower()}\")\n",
    "\n",
    "# Define directory and output file path\n",
    "dir2=dir+'Project_Algorithms/Tracked_Profiles/job_out2/'\n",
    "output_file=dir2+f\"CL_nonCL_tracked_profiles_SE_{res}_{t_res}_{Np_str}.h5\"\n",
    "\n",
    "# Open the HDF5 file and read the stored datasets into dynamically named variables\n",
    "with h5py.File(output_file, 'r') as f:\n",
    "    for var in vars_list + SE_list:\n",
    "        globals()[var] = f[f'{var}'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd3e70f-b808-4bf3-be8a-20a07f75d375",
   "metadata": {},
   "outputs": [],
   "source": [
    "#READING BACK IN\n",
    "\n",
    "types = [\"ALL\", \"SHALLOW\", \"DEEP\"]\n",
    "variables = [\"W\", \"QV\", \"QCQI\", \"TH\", \"TH_E\", \"BUOYANCY\", \"HMC\"]\n",
    "\n",
    "vars_list = []\n",
    "SE_list = []\n",
    "\n",
    "for t in types:\n",
    "    for var in variables:\n",
    "        vars_list.append(f\"SBZ_{t}_profile_array_{var.lower()}\")\n",
    "        vars_list.append(f\"nonSBZ_{t}_profile_array_{var.lower()}\")\n",
    "for t in types:\n",
    "    for var in variables:\n",
    "        SE_list.append(f\"SBZ_{t}_SE_{var.lower()}\")\n",
    "        SE_list.append(f\"nonSBZ_{t}_SE_{var.lower()}\")\n",
    "\n",
    "# Define directory and output file path\n",
    "dir2=dir+'Project_Algorithms/Tracked_Profiles/job_out2/'\n",
    "output_file=dir2+f\"SBZ_nonSBZ_tracked_profiles_SE_{res}_{t_res}_{Np_str}.h5\"\n",
    "\n",
    "# Open the HDF5 file and read the stored datasets into dynamically named variables\n",
    "with h5py.File(output_file, 'r') as f:\n",
    "    for var in vars_list + SE_list:\n",
    "        globals()[var] = f[f'{var}'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c49051f-0be9-4ca9-91e1-28f2596d2bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#READING BACK IN\n",
    "\n",
    "types = [\"ALL\", \"SHALLOW\", \"DEEP\"]\n",
    "variables = [\"W\", \"QV\", \"QCQI\", \"TH\", \"TH_E\", \"BUOYANCY\", \"HMC\"]\n",
    "\n",
    "vars_list = []\n",
    "SE_list = []\n",
    "\n",
    "for t in types:\n",
    "    for var in variables:\n",
    "        vars_list.append(f\"ColdPool_{t}_profile_array_{var.lower()}\")\n",
    "for t in types:\n",
    "    for var in variables:\n",
    "        SE_list.append(f\"ColdPool_{t}_SE_{var.lower()}\")\n",
    "        \n",
    "# Define directory and output file path\n",
    "dir2=dir+'Project_Algorithms/Tracked_Profiles/job_out2/'\n",
    "output_file=dir2+f\"ColdPool_tracked_profiles_SE_{res}_{t_res}_{Np_str}.h5\"\n",
    "\n",
    "# Open the HDF5 file and read the stored datasets into dynamically named variables\n",
    "with h5py.File(output_file, 'r') as f:\n",
    "    for var in vars_list + SE_list:\n",
    "        globals()[var] = f[f'{var}'][:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
