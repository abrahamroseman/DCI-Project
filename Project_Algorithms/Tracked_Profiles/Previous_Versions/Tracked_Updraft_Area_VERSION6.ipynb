{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48931aee-7865-4438-b30a-6b757a05e2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS FUNCTION IS FOR RUNNING WITH SLURM JOB ARRAY\n",
    "#(SPLITS UP JOB_ARRAY BELOW INTO EVEN MORE TASKS)\n",
    "def StartSlurmJobArray(num_jobs,num_slurm_jobs, ISRUN):\n",
    "    job_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0)) #this is the current SBATCH job id\n",
    "    if job_id==0: job_id=1\n",
    "    if ISRUN==False:\n",
    "        start_job=1;end_job=num_jobs+1\n",
    "        return start_job,end_job\n",
    "    total_elements=num_jobs #total num of variables\n",
    "\n",
    "    job_range = total_elements // num_slurm_jobs  # Base size for each chunk\n",
    "    remaining = total_elements % num_slurm_jobs   # Number of chunks with 1 extra \n",
    "    \n",
    "    # Function to compute the start and end for each job_id\n",
    "    def get_job_range(job_id, num_slurm_jobs):\n",
    "        job_id-=1\n",
    "        # Add one extra element to the first 'remaining' chunks\n",
    "        start_job = job_id * job_range + min(job_id, remaining)\n",
    "        end_job = start_job + job_range + (1 if job_id < remaining else 0)\n",
    "    \n",
    "        if job_id == num_slurm_jobs - 1: \n",
    "            end_job = total_elements \n",
    "        return start_job, end_job\n",
    "    # def job_testing():\n",
    "    #     #TESTING\n",
    "    #     start=[];end=[]\n",
    "    #     for job_id in range(1,num_slurm_jobs+1):\n",
    "    #         start_job, end_job = get_job_range(job_id)\n",
    "    #         print(start_job,end_job)\n",
    "    #         start.append(start_job)\n",
    "    #         end.append(end_job)\n",
    "    #     print(np.all(start!=end))\n",
    "    #     print(len(np.unique(start))==len(start))\n",
    "    #     print(len(np.unique(end))==len(end))\n",
    "    # job_testing()\n",
    "\n",
    "    # if sbatch==True:\n",
    "        \n",
    "    start_job, end_job = get_job_range(job_id, num_slurm_jobs)\n",
    "    index_adjust=start_job\n",
    "    # print(f'start_job = {start_job}, end_job = {end_job}')\n",
    "    if start_job==0: start_job=1\n",
    "    if end_job==total_elements: end_job+=1\n",
    "    return start_job,end_job\n",
    "\n",
    "# job_id=1\n",
    "# [start_slurm_job,end_slurm_job,slurm_index_adjust]=StartSlurmJobArray(num_jobs,num_slurm_jobs,ISRUN)\n",
    "# parcel=parcel1.isel(xh=slice(start_job,end_job))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08571ff-cec1-44ab-af63-8ef778b199b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in Packages and Data\n",
    "\n",
    "#Importing Packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import matplotlib.gridspec as gridspec\n",
    "import xarray as xr\n",
    "import os; import time\n",
    "import pickle\n",
    "import h5py\n",
    "###############################################################\n",
    "def coefs(coefficients,degree):\n",
    "    coef=coefficients\n",
    "    coefs=\"\"\n",
    "    for n in range(degree, -1, -1):\n",
    "        string=f\"({coefficients[len(coef)-(n+1)]:.1e})\"\n",
    "        coefs+=string + f\"x^{n}\"\n",
    "        if n != 0:\n",
    "            coefs+=\" + \"\n",
    "    return coefs\n",
    "###############################################################\n",
    "\n",
    "# Importing Model Data\n",
    "check=False\n",
    "dir='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "\n",
    "# # dx = 1 km; Np = 1M; Nt = 5 min\n",
    "# data1=xr.open_dataset(dir+'../cm1r20.3/run/cm1out_1km_5min.nc', decode_timedelta=True) #***\n",
    "# parcel1=xr.open_dataset(dir+'../cm1r20.3/run/cm1out_pdata_1km_5min_1e6.nc', decode_timedelta=True) #***\n",
    "# res='1km';t_res='5min'\n",
    "# Np_str='1e6'\n",
    "\n",
    "# # dx = 1km; Np = 50M\n",
    "# #Importing Model Data\n",
    "# dir2='/home/air673/koa_scratch/'\n",
    "# data1=xr.open_dataset(dir2+'cm1out_1km_1min.nc', decode_timedelta=True) #***\n",
    "# parcel1=xr.open_dataset(dir2+'cm1out_pdata_1km_1min_50M.nc', decode_timedelta=True) #***\n",
    "# res='1km'; t_res='1min'; Np_str='50e6'\n",
    "\n",
    "# # dx = 1km; Np = 50M; Nz = 95\n",
    "# #Importing Model Data\n",
    "# dir2='/home/air673/koa_scratch/'\n",
    "# data1=xr.open_dataset(dir2+'cm1out_1km_1min_95nz.nc', decode_timedelta=True) #***\n",
    "# parcel1=xr.open_dataset(dir2+'cm1out_pdata_1km_1min_95nz.nc', decode_timedelta=True) #***\n",
    "# res='1km'; t_res='1min_95nz'; Np_str='50e6'\n",
    "\n",
    "# dx = 250m; Np = 50M\n",
    "#Importing Model Data\n",
    "dir2='/home/air673/koa_scratch/'\n",
    "data1=xr.open_dataset(dir2+'cm1out_250m_1min_50M.nc', decode_timedelta=True) #***\n",
    "parcel1=xr.open_dataset(dir2+'cm1out_pdata_250m_1min_50M.nc', decode_timedelta=True) #***\n",
    "res='250m'; t_res='1min'; Np_str='50e6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f63e97-44fa-4e1a-9b43-fab37ba7af4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "times=data1['time'].values/(1e9 * 60); times=times.astype(float);\n",
    "minutes=1/times[1] #1 / minutes per timestep = timesteps per minute\n",
    "kms=np.argmax(data1['xh'].values-data1['xh'][0].values >= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d7bb20-ab7d-47df-840e-e29a6b2fe1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "dir2='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "path=dir2+'../Functions/'\n",
    "sys.path.append(path)\n",
    "\n",
    "import NumericalFunctions\n",
    "from NumericalFunctions import * # import NumericalFunctions \n",
    "import PlottingFunctions\n",
    "from PlottingFunctions import * # import PlottingFunctions\n",
    "\n",
    "\n",
    "# # Get all functions in NumericalFunctions\n",
    "# import inspect\n",
    "# functions = [f[0] for f in inspect.getmembers(NumericalFunctions, inspect.isfunction)]\n",
    "# functions\n",
    "\n",
    "# # Get all functions in NumericalFunctions\n",
    "# import inspect\n",
    "# functions = [f[0] for f in inspect.getmembers(PlottingFunctions, inspect.isfunction)]\n",
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a958909a-7bff-40d5-93b1-8bec2cd18a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#READING BACK IN\n",
    "########################\n",
    "def LoadFinalData(in_file):\n",
    "    dict = {}\n",
    "    with h5py.File(in_file, 'r') as f:\n",
    "        for key in f.keys():\n",
    "            dict[key] = f[key][:]\n",
    "    return dict\n",
    "\n",
    "def LoadAllCloudBase():\n",
    "    dir2 = dir + f'Project_Algorithms/Tracking_Algorithms/'\n",
    "    in_file = dir2 + f\"all_cloudbase_{res}_{t_res}_{Np_str}.pkl\"\n",
    "    with open(in_file, 'rb') as f:\n",
    "        all_cloudbase = pickle.load(f)\n",
    "    return(all_cloudbase)\n",
    "min_all_cloudbase=np.nanmin(LoadAllCloudBase())\n",
    "print(f\"Minimum Cloudbase is: {min_all_cloudbase*1000:.0f} m\")\n",
    "\n",
    "def LoadMeanLFC():\n",
    "    dir2 = dir + f'Project_Algorithms/Tracking_Algorithms/'\n",
    "    in_file = dir2 + f\"MeanLFC_{res}_{t_res}_{Np_str}.pkl\"\n",
    "    with open(in_file, 'rb') as f:\n",
    "        MeanLFC = pickle.load(f)\n",
    "    return MeanLFC\n",
    "MeanLFC=LoadMeanLFC()\n",
    "print(f\"Mean LFC is: {MeanLFC:.0f} m\\n\")\n",
    "\n",
    "dir2 = dir + f'Project_Algorithms/Tracking_Algorithms/'\n",
    "in_file=dir2+f\"parcel_tracking_SUBSET_{res}_{t_res}_{Np_str}.h5\"\n",
    "final_dict=LoadFinalData(in_file)\n",
    "\n",
    "\n",
    "#DYNAMICALLY CREATING VARIABLES\n",
    "for key, value in final_dict.items():\n",
    "    globals()[key] = value\n",
    "\n",
    "# #DYNAMICALLY PRINTING VARIABLE SIZES\n",
    "# for key in final_dict:\n",
    "#     print(f\"{key} has {final_dict[key].shape[0]} parcels\")\n",
    "\n",
    "# PRINTING VARIABLE SIZES (ONE BY ONE)\n",
    "print(f'ALL: {len(CL_ALL_out_arr)} CL parcels and {len(nonCL_ALL_out_arr)} nonCL parcels')\n",
    "print(f'SHALLOW: {len(CL_SHALLOW_out_arr)} CL parcels and {len(nonCL_SHALLOW_out_arr)} nonCL parcels')\n",
    "print(f'DEEP: {len(CL_DEEP_out_arr)} CL parcels and {len(nonCL_DEEP_out_arr)} nonCL parcels')\n",
    "print('\\n')\n",
    "print(f'ALL: {len(SBZ_ALL_out_arr)} SBZ parcels and {len(nonSBZ_ALL_out_arr)} nonSBZ parcels')\n",
    "print(f'SHALLOW: {len(SBZ_SHALLOW_out_arr)} SBZ parcels and {len(nonSBZ_SHALLOW_out_arr)} nonSBZ parcels')\n",
    "print(f'DEEP: {len(SBZ_DEEP_out_arr)} SBZ parcels and {len(nonSBZ_DEEP_out_arr)} nonSBZ parcels')\n",
    "print('\\n')\n",
    "print(f'ALL: {len(ColdPool_ALL_out_arr)} ColdPool parcels')\n",
    "print(f'SHALLOW: {len(ColdPool_SHALLOW_out_arr)} ColdPool parcels')\n",
    "print(f'DEEP: {len(ColdPool_DEEP_out_arr)} ColdPool parcels')\n",
    "\n",
    "\n",
    "def apply_job_array_filter(start_job, end_job):\n",
    "    # print(\"APPLYING JOB ARRAY\")\n",
    "\n",
    "    def job_filter(arr):\n",
    "        return arr[(arr[:, 0] >= start_job) & (arr[:, 0] < end_job)]\n",
    "\n",
    "    target_names = [\n",
    "        'CL_ALL_out_arr', 'nonCL_ALL_out_arr',\n",
    "        'CL_SHALLOW_out_arr', 'nonCL_SHALLOW_out_arr',\n",
    "        'CL_DEEP_out_arr', 'nonCL_DEEP_out_arr',\n",
    "        'SBZ_ALL_out_arr', 'nonSBZ_ALL_out_arr',\n",
    "        'SBZ_SHALLOW_out_arr', 'nonSBZ_SHALLOW_out_arr',\n",
    "        'SBZ_DEEP_out_arr', 'nonSBZ_DEEP_out_arr',\n",
    "        'ColdPool_ALL_out_arr', 'ColdPool_SHALLOW_out_arr', 'ColdPool_DEEP_out_arr'\n",
    "    ]\n",
    "\n",
    "    for name in target_names:\n",
    "        globals()[name+'_2'] = job_filter(globals()[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfda27a-7afd-488e-8078-58ce5baa88dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "#SETUP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884712d5-d793-4614-9feb-2048fdd7230c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def MakeDictionary(**vars):\n",
    "#     return vars\n",
    "    \n",
    "# def GetData(start_job,end_job):\n",
    "#     dir2=dir+'Project_Algorithms/Lagrangian_Arrays/'\n",
    "#     in_file=dir2+f'lagrangian_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "    \n",
    "#     var_names = ['W', 'QCQI']\n",
    "#     data_dict = make_data_dict(in_file,var_names,read_type,start_job,end_job)\n",
    "#     W, QCQI = (data_dict[k] for k in var_names)\n",
    "    \n",
    "#     # #Making Time Matrix\n",
    "#     # rows, cols = A.shape[0], A.shape[1]\n",
    "#     # T = np.arange(rows).reshape(-1, 1) * np.ones((1, cols), dtype=int)\n",
    "\n",
    "#     return VARs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8d1915-861f-4ff7-a81d-a725b1f015e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "#DATA SETUP\n",
    "################################\n",
    "data_type=\"Tracked_Area\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388827fd-1d69-4ace-92ce-09c9cd499b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "#JOB ARRAY SETUP\n",
    "################################\n",
    "#*#*\n",
    "# how many total jobs are being run? i.e. array=1-100 ==> num_jobs=100\n",
    "if Np_str=='1e6':\n",
    "    num_jobs=180 #1M parcels\n",
    "    num_slurm_jobs=60\n",
    "if Np_str=='50e6':\n",
    "    num_jobs=2100 #50M parcels\n",
    "    num_slurm_jobs=300\n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe0e4ab-13c3-4416-b837-59c0628f5955",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "#DATA LOADING FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3773d22b-2e3e-4f5c-8d61-44ea190a40aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JOB ARRAY SETUP\n",
    "def StartJobArray(job_id,num_jobs):\n",
    "    total_elements=len(parcel1['xh']) #total num of variables\n",
    "\n",
    "    if num_jobs >= total_elements:\n",
    "        raise ValueError(\"Number of jobs cannot be greater than or equal to total elements.\")\n",
    "    \n",
    "    job_range = total_elements // num_jobs  # Base size for each chunk\n",
    "    remaining = total_elements % num_jobs   # Number of chunks with 1 extra \n",
    "    \n",
    "    # Function to compute the start and end for each job_id\n",
    "    def get_job_range(job_id, num_jobs):\n",
    "        job_id-=1\n",
    "        # Add one extra element to the first 'remaining' chunks\n",
    "        start_job = job_id * job_range + min(job_id, remaining)\n",
    "        end_job = start_job + job_range + (1 if job_id < remaining else 0)\n",
    "    \n",
    "        if job_id == num_jobs - 1: \n",
    "            end_job = total_elements #- 1\n",
    "        return start_job, end_job\n",
    "    # def job_testing():\n",
    "    #     #TESTING\n",
    "    #     start=[];end=[]\n",
    "    #     for job_id in range(1,num_jobs+1):\n",
    "    #         start_job, end_job = get_job_range(job_id)\n",
    "    #         print(start_job,end_job)\n",
    "    #         start.append(start_job)\n",
    "    #         end.append(end_job)\n",
    "    #     print(np.all(start!=end))\n",
    "    #     print(len(np.unique(start))==len(start))\n",
    "    #     print(len(np.unique(end))==len(end))\n",
    "    # job_testing()\n",
    "\n",
    "    # if sbatch==True:\n",
    "    #     job_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0)) #this is the current SBATCH job id\n",
    "    #     if job_id==0: job_id=1\n",
    "        \n",
    "    start_job, end_job = get_job_range(job_id, num_jobs)\n",
    "    index_adjust=start_job\n",
    "    # print(f'start_job = {start_job}, end_job = {end_job}')\n",
    "    return start_job,end_job,index_adjust\n",
    "\n",
    "# job_id=1\n",
    "# [start_job,end_job,index_adjust]=StartJobArray(job_id,num_jobs)\n",
    "# parcel=parcel1.isel(xh=slice(start_job,end_job))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa216524-1b26-47df-a396-ca8db4e831c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Back Data Later\n",
    "##############\n",
    "def make_data_dict(in_file,var_names,read_type,start_job,end_job):\n",
    "    if read_type=='h5py':\n",
    "        with h5py.File(in_file, 'r') as f:\n",
    "           data_dict = {var_name: f[var_name][:,start_job:end_job] for var_name in var_names}\n",
    "            \n",
    "    elif read_type=='xarray':\n",
    "        in_data = xr.open_dataset(\n",
    "            in_file,\n",
    "            engine='h5netcdf',\n",
    "            phony_dims='sort',\n",
    "            chunks={'phony_dim_0': 100, 'phony_dim_1': 1_000_000} \n",
    "        )\n",
    "        data_dict = {k: in_data[k][:,start_job:end_job].compute().data for k in var_names}\n",
    "    return data_dict\n",
    "\n",
    "# read_type='xarray'\n",
    "read_type='h5py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b34545-7be4-457f-ab43-1b49e6780c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetSpatialData(start_job,end_job):\n",
    "    dir2=dir+'Project_Algorithms/Lagrangian_Arrays/'\n",
    "    in_file=dir2+f'lagrangian_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "    \n",
    "    var_names = ['Z', 'Y', 'X', 'z']\n",
    "    data_dict = make_data_dict(in_file,var_names,read_type,start_job,end_job)\n",
    "    Z, Y, X, parcel_z = (data_dict[k] for k in var_names)\n",
    "    \n",
    "    # #Making Time Matrix\n",
    "    # rows, cols = A.shape[0], A.shape[1]\n",
    "    # T = np.arange(rows).reshape(-1, 1) * np.ones((1, cols), dtype=int)\n",
    "    \n",
    "    return Z,Y,X,parcel_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee62b8c1-d95b-4508-a9cb-0690fd15cef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ada8b8-0f8f-4691-9a38-81cfa4e9667d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "#MAKE PROFILES FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d2376d-45b5-4222-a4b2-0edc55620a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #CALL_VARIABLES FUNCTION\n",
    "def call_variables(t,z,y,x):\n",
    "    t = t if t is not None else slice(None)\n",
    "    z = z if z is not None else slice(None)\n",
    "    y = y if y is not None else slice(None)\n",
    "    x = x if x is not None else slice(None)\n",
    "    \n",
    "    variable='winterp'; w_data=data1[variable].isel(time=t,zh=z,yh=y,xh=x).data\n",
    "    variable='qc'; qc_data=data1[variable].isel(time=t,zh=z,yh=y,xh=x).data\n",
    "    variable='qi'; qi_data=data1[variable].isel(time=t,zh=z,yh=y,xh=x).data\n",
    "    qc_plus_qi=qc_data+qi_data\n",
    "    return w_data,qc_plus_qi\n",
    "\n",
    "# def call_variables1(data1,t): #TESTING METHOD 2 (SLOWER)\n",
    "#     variable='winterp'; w_data_t=data1[variable].isel(time=t).data\n",
    "#     variable='qc'; qc_data_t=data1[variable].isel(time=t).data\n",
    "#     variable='qi'; qi_data_t=data1[variable].isel(time=t).data\n",
    "#     return w_data_t,qc_data_t,qi_data_t\n",
    "\n",
    "# def call_variables2(w_data_t,qc_data_t,qi_data_t, z,y,x): #TESTING METHOD 2 (SLOWER)\n",
    "#     z = z if z is not None else slice(None)\n",
    "#     y = y if y is not None else slice(None)\n",
    "#     x = x if x is not None else slice(None)\n",
    "    \n",
    "#     variable='winterp'; w_data=w_data_t[z,y,x]\n",
    "#     variable='qc'; qc_data=qc_data_t[z,y,x]\n",
    "#     variable='qi'; qi_data=qi_data_t[z,y,x]\n",
    "#     qc_plus_qi=qc_data+qi_data\n",
    "#     return w_data,qc_plus_qi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1b566d-2bc6-433b-b418-ccd88b6a1418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetIndices(out_arr, Z, Y, X, index_adjust):\n",
    "    all_ts = []; all_zs = []; all_ys = []; all_xs = []\n",
    "\n",
    "    for row in range(out_arr.shape[0]):\n",
    "        after=out_arr[row,3]\n",
    "        if np.mod(row,50)==0: print(f'{row}/{out_arr.shape[0]}')\n",
    "        p=out_arr[row,0]\n",
    "        \n",
    "        # ts=np.arange(out_arr[row,4],out_arr[row,5]+1 + after)\n",
    "        ts_end = min(out_arr[row, 2] + 1 + after, len(data1['time'])) #this takes care of exceeding buffers\n",
    "        ts = np.arange(out_arr[row, 1], ts_end)\n",
    "        \n",
    "        zs=Z[ts,p-index_adjust] #JOBARRAY INDEXADJUST\n",
    "        ys=Y[ts,p-index_adjust] #JOBARRAY INDEXADJUST\n",
    "        xs=X[ts,p-index_adjust] #JOBARRAY INDEXADJUST\n",
    "        all_ts.extend(ts); all_zs.extend(zs); \n",
    "        all_ys.extend(ys);all_xs.extend(xs)\n",
    "\n",
    "    ################################################################################\n",
    "    # Convert to numpy arrays\n",
    "    all_ts = np.array(all_ts, dtype=int); all_zs = np.array(all_zs, dtype=int)\n",
    "    all_ys = np.array(all_ys, dtype=int); all_xs = np.array(all_xs, dtype=int)\n",
    "\n",
    "    # Sort by time\n",
    "    sort_order = np.argsort(all_ts)\n",
    "    ts_sorted = all_ts[sort_order]; zs_sorted = all_zs[sort_order]\n",
    "    ys_sorted = all_ys[sort_order]; xs_sorted = all_xs[sort_order]\n",
    "    return ts_sorted, zs_sorted, ys_sorted, xs_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58a9929-b339-4b27-8bea-e39e790f8c8b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #MAKING PROFILE ARRAY #METHOD 1 (DOUBLE FOR LOOP in p and (t,z,y,x)) (FASTEST CURRENTLY)\n",
    "\n",
    "# #thresholds\n",
    "# w_thresh1=0.1\n",
    "# w_thresh2=0.5\n",
    "# qcqi_thresh=1e-6\n",
    "\n",
    "# dx=(data1['xf'][1].item()-data1['xf'][0].item())# km *1e3 #meters\n",
    "# dy=(data1['yf'][1].item()-data1['yf'][0].item())# km *1e3 #meters\n",
    "\n",
    "# def TrackedProfile_AREA_VERSION1(Z,Y,X,type,type2,updraft_type):\n",
    "#     global index_adjust\n",
    "#     out_arr = globals()[f\"{type2}_{type.upper()}_out_arr\"+\"_2\"].copy()\n",
    "   \n",
    "#     zhs=data1['zh'].values\n",
    "#     profile_array =np.zeros((len(zhs), 3)) #column 1: var, column 2: counter, column 3: list of zhs\n",
    "#     profile_array[:,2]=zhs;\n",
    "\n",
    "#     #####\n",
    "#     zhs=data1['zh'].values\n",
    "#     profile_array_squares =np.zeros((len(zhs), 3)) #column 1: var, column 2: counter, column 3: list of zhs\n",
    "#     profile_array_squares[:,2]=zhs;\n",
    "#     #####\n",
    "    \n",
    "#     for row in range(out_arr.shape[0]):\n",
    "#         after=out_arr[row,3]\n",
    "#         if np.mod(row,50)==0: print(f'{row}/{out_arr.shape[0]}')\n",
    "#         p=out_arr[row,0]\n",
    "        \n",
    "#         # ts=np.arange(out_arr[row,4],out_arr[row,5]+1 + after)\n",
    "#         ts_end = min(out_arr[row, 2] + 1 + after, len(data1['time'])) #this takes care of exceeding buffers\n",
    "#         ts = np.arange(out_arr[row, 1], ts_end)\n",
    "        \n",
    "#         zs=Z[ts,p-index_adjust] #JOBARRAY INDEXADJUST\n",
    "#         ys=Y[ts,p-index_adjust] #JOBARRAY INDEXADJUST\n",
    "#         xs=X[ts,p-index_adjust] #JOBARRAY INDEXADJUST\n",
    "\n",
    "#         for t, z, y, x in zip(ts, zs, ys, xs):\n",
    "#             #CALCULATING AREA (CURRENTLY IGNORES CASES WHERE UPDRAFT CROSSES THE BOUNDARY)\n",
    "#             ########################################################\n",
    "            \n",
    "#             #FINDING XLENGTH\n",
    "#             ########################################################\n",
    "#             [x_slice,x_slice2]=call_variables(t,z,y,None)\n",
    "            \n",
    "#             if updraft_type=='general':\n",
    "#                 x_slice = np.where((x_slice >= w_thresh1) & (x_slice2 < qcqi_thresh), 1, 0)\n",
    "#             elif updraft_type=='cloudy':\n",
    "#                 x_slice = np.where((x_slice >= w_thresh2) & (x_slice2 >= qcqi_thresh), 1, 0)\n",
    "    \n",
    "#             if np.all(x_slice[x+1:]==1) or np.all(x_slice[:(x-1)+1]==1): #*AVOID BOUNDARY CASES*\n",
    "#                 continue\n",
    "#             x_left=np.where(x_slice[:(x)+1]==0)[0][-1]\n",
    "#             x_right=np.where(x_slice[x:]==0)[0][0]+(x)\n",
    "#             x_length=(x_right-x_left)*dx\n",
    "            \n",
    "#             #FINDING YLENGTH\n",
    "#             ########################################################\n",
    "#             [y_slice,y_slice2]=call_variables(t,z,None,x)\n",
    "            \n",
    "#             if updraft_type=='general':\n",
    "#                 y_slice = np.where((y_slice >= w_thresh1) & (y_slice2 < qcqi_thresh), 1, 0)\n",
    "#             elif updraft_type=='cloudy':\n",
    "#                 y_slice = np.where((y_slice >= w_thresh2) & (y_slice2 >= qcqi_thresh), 1, 0)\n",
    "    \n",
    "#             if np.all(y_slice[y+1:]==1) or np.all(y_slice[:(y-1)+1]==1): #*TO AVOID BOUNDARY CASES*\n",
    "#                 continue\n",
    "#             y_left=np.where(y_slice[:(y)+1]==0)[0][-1]\n",
    "#             y_right=np.where(y_slice[y:]==0)[0][0]+(y)\n",
    "#             y_length=(y_right-y_left)*dy\n",
    "\n",
    "#             #CALCULATING AREA\n",
    "#             ########################################################\n",
    "#             AREA=x_length*y_length #square area approximation \n",
    "#             AREA*=np.pi/4 #include for oval area approximation\n",
    "\n",
    "#             #ADDING TO PROFILE\n",
    "#             ########################################################\n",
    "#             var=AREA\n",
    "#             profile_array[z,0]+=var;profile_array[z,1]+=1\n",
    "#             profile_array_squares[z,0]+=var**2;profile_array_squares[z,1]+=1\n",
    "#     return profile_array,profile_array_squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a0d79a-2ba9-44e5-94aa-fc3f5c756fee",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#MAKING PROFILE ARRAY #METHOD 2 (SINGLE FOR LOOP in (t,z,y,x)) (ONLY SLIGHLY SLOWER, BUT MAY BE FASTER IN THE LONG RUN)\n",
    "#(SLIGHTLY DIFFERENT RESULTS FOR SOME Z LEVELS (count is the same), ORDER OF 1e-14, CAUSED BY SOME SUPER SLIGHT NUMERICAL DIFFERENCE FROM COMBINING FOR LOOP()\n",
    "\n",
    "#thresholds\n",
    "w_thresh1=0.1\n",
    "w_thresh2=0.5\n",
    "qcqi_thresh=1e-6\n",
    "\n",
    "dx=(data1['xf'][1].item()-data1['xf'][0].item())# km *1e3 #meters\n",
    "dy=(data1['yf'][1].item()-data1['yf'][0].item())# km *1e3 #meters\n",
    "\n",
    "def TrackedProfile_AREA_VERSION2(Z,Y,X,type,type2,updraft_type):\n",
    "    global index_adjust\n",
    "    out_arr = globals()[f\"{type2}_{type.upper()}_out_arr\"+\"_2\"].copy()\n",
    "   \n",
    "    zhs=data1['zh'].values\n",
    "    profile_array =np.zeros((len(zhs), 3)) #column 1: var, column 2: counter, column 3: list of zhs\n",
    "    profile_array[:,2]=zhs;\n",
    "\n",
    "    #####\n",
    "    zhs=data1['zh'].values\n",
    "    profile_array_squares =np.zeros((len(zhs), 3)) #column 1: var, column 2: counter, column 3: list of zhs\n",
    "    profile_array_squares[:,2]=zhs;\n",
    "    #####\n",
    "\n",
    "    ts,zs,ys,xs=GetIndices(out_arr,Z,Y,X,index_adjust)\n",
    "    for t, z, y, x in zip(ts, zs, ys, xs):\n",
    "        if t % 5 == 25: print(f\"{t}/{len(ts)}\")\n",
    "        #CALCULATING AREA (CURRENTLY IGNORES CASES WHERE UPDRAFT CROSSES THE BOUNDARY)\n",
    "        ########################################################\n",
    "        \n",
    "        #FINDING XLENGTH\n",
    "        ########################################################\n",
    "        [x_slice,x_slice2]=call_variables(t,z,y,None)\n",
    "        \n",
    "        if updraft_type=='general':\n",
    "            x_slice = np.where((x_slice >= w_thresh1) & (x_slice2 < qcqi_thresh), 1, 0)\n",
    "        elif updraft_type=='cloudy':\n",
    "            x_slice = np.where((x_slice >= w_thresh2) & (x_slice2 >= qcqi_thresh), 1, 0)\n",
    "\n",
    "        if np.all(x_slice[x+1:]==1) or np.all(x_slice[:(x-1)+1]==1): #*AVOID BOUNDARY CASES*\n",
    "            continue\n",
    "        x_left=np.where(x_slice[:(x)+1]==0)[0][-1]\n",
    "        x_right=np.where(x_slice[x:]==0)[0][0]+(x)\n",
    "        x_length=(x_right-x_left)*dx\n",
    "        \n",
    "        #FINDING YLENGTH\n",
    "        ########################################################\n",
    "        [y_slice,y_slice2]=call_variables(t,z,None,x)\n",
    "        \n",
    "        if updraft_type=='general':\n",
    "            y_slice = np.where((y_slice >= w_thresh1) & (y_slice2 < qcqi_thresh), 1, 0)\n",
    "        elif updraft_type=='cloudy':\n",
    "            y_slice = np.where((y_slice >= w_thresh2) & (y_slice2 >= qcqi_thresh), 1, 0)\n",
    "\n",
    "        if np.all(y_slice[y+1:]==1) or np.all(y_slice[:(y-1)+1]==1): #*TO AVOID BOUNDARY CASES*\n",
    "            continue\n",
    "        y_left=np.where(y_slice[:(y)+1]==0)[0][-1]\n",
    "        y_right=np.where(y_slice[y:]==0)[0][0]+(y)\n",
    "        y_length=(y_right-y_left)*dy\n",
    "\n",
    "        #CALCULATING AREA\n",
    "        ########################################################\n",
    "        AREA=x_length*y_length #square area approximation \n",
    "        AREA*=np.pi/4 #include for oval area approximation\n",
    "\n",
    "        #ADDING TO PROFILE\n",
    "        ########################################################\n",
    "        var=AREA\n",
    "        profile_array[z,0]+=var;profile_array[z,1]+=1\n",
    "        profile_array_squares[z,0]+=var**2;profile_array_squares[z,1]+=1\n",
    "    return profile_array,profile_array_squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78b6a8d-8281-473b-b637-478372594d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #MAKING PROFILE ARRAY #TESTING METHOD 3 (DOUBLE FOR LOOP in t_unique and (z,y,x)) (SLOWEST, LIKELY NOT IN THE LONG RUN EITHER)\n",
    "\n",
    "# #thresholds\n",
    "# w_thresh1=0.1\n",
    "# w_thresh2=0.5\n",
    "# qcqi_thresh=1e-6\n",
    "\n",
    "# dx=(data1['xf'][1].item()-data1['xf'][0].item())# km *1e3 #meters\n",
    "# dy=(data1['yf'][1].item()-data1['yf'][0].item())# km *1e3 #meters\n",
    "\n",
    "# def TrackedProfile_AREA(Z,Y,X,type,type2,updraft_type):\n",
    "#     global index_adjust\n",
    "#     out_arr = globals()[f\"{type2}_{type.upper()}_out_arr\"+\"_2\"].copy()\n",
    "   \n",
    "#     zhs=data1['zh'].values\n",
    "#     profile_array =np.zeros((len(zhs), 3)) #column 1: var, column 2: counter, column 3: list of zhs\n",
    "#     profile_array[:,2]=zhs;\n",
    "#     #####\n",
    "#     zhs=data1['zh'].values\n",
    "#     profile_array_squares =np.zeros((len(zhs), 3)) #column 1: var, column 2: counter, column 3: list of zhs\n",
    "#     profile_array_squares[:,2]=zhs;\n",
    "#     #####\n",
    "\n",
    "#     ts,zs,ys,xs=GetIndices(out_arr,Z,Y,X,index_adjust) #GET ALL INDICES\n",
    "#     for count,t in enumerate(np.unique(ts)): #LOOP THROUGH T OF UNIQUE T INDICES\n",
    "#         if count % 30 == 0: print(f\"{count}/{len(np.unique(ts))}\")\n",
    "#         w_data_t,qc_data_t,qi_data_t=call_variables1(data1,t) #CALL IN DATA AT EACH TIME\n",
    "#         t_indices=np.where(ts==t); z_indices=zs[t_indices];y_indices=ys[t_indices];x_indices=xs[t_indices] #GET Z,Y,X, INDICIES FOR SPECIFIC TIME\n",
    "#         for count, (z, y, x) in enumerate(zip(z_indices, y_indices, x_indices)):\n",
    "#             #CALCULATING AREA (CURRENTLY IGNORES CASES WHERE UPDRAFT CROSSES THE BOUNDARY)\n",
    "#             ########################################################\n",
    "            \n",
    "#             #FINDING XLENGTH\n",
    "#             ########################################################\n",
    "#             [x_slice,x_slice2]=call_variables2(w_data_t,qc_data_t,qi_data_t, z,y,None) #*#*#*#\n",
    "            \n",
    "#             if updraft_type=='general':\n",
    "#                 x_slice = np.where((x_slice >= w_thresh1) & (x_slice2 < qcqi_thresh), 1, 0)\n",
    "#             elif updraft_type=='cloudy':\n",
    "#                 x_slice = np.where((x_slice >= w_thresh2) & (x_slice2 >= qcqi_thresh), 1, 0)\n",
    "    \n",
    "#             if np.all(x_slice[x+1:]==1) or np.all(x_slice[:(x-1)+1]==1): #*AVOID BOUNDARY CASES*\n",
    "#                 continue\n",
    "#             x_left=np.where(x_slice[:(x)+1]==0)[0][-1]\n",
    "#             x_right=np.where(x_slice[x:]==0)[0][0]+(x)\n",
    "#             x_length=(x_right-x_left)*dx\n",
    "            \n",
    "#             #FINDING YLENGTH\n",
    "#             ########################################################\n",
    "#             [y_slice,y_slice2]=call_variables2(w_data_t,qc_data_t,qi_data_t, z,None,x)\n",
    "            \n",
    "#             if updraft_type=='general':\n",
    "#                 y_slice = np.where((y_slice >= w_thresh1) & (y_slice2 < qcqi_thresh), 1, 0)\n",
    "#             elif updraft_type=='cloudy':\n",
    "#                 y_slice = np.where((y_slice >= w_thresh2) & (y_slice2 >= qcqi_thresh), 1, 0)\n",
    "    \n",
    "#             if np.all(y_slice[y+1:]==1) or np.all(y_slice[:(y-1)+1]==1): #*TO AVOID BOUNDARY CASES*\n",
    "#                 continue\n",
    "#             y_left=np.where(y_slice[:(y)+1]==0)[0][-1]\n",
    "#             y_right=np.where(y_slice[y:]==0)[0][0]+(y)\n",
    "#             y_length=(y_right-y_left)*dy\n",
    "    \n",
    "#             #CALCULATING AREA\n",
    "#             ########################################################\n",
    "#             AREA=x_length*y_length #square area approximation \n",
    "#             AREA*=np.pi/4 #include for oval area approximation\n",
    "    \n",
    "#             #ADDING TO PROFILE\n",
    "#             ########################################################\n",
    "#             var=AREA\n",
    "#             profile_array[z,0]+=var;profile_array[z,1]+=1\n",
    "#             profile_array_squares[z,0]+=var**2;profile_array_squares[z,1]+=1\n",
    "#     return profile_array,profile_array_squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768cf557-dad5-4e19-afcb-2eed36654cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunCalculation(Z,Y,X):\n",
    "    type1_options = ['all', 'shallow', 'deep']\n",
    "    type2_options = ['CL', 'nonCL', 'SBZ', 'nonSBZ', 'ColdPool']\n",
    "    updraft_types = ['general', 'cloudy']\n",
    "\n",
    "    Dictionary = {}\n",
    "\n",
    "    for t1 in type1_options:\n",
    "        print(f'{t1.upper()}')\n",
    "        for t2 in type2_options:\n",
    "            for utype in updraft_types:\n",
    "                key = f\"{t2}_{t1.upper()}_profile_array_{utype}\"\n",
    "                key_squares = f\"{t2}_{t1.upper()}_profile_array_{utype}_squares\"\n",
    "                # [Dictionary[key],Dictionary[key_squares]] = TrackedProfile_AREA_VERSION1(Z, Y, X, type=t1, type2=t2, updraft_type=utype) #VERSION 1\n",
    "                [Dictionary[key],Dictionary[key_squares]] = TrackedProfile_AREA_VERSION2(Z, Y, X, type=t1, type2=t2, updraft_type=utype)   #VERSION 2\n",
    "\n",
    "    return Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8679111d-b454-44ba-8d69-ab2886465fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveProfile(Dictionary,data_type):\n",
    "    # print(\"Saving all profiles...\")\n",
    "\n",
    "    # Construct the output file path\n",
    "    dir2 = dir + 'Project_Algorithms/Tracked_Profiles/job_out2/'\n",
    "    output_file = dir2 + f\"{data_type}_\" + f\"tracked_profiles_{res}_{t_res}_{Np_str}\"\n",
    "    output_file += f\"_{job_id}.h5\"\n",
    "\n",
    "    # Write the entire dictionary to HDF5\n",
    "    with h5py.File(output_file, \"w\") as h5f:\n",
    "        for key, profile_data in Dictionary.items():\n",
    "            h5f.create_dataset(key, data=profile_data)\n",
    "            \n",
    "    # print(\"Done saving.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c97cc1-91ab-4402-b37e-e208b91cdffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "#RUNNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98ab8f3-943c-4666-91bf-b4316d6209a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[start_slurm_job,end_slurm_job]=StartSlurmJobArray(num_jobs=num_jobs,num_slurm_jobs=num_slurm_jobs,ISRUN=True) #if ISRUN is False, then will not run using slurm_job_array\n",
    "\n",
    "print(f\"Running on Slurm_Jobs for Slurm_Job_Ids: {(start_slurm_job,end_slurm_job)}\")\n",
    "\n",
    "job_id_list=np.arange(start_slurm_job,end_slurm_job)\n",
    "for job_id in job_id_list:\n",
    "    if job_id % 1 == 0: print(f'current job_id = {job_id}\\n')\n",
    "    [start_job,end_job,index_adjust] = StartJobArray(job_id,num_jobs)\n",
    "    print(f\"Running Job_Array ID = {job_id}: {(start_job,end_job)}\")\n",
    "\n",
    "    #SLICING DATA\n",
    "    parcel=parcel1.isel(xh=slice(start_job,end_job))\n",
    "    apply_job_array_filter(start_job, end_job)\n",
    "\n",
    "    #GETTING DATA AND PUTTING IN A DICTIONARY\n",
    "    [Z,Y,X,parcel_z] = GetSpatialData(start_job,end_job) \n",
    "\n",
    "    #CALCULATING AND SAVING\n",
    "    Dictionary=RunCalculation(Z,Y,X) #VERSION1: EACH VARIABLE SEPERATELY\n",
    "    SaveProfile(Dictionary,data_type)\n",
    "\n",
    "    #check_memory(globals())\n",
    "    del Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172d58f5-526c-49cb-9751-a18ce59b6ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "#RECOMBINE SEPERATE JOB_ARRAYS AFTER\n",
    "recombine=False #KEEP FALSE WHEN JOBARRAY IS RUNNING\n",
    "# recombine=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379cd931-4e79-41b2-8cb9-46f142cd36be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetInputFile(data_type,job_id,dir2):\n",
    "    #CALLING IN DATA\n",
    "    input_file=dir2 + f\"{data_type}_\" + f\"tracked_profiles_{res}_{t_res}_{Np_str}_{job_id}.h5\"\n",
    "    return input_file\n",
    "def GetOutputFile(data_type,type1,type2,dir3):\n",
    "    if type2==None:\n",
    "        output_file=dir3 + f\"{data_type}_\" + f\"{type1}_tracked_profiles_{res}_{t_res}_{Np_str}.h5\"\n",
    "    else:\n",
    "        output_file=dir3 + f\"{data_type}_\" + f\"{type1}_{type2}_tracked_profiles_{res}_{t_res}_{Np_str}.h5\"\n",
    "    return output_file\n",
    "def Recombine(type1,type2,num_jobs):\n",
    "    global variables\n",
    "    print(f'recombining {type1}_{type2}')\n",
    "    if type2==None:\n",
    "        type2s = [f\"{type1}\"]\n",
    "    else:\n",
    "        type2s = [f\"{type1}\",f\"{type2}\"]\n",
    "    types = [\"ALL\", \"SHALLOW\", \"DEEP\"]\n",
    "\n",
    "    dir2=dir+'Project_Algorithms/Tracked_Profiles/job_out2/'\n",
    "    dir3=dir+'Project_Algorithms/Tracked_Profiles/OUTPUT_FILES/'\n",
    "    \n",
    "    #MAKING OUTPUT FILE PATH\n",
    "    output_file=GetOutputFile(data_type,type1,type2,dir3)\n",
    "\n",
    "    job_id=1; input_file=GetInputFile(data_type,job_id,dir2)\n",
    "    with h5py.File(input_file, 'r') as f:\n",
    "        all_keys = list(f.keys())\n",
    "        vars_list = [k for k in all_keys if any(t2 in k for t2 in type2s)]\n",
    "\n",
    "    \n",
    "    #MAKING PROFILES DICTIONARY\n",
    "    zhs = data1['zh'].values\n",
    "    profiles = {}  # Store profiles for all variables\n",
    "    for var in vars_list:\n",
    "        profiles[var] = np.zeros((len(zhs), 3))  # column 1: var, column 2: counter, column 3: list of zhs\n",
    "        profiles[var][:, 2] = zhs \n",
    "\n",
    "    for job_id in np.arange(1,num_jobs+1):\n",
    "        if np.mod(job_id,20)==0: print(f\"job_id = {job_id}\")\n",
    "    \n",
    "        #CALLING IN DATA\n",
    "        input_file=GetInputFile(data_type,job_id,dir2)\n",
    "    \n",
    "        #COMPILING PROFILES\n",
    "        with h5py.File(input_file, 'r') as f:\n",
    "            for var in vars_list:\n",
    "                profiles[var][:,0:1+1]+=f[f'{var}'][:,0:1+1]\n",
    "    \n",
    "    #SAVING INTO FINAL FORM\n",
    "    with h5py.File(output_file, 'w') as f:\n",
    "        for var in profiles:\n",
    "            profile_var = profiles[var]\n",
    "            f.create_dataset(f'{var}', data=profile_var, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e375acc2-c018-4d71-9b1c-fb714032f161",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if recombine==True:\n",
    "    Recombine(type1='CL',type2='nonCL',num_jobs=num_jobs)\n",
    "    Recombine(type1='SBZ',type2='nonSBZ',num_jobs=num_jobs)\n",
    "    Recombine(type1='ColdPool',type2=None,num_jobs=num_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48b7d9c-ed37-419b-9d42-ed73e6c1bda6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78038cfe-e27b-4aef-9afa-0ab46c7163a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066415cf-cd51-4763-af24-054c536ad946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540f62a1-4e21-472a-9022-da68ca2c98b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# #TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34449139-a9d4-4214-afc6-63df2f5a2749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_id=50\n",
    "# type1='CL';type2='nonCL'\n",
    "# dir2 = dir + 'Project_Algorithms/Tracked_Profiles/job_out2/'\n",
    "# output_file = dir2 + f\"{data_type}_\" + f\"tracked_profiles_{res}_{t_res}_{Np_str}\"\n",
    "# output_file += f\"_{job_id}.h5\"\n",
    "# key_list=[]\n",
    "# with h5py.File(output_file, 'r') as h5f:\n",
    "#     for key in h5f.keys():\n",
    "#         # one=h5f[\"CL_ALL_profile_array_cloudy\"][:]\n",
    "#         two=h5f[\"CL_ALL_profile_array_cloudy\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4061f279-9b14-4898-8e78-7a9c5c931cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(one==two)\n",
    "# print(one[:,0]-two[:,0])\n",
    "# plt.plot(two[:,0],two[:,2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
