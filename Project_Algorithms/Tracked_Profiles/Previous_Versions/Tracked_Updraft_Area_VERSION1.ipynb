{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48931aee-7865-4438-b30a-6b757a05e2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in Packages and Data\n",
    "\n",
    "#Importing Packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import matplotlib.gridspec as gridspec\n",
    "import xarray as xr\n",
    "import os; import time\n",
    "import pickle\n",
    "import h5py\n",
    "###############################################################\n",
    "def coefs(coefficients,degree):\n",
    "    coef=coefficients\n",
    "    coefs=\"\"\n",
    "    for n in range(degree, -1, -1):\n",
    "        string=f\"({coefficients[len(coef)-(n+1)]:.1e})\"\n",
    "        coefs+=string + f\"x^{n}\"\n",
    "        if n != 0:\n",
    "            coefs+=\" + \"\n",
    "    return coefs\n",
    "###############################################################\n",
    "\n",
    "# Importing Model Data\n",
    "check=False\n",
    "dir='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "\n",
    "# # dx = 1 km; Np = 1M; Nt = 5 min\n",
    "# data=xr.open_dataset(dir+'../cm1r20.3/run/cm1out_1km_5min.nc') #***\n",
    "# parcel=xr.open_dataset(dir+'../cm1r20.3/run/cm1out_pdata_1km_5min_1e6.nc') #***\n",
    "# res='1km';t_res='5min'\n",
    "# Np_str='1e6'\n",
    "\n",
    "# dx = 1km; Np = 50M\n",
    "#Importing Model Data\n",
    "check=False\n",
    "dir2='/home/air673/koa_scratch/'\n",
    "data=xr.open_dataset(dir2+'cm1out_1km_1min.nc') #***\n",
    "parcel=xr.open_dataset(dir2+'cm1out_pdata_1km_1min_50M.nc') #***\n",
    "res='1km'; t_res='1min'; Np_str='50e6'\n",
    "\n",
    "# # dx = 1km; Np = 100M\n",
    "# #Importing Model Data\n",
    "# check=False\n",
    "# dir2='/home/air673/koa_scratch/'\n",
    "# data=xr.open_dataset(dir2+'cm1out_1km_1min.nc') #***\n",
    "# parcel=xr.open_dataset(dir2+'cm1out_pdata_1km_1min_100M.nc') #***\n",
    "# res='1km'; t_res='1min'; Np_str='100e6'\n",
    "\n",
    "# #uncomment if using 250m data\n",
    "# #Importing Model Data\n",
    "# check=False\n",
    "# dir2='/home/air673/koa_scratch/'\n",
    "# data=xr.open_dataset(dir2+'cm1out_250m.nc') #***\n",
    "# # # parcel=xr.open_dataset(dir2+'cm1out_pdata_250m.nc') #***\n",
    "\n",
    "# # Restricts the timesteps of the data from timesteps0 to 140\n",
    "# data=data.isel(time=np.arange(0,400+1))\n",
    "# # # parcel=parcel.isel(time=np.arange(0,400+1))\n",
    "# res='250m'\n",
    "\n",
    "times=data['time'].values/(1e9 * 60); times=times.astype(float);\n",
    "minutes=1/times[1] #1 / minutes per timestep = timesteps per minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d7bb20-ab7d-47df-840e-e29a6b2fe1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "dir2='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "path=dir2+'../Functions/'\n",
    "sys.path.append(path)\n",
    "\n",
    "import NumericalFunctions\n",
    "from NumericalFunctions import * # import NumericalFunctions \n",
    "import PlottingFunctions\n",
    "from PlottingFunctions import * # import PlottingFunctions\n",
    "\n",
    "\n",
    "# # Get all functions in NumericalFunctions\n",
    "# import inspect\n",
    "# functions = [f[0] for f in inspect.getmembers(NumericalFunctions, inspect.isfunction)]\n",
    "# functions\n",
    "\n",
    "# # Get all functions in NumericalFunctions\n",
    "# import inspect\n",
    "# functions = [f[0] for f in inspect.getmembers(PlottingFunctions, inspect.isfunction)]\n",
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d721dd13-2db8-4e49-a04d-6cd94ad20d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_memory():\n",
    "    import sys\n",
    "    ipython_vars = [\"In\", \"Out\", \"exit\", \"quit\", \"get_ipython\", \"ipython_vars\"]\n",
    "    print(\"Top 10 objects with highest memory usage\")\n",
    "    # Get a sorted list of the objects and their sizes\n",
    "    mem = {\n",
    "        key: round(value/1e6,2)\n",
    "        for key, value in sorted(\n",
    "            [\n",
    "                (x, sys.getsizeof(globals().get(x)))\n",
    "                for x in globals()\n",
    "                if not x.startswith(\"_\") and x not in sys.modules and x not in ipython_vars\n",
    "            ],\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True)[:10]\n",
    "    }\n",
    "    print({key:f\"{value} MB\" for key,value in mem.items()})\n",
    "    print(f\"\\n{round(sum(mem.values()),2)/1000} GB in use overall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3773d22b-2e3e-4f5c-8d61-44ea190a40aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JOB ARRAY SETUP\n",
    "job_array=False;index_adjust=0\n",
    "job_array=True\n",
    "##############################*#*\n",
    "\n",
    "if job_array==True:\n",
    "    num_jobs=300 #how many total jobs are being run? i.e. array=1-100 ==> num_jobs=100 #***\n",
    "    total_elements=len(parcel['xh']) #total num of variables\n",
    "\n",
    "    if num_jobs >= total_elements:\n",
    "        raise ValueError(\"Number of jobs cannot be greater than or equal to total elements.\")\n",
    "    \n",
    "    job_range = total_elements // num_jobs  # Base size for each chunk\n",
    "    remaining = total_elements % num_jobs   # Number of chunks with 1 extra \n",
    "    \n",
    "    # Function to compute the start and end for each job_id\n",
    "    def get_job_range(job_id, num_jobs):\n",
    "        job_id-=1\n",
    "        # Add one extra element to the first 'remaining' chunks\n",
    "        start_job = job_id * job_range + min(job_id, remaining)\n",
    "        end_job = start_job + job_range + (1 if job_id < remaining else 0)\n",
    "    \n",
    "        if job_id == num_jobs - 1: \n",
    "            end_job = total_elements #- 1\n",
    "        return start_job, end_job\n",
    "    # def job_testing():\n",
    "    #     #TESTING\n",
    "    #     start=[];end=[]\n",
    "    #     for job_id in range(1,num_jobs+1):\n",
    "    #         start_job, end_job = get_job_range(job_id)\n",
    "    #         print(start_job,end_job)\n",
    "    #         start.append(start_job)\n",
    "    #         end.append(end_job)\n",
    "    #     print(np.all(start!=end))\n",
    "    #     print(len(np.unique(start))==len(start))\n",
    "    #     print(len(np.unique(end))==len(end))\n",
    "    # job_testing()\n",
    "    \n",
    "    job_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0)) #this is the current SBATCH job id\n",
    "    if job_id==0: job_id=1\n",
    "    start_job, end_job = get_job_range(job_id, num_jobs)\n",
    "    index_adjust=start_job\n",
    "    print(f'start_job = {start_job}, end_job = {end_job}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfbf147-c309-4c71-9625-4a5a68e4e915",
   "metadata": {},
   "outputs": [],
   "source": [
    "if job_array==True:\n",
    "    #Indexing Array with JobArray\n",
    "    parcel=parcel.isel(xh=slice(start_job,end_job))\n",
    "    #(for 150_000_000 parcels use 500-1000 jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a44209f-1e34-4853-bf7a-137b3f548b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Back Data Later\n",
    "##############\n",
    "def make_data_dict(in_file,var_names,read_type):\n",
    "    if read_type=='h5py':\n",
    "        with h5py.File(in_file, 'r') as f:\n",
    "            if job_array==True:\n",
    "                data_dict = {var_name: f[var_name][:,start_job:end_job] for var_name in var_names}\n",
    "            elif job_array==False:\n",
    "                data_dict = {var_name: f[var_name][:] for var_name in var_names}\n",
    "            \n",
    "    elif read_type=='xarray':\n",
    "        in_data = xr.open_dataset(\n",
    "            in_file,\n",
    "            engine='h5netcdf',\n",
    "            phony_dims='sort',\n",
    "            chunks={'phony_dim_0': 100, 'phony_dim_1': 1_000_000} \n",
    "        )\n",
    "        if job_array==True:\n",
    "            data_dict = {k: in_data[k][:,start_job:end_job].compute().data for k in var_names}\n",
    "        elif job_array==False:\n",
    "            data_dict = {k: in_data[k][:].compute().data for k in var_names}\n",
    "    return data_dict\n",
    "\n",
    "# read_type='xarray'\n",
    "read_type='h5py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526413e1-3f89-472d-a3e1-eddc8b2af983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "dir2=dir+'Project_Algorithms/Lagrangian_Arrays/'\n",
    "in_file=dir2+f'lagrangian_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "\n",
    "var_names = ['W', 'QCQI', 'Z', 'Y', 'X', 'z']\n",
    "data_dict = make_data_dict(in_file,var_names,read_type)\n",
    "W, QCQI, Z, Y, X, parcel_z = (data_dict[k] for k in var_names)\n",
    "\n",
    "# #Making Time Matrix\n",
    "# rows, cols = A.shape[0], A.shape[1]\n",
    "# T = np.arange(rows).reshape(-1, 1) * np.ones((1, cols), dtype=int)\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee62b8c1-d95b-4508-a9cb-0690fd15cef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aa4513-2b30-4419-9195-0a7d00472e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "#READING BACK IN\n",
    "def LoadFinalData(in_file):\n",
    "    dict = {}\n",
    "    with h5py.File(in_file, 'r') as f:\n",
    "        for key in f.keys():\n",
    "            dict[key] = f[key][:]\n",
    "    return dict\n",
    "\n",
    "def LoadAllCloudBase():\n",
    "    dir2 = dir + f'Project_Algorithms/Tracking_Algorithms/'\n",
    "    in_file = dir2 + f\"all_cloudbase_{res}_{t_res}_{Np_str}.pkl\"\n",
    "    with open(in_file, 'rb') as f:\n",
    "        all_cloudbase = pickle.load(f)\n",
    "    return(all_cloudbase)\n",
    "min_all_cloudbase=np.nanmin(LoadAllCloudBase())\n",
    "print(f\"Minimum Cloudbase is: {min_all_cloudbase}\\n\")\n",
    "\n",
    "dir2 = dir + f'Project_Algorithms/Tracking_Algorithms/'\n",
    "in_file=dir2+f\"parcel_tracking_SUBSET_{res}_{t_res}_{Np_str}\"\n",
    "final_dict=LoadFinalData(in_file)\n",
    "\n",
    "\n",
    "#DYNAMICALLY CREATING VARIABLES\n",
    "for key, value in final_dict.items():\n",
    "    globals()[key] = value\n",
    "\n",
    "# #DYNAMICALLY PRINTING VARIABLE SIZES\n",
    "# for key in final_dict:\n",
    "#     print(f\"{key} has {final_dict[key].shape[0]} parcels\")\n",
    "\n",
    "# PRINTING VARIABLE SIZES (ONE BY ONE)\n",
    "print(f'ALL: {len(CL_ALL_out_arr)} CL parcels and {len(nonCL_ALL_out_arr)} nonCL parcels')\n",
    "print(f'SHALLOW: {len(CL_SHALLOW_out_arr)} CL parcels and {len(nonCL_SHALLOW_out_arr)} nonCL parcels')\n",
    "print(f'DEEP: {len(CL_DEEP_out_arr)} CL parcels and {len(nonCL_DEEP_out_arr)} nonCL parcels')\n",
    "print('\\n')\n",
    "print(f'ALL: {len(SBZ_ALL_out_arr)} SBZ parcels and {len(nonSBZ_ALL_out_arr)} nonSBZ parcels')\n",
    "print(f'SHALLOW: {len(SBZ_SHALLOW_out_arr)} SBZ parcels and {len(nonSBZ_SHALLOW_out_arr)} nonSBZ parcels')\n",
    "print(f'DEEP: {len(SBZ_DEEP_out_arr)} SBZ parcels and {len(nonSBZ_DEEP_out_arr)} nonSBZ parcels')\n",
    "print('\\n')\n",
    "print(f'ALL: {len(ColdPool_ALL_out_arr)} ColdPool parcels')\n",
    "print(f'SHALLOW: {len(ColdPool_SHALLOW_out_arr)} ColdPool parcels')\n",
    "print(f'DEEP: {len(ColdPool_DEEP_out_arr)} ColdPool parcels')\n",
    "\n",
    "\n",
    "#APPLYING JOB ARRAY\n",
    "if job_array==True:\n",
    "    print('APPLYING JOB ARRAY')\n",
    "    def job_filter(arr):\n",
    "        return arr[(arr[:,0]>=start_job)&(arr[:,0]<end_job)]\n",
    "    for name in [\n",
    "        'CL_ALL_out_arr', 'nonCL_ALL_out_arr',\n",
    "        'CL_SHALLOW_out_arr', 'nonCL_SHALLOW_out_arr',\n",
    "        'CL_DEEP_out_arr', 'nonCL_DEEP_out_arr',\n",
    "        'SBZ_ALL_out_arr', 'nonSBZ_ALL_out_arr',\n",
    "        'SBZ_SHALLOW_out_arr', 'nonSBZ_SHALLOW_out_arr',\n",
    "        'SBZ_DEEP_out_arr', 'nonSBZ_DEEP_out_arr',\n",
    "        'ColdPool_ALL_out_arr', 'ColdPool_SHALLOW_out_arr', 'ColdPool_DEEP_out_arr'\n",
    "    ]:\n",
    "        globals()[name] = job_filter(globals()[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ada8b8-0f8f-4691-9a38-81cfa4e9667d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALL/DEEP/SHALLOW CL vs non-CL Tracked Parcel Plots\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d2376d-45b5-4222-a4b2-0edc55620a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TOO SLOW TO LOAD INTO MEMORY (TESTING WITH F(t))\n",
    "\n",
    "# # Loading Important Variables\n",
    "# ##############\n",
    "# if 'emptylike' not in globals():\n",
    "#     print('loading neccessary variables')\n",
    "#     variable='winterp'; w_data=data[variable].data #get w data\n",
    "#     print('done')\n",
    "#     empty_like=True\n",
    "# check_memory()\n",
    "\n",
    "# import dask.array as da\n",
    "# from dask.diagnostics import ProgressBar\n",
    "\n",
    "# # Ensure xarray automatically handles the Dask array\n",
    "# variable = 'qc'; qc_data = data[variable]  # Get qc data\n",
    "# variable = 'qi'; qi_data = data[variable]  # Get qi data\n",
    "\n",
    "# chunk_sizes = {'time': 5}  # Chunk only on time, leave others unchunked\n",
    "# qc_data = qc_data.chunk(chunk_sizes)\n",
    "# qi_data = qi_data.chunk(chunk_sizes)\n",
    "\n",
    "# # Perform the computation\n",
    "# qc_plus_qi = qc_data + qi_data  # Dask handles this efficiently\n",
    "\n",
    "# # Enable progress bar and compute\n",
    "# with ProgressBar():\n",
    "#     qc_plus_qi = qc_plus_qi.compute()\n",
    "\n",
    "def call_variables(t,z,y,x):\n",
    "    t = t if t is not None else slice(None)\n",
    "    z = z if z is not None else slice(None)\n",
    "    y = y if y is not None else slice(None)\n",
    "    x = x if x is not None else slice(None)\n",
    "    \n",
    "    variable='winterp'; w_data=data[variable].isel(time=t,zh=z,yh=y,xh=x).data\n",
    "    variable='qc'; qc_data=data[variable].isel(time=t,zh=z,yh=y,xh=x).data\n",
    "    variable='qi'; qi_data=data[variable].isel(time=t,zh=z,yh=y,xh=x).data\n",
    "    qc_plus_qi=qc_data+qi_data\n",
    "    return w_data,qc_plus_qi\n",
    "\n",
    "# t=100\n",
    "# z=10\n",
    "# y=100\n",
    "# x=400\n",
    "# [x_slice,x_slice2]=call_variables(t,z,y,None)\n",
    "# x_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab1f7a0-d5cb-4f63-a46c-06ec04311043",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKING PROFILE ARRAY\n",
    "\n",
    "#thresholds\n",
    "w_thresh1=0.1\n",
    "w_thresh2=0.5\n",
    "qcqi_thresh=1e-6\n",
    "\n",
    "dx=(data['xf'][1].item()-data['xf'][0].item())# km *1e3 #meters\n",
    "dy=(data['yf'][1].item()-data['yf'][0].item())# km *1e3 #meters\n",
    "\n",
    "def tracked_AREA(type,type2,updraft_type):\n",
    "    out_arr = globals()[f\"{type2}_{type.upper()}_out_arr\"].copy()\n",
    "   \n",
    "    zhs=data['zh'].values\n",
    "    profile_array =np.zeros((len(zhs), 3)) #column 1: var, column 2: counter, column 3: list of zhs\n",
    "    profile_array[:,2]=zhs;\n",
    "    \n",
    "    for row in range(out_arr.shape[0]):\n",
    "        after=out_arr[row,3]\n",
    "        if np.mod(row,50)==0: print(f'{row}/{out_arr.shape[0]}')\n",
    "        p=out_arr[row,0]\n",
    "        \n",
    "        # ts=np.arange(out_arr[row,4],out_arr[row,5]+1 + after)\n",
    "        ts_end = min(out_arr[row, 2] + 1 + after, len(data['time'])) #this takes care of exceeding buffers\n",
    "        ts = np.arange(out_arr[row, 1], ts_end)\n",
    "        \n",
    "        zs=Z[ts,p-index_adjust] #JOBARRAY INDEXADJUST\n",
    "        ys=Y[ts,p-index_adjust] #JOBARRAY INDEXADJUST\n",
    "        xs=X[ts,p-index_adjust] #JOBARRAY INDEXADJUST\n",
    "\n",
    "        for t, z, y, x in zip(ts, zs, ys, xs):\n",
    "\n",
    "            if True==True: #x in np.arange(15,500) and y in np.arange(15,85): (OLD) USED TO AVOID BOUNDARIES\n",
    "    \n",
    "                #CALCULATING AREA (CURRENTLY IGNORES CASES WHERE UPDRAFT CROSSES THE BOUNDARY)\n",
    "                ########################################################\n",
    "                \n",
    "                #FINDING XLENGTH\n",
    "                # x_slice=w_data[t,z,y,:]\n",
    "                # x_slice2=qc_plus_qi[t,z,y,:]\n",
    "                [x_slice,x_slice2]=call_variables(t,z,y,None)#JOBARRAY\n",
    "                \n",
    "                \n",
    "                if updraft_type=='general':\n",
    "                    # x_slice[(x_slice >= w_thresh1)] = 1; x_slice[x_slice < w_thresh1] = 0\n",
    "                    x_slice = np.where(x_slice >= w_thresh1, 1, 0)\n",
    "                elif updraft_type=='cloudy':\n",
    "                    # x_slice[(x_slice>=w_thresh2)&(x_slice2>=qcqi_thresh)]=1; x_slice[(x_slice<w_thresh2)|(x_slice2<qcqi_thresh)]=0\n",
    "                    x_slice = np.where((x_slice >= w_thresh2) & (x_slice2 >= qcqi_thresh), 1, 0)\n",
    "\n",
    "                if np.all(x_slice[x+1:]==1) or np.all(x_slice[:(x-1)+1]==1): #*AVOID BOUNDARY CASES*\n",
    "                    continue #*AVOID BOUNDARY CASES*\n",
    "                # x_right=np.where(x_slice[x+1:]==0)[0][0]+(x+1)\n",
    "                # x_left=np.where(x_slice[:(x-1)+1]==0)[0][-1]\n",
    "                x_right=np.where(x_slice[x:]==0)[0][0]+(x)\n",
    "                x_left=np.where(x_slice[:(x)+1]==0)[0][-1]\n",
    "                x_length=(x_right-x_left)*dx\n",
    "                \n",
    "                #FINDING YLENGTH\n",
    "                # y_slice=w_data[t,z,:,x]\n",
    "                # y_slice2=qc_plus_qi[t,z,:,x]\n",
    "                [y_slice,y_slice2]=call_variables(t,z,None,x) #JOBARRAY\n",
    "                \n",
    "                if updraft_type=='general':\n",
    "                    # y_slice[(y_slice>=w_thresh1)]=1; y_slice[y_slice<w_thresh1]=0\n",
    "                    y_slice = np.where(y_slice >= w_thresh1, 1, 0)\n",
    "                elif updraft_type=='cloudy':\n",
    "                    y_slice = np.where((y_slice >= w_thresh2) & (y_slice2 >= qcqi_thresh), 1, 0)\n",
    "\n",
    "                if np.all(y_slice[y+1:]==1) or np.all(y_slice[:(y-1)+1]==1): #*TOO AVOID BOUNDARY CASES*\n",
    "                    continue #*TOO AVOID BOUNDARY CASES*\n",
    "                # y_right=np.where(y_slice[y+1:]==0)[0][0]+(y+1)\n",
    "                # y_left=np.where(y_slice[:(y-1)+1]==0)[0][-1]\n",
    "                y_right=np.where(y_slice[y:]==0)[0][0]+(y)\n",
    "                y_left=np.where(y_slice[:(y)+1]==0)[0][-1]\n",
    "                y_length=(y_right-y_left)*dy\n",
    "                \n",
    "                AREA=x_length*y_length#square area approximation. \n",
    "                AREA*=np.pi/4 #include for oval area approximation\n",
    "                ########################################################\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            #ADDING TO PROFILE\n",
    "            var=AREA\n",
    "            profile_array[z,0]+=var;profile_array[z,1]+=1\n",
    "    return profile_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c97cc1-91ab-4402-b37e-e208b91cdffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "#CALCULATING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14989f2d-a4c3-44bb-8705-6897c5336eda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('ALL')\n",
    "CL_ALL_profile_array_AREA_general=tracked_AREA(type='all',type2='CL',updraft_type='general')\n",
    "CL_ALL_profile_array_AREA_cloudy=tracked_AREA(type='all',type2='CL',updraft_type='cloudy')\n",
    "nonCL_ALL_profile_array_AREA_general=tracked_AREA(type='all',type2='nonCL',updraft_type='general')\n",
    "nonCL_ALL_profile_array_AREA_cloudy=tracked_AREA(type='all',type2='nonCL',updraft_type='cloudy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231bbd12-232a-4bb8-ac1d-c783d8d80d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SHALLOW')\n",
    "CL_SHALLOW_profile_array_AREA_general=tracked_AREA(type='shallow',type2='CL',updraft_type='general')\n",
    "CL_SHALLOW_profile_array_AREA_cloudy=tracked_AREA(type='shallow',type2='CL',updraft_type='cloudy')\n",
    "\n",
    "nonCL_SHALLOW_profile_array_AREA_general=tracked_AREA(type='shallow',type2='nonCL',updraft_type='general')\n",
    "nonCL_SHALLOW_profile_array_AREA_cloudy=tracked_AREA(type='shallow',type2='nonCL',updraft_type='cloudy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b77a4b-c19e-4252-ad36-4b67ff5406cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DEEP')\n",
    "CL_DEEP_profile_array_AREA_general=tracked_AREA(type='deep',type2='CL',updraft_type='general')\n",
    "CL_DEEP_profile_array_AREA_cloudy=tracked_AREA(type='deep',type2='CL',updraft_type='cloudy')\n",
    "\n",
    "nonCL_DEEP_profile_array_AREA_general=tracked_AREA(type='deep',type2='nonCL',updraft_type='general')\n",
    "nonCL_DEEP_profile_array_AREA_cloudy=tracked_AREA(type='deep',type2='nonCL',updraft_type='cloudy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f849e8e-971b-4ad8-b8a7-1c8fed45bac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVING\n",
    "\n",
    "profile_dict = {}\n",
    "\n",
    "categories = ['ALL', 'SHALLOW', 'DEEP']\n",
    "types2 = ['CL', 'nonCL']\n",
    "updraft_types = ['general', 'cloudy']\n",
    "\n",
    "for category in categories:\n",
    "    print(category)  # Print category name\n",
    "    for type2 in types2:\n",
    "        for updraft_type in updraft_types:\n",
    "            var_name = f\"{type2}_{category}_profile_array_AREA_{updraft_type}\"\n",
    "            profile_dict[var_name] = eval(var_name)  # Dynamically reference existing variables\n",
    "\n",
    "dir2=dir+'Project_Algorithms/Tracked_Profiles/job_out2/'\n",
    "output_file=dir2+f\"CL_nonCL_tracked_Area_profiles_{res}_{t_res}_{Np_str}\"\n",
    "if job_array==True:\n",
    "    output_file+=f\"_{job_id}.h5\"\n",
    "elif job_array==False:\n",
    "    output_file+=f\".h5\"\n",
    "with open(output_file, \"wb\") as f:\n",
    "    pickle.dump(profile_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad7f3ec-d193-431a-987e-6c9023d9a145",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "#RECOMBINE SEPERATE JOB_ARRAYS AFTER\n",
    "recombine=False #KEEP FALSE WHEN JOBARRAY IS RUNNING\n",
    "# recombine=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9558d2f5-1117-46f4-b6be-ca3b55c768fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if recombine==True:\n",
    "    def load_dict(job_id):\n",
    "        dir2=dir+'Project_Algorithms/Tracked_Profiles/job_out2/'\n",
    "        input_file=dir2+f\"CL_nonCL_tracked_Area_profiles_{res}_{t_res}_{Np_str}_{job_id}.h5\"\n",
    "        with open(input_file, \"rb\") as f:\n",
    "            profile_dict = pickle.load(f)\n",
    "        return profile_dict\n",
    "    \n",
    "    job_id=1\n",
    "    profile_dict1 = load_dict(job_id)\n",
    "    \n",
    "    num_jobs=300\n",
    "    for job_id in np.arange(1,num_jobs+1):\n",
    "        profile_dict2 = load_dict(job_id)\n",
    "        if np.mod(job_id,10)==0: print(f\"job_id = {job_id}\")\n",
    "        for var in profile_dict2:\n",
    "            profile_dict1[var][:,0:1+1]+=profile_dict2[var][:,0:1+1]\n",
    "    \n",
    "    dir2=dir+'Project_Algorithms/Tracked_Profiles/OUTPUT_FILES/'\n",
    "    output_file=dir2+f\"CL_nonCL_tracked_Area_profiles_{res}_{t_res}_{Np_str}.h5\"\n",
    "    \n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump(profile_dict1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbae90ea-76ef-4236-b7fc-dd08e2649773",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "#SBZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4695be6c-39d8-463b-8455-b7ffcda7480e",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "#CALCULATING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb80ee88-fa30-4e13-83e9-08dcdf1e21b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('\\n'+'ALL')\n",
    "SBZ_ALL_profile_array_AREA_general=tracked_AREA(type='all',type2='SBZ',updraft_type='general')\n",
    "SBZ_ALL_profile_array_AREA_cloudy=tracked_AREA(type='all',type2='SBZ',updraft_type='cloudy')\n",
    "nonSBZ_ALL_profile_array_AREA_general=tracked_AREA(type='all',type2='nonSBZ',updraft_type='general')\n",
    "nonSBZ_ALL_profile_array_AREA_cloudy=tracked_AREA(type='all',type2='nonSBZ',updraft_type='cloudy')\n",
    "\n",
    "print('\\n'+'SHALLOW')\n",
    "SBZ_SHALLOW_profile_array_AREA_general=tracked_AREA(type='shallow',type2='SBZ',updraft_type='general')\n",
    "SBZ_SHALLOW_profile_array_AREA_cloudy=tracked_AREA(type='shallow',type2='SBZ',updraft_type='cloudy')\n",
    "\n",
    "nonSBZ_SHALLOW_profile_array_AREA_general=tracked_AREA(type='shallow',type2='nonSBZ',updraft_type='general')\n",
    "nonSBZ_SHALLOW_profile_array_AREA_cloudy=tracked_AREA(type='shallow',type2='nonSBZ',updraft_type='cloudy')\n",
    "\n",
    "print('\\n'+'DEEP')\n",
    "SBZ_DEEP_profile_array_AREA_general=tracked_AREA(type='deep',type2='SBZ',updraft_type='general')\n",
    "SBZ_DEEP_profile_array_AREA_cloudy=tracked_AREA(type='deep',type2='SBZ',updraft_type='cloudy')\n",
    "\n",
    "nonSBZ_DEEP_profile_array_AREA_general=tracked_AREA(type='deep',type2='nonSBZ',updraft_type='general')\n",
    "nonSBZ_DEEP_profile_array_AREA_cloudy=tracked_AREA(type='deep',type2='nonSBZ',updraft_type='cloudy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b574c76-9c08-40af-a786-8418299a5091",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVING\n",
    "\n",
    "profile_dict = {}\n",
    "\n",
    "categories = ['ALL', 'SHALLOW', 'DEEP']\n",
    "types2 = ['SBZ', 'nonSBZ']\n",
    "updraft_types = ['general', 'cloudy']\n",
    "\n",
    "for category in categories:\n",
    "    print(category)  # Print category name\n",
    "    for type2 in types2:\n",
    "        for updraft_type in updraft_types:\n",
    "            var_name = f\"{type2}_{category}_profile_array_AREA_{updraft_type}\"\n",
    "            profile_dict[var_name] = eval(var_name)  # Dynamically reference existing variables\n",
    "\n",
    "dir2=dir+'Project_Algorithms/Tracked_Profiles/job_out2/'\n",
    "output_file=dir2+f\"SBZ_nonSBZ_tracked_Area_profiles_{res}_{t_res}_{Np_str}\"\n",
    "if job_array==True:\n",
    "    output_file+=f\"_{job_id}.h5\"\n",
    "elif job_array==False:\n",
    "    output_file+=f\".h5\"\n",
    "with open(output_file, \"wb\") as f:\n",
    "    pickle.dump(profile_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199a94fc-1787-4584-a2a0-ff8ed5e3b27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "#RECOMBINE SEPERATE JOB_ARRAYS AFTER\n",
    "recombine=False #KEEP FALSE WHEN JOBARRAY IS RUNNING\n",
    "# recombine=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a836c552-d942-4519-bd15-1203c68c1121",
   "metadata": {},
   "outputs": [],
   "source": [
    "if recombine==True:\n",
    "    def load_dict(job_id):\n",
    "        dir2=dir+'Project_Algorithms/Tracked_Profiles/job_out2/'\n",
    "        input_file=dir2+f\"SBZ_nonSBZ_tracked_Area_profiles_{res}_{t_res}_{Np_str}_{job_id}.h5\"\n",
    "        with open(input_file, \"rb\") as f:\n",
    "            profile_dict = pickle.load(f)\n",
    "        return profile_dict\n",
    "    \n",
    "    job_id=1\n",
    "    profile_dict1 = load_dict(job_id)\n",
    "    \n",
    "    num_jobs=300\n",
    "    for job_id in np.arange(1,num_jobs+1):\n",
    "        profile_dict2 = load_dict(job_id)\n",
    "        if np.mod(job_id,10)==0: print(f\"job_id = {job_id}\")\n",
    "        for var in profile_dict2:\n",
    "            profile_dict1[var][:,0:1+1]+=profile_dict2[var][:,0:1+1]\n",
    "    \n",
    "    dir2=dir+'Project_Algorithms/Tracked_Profiles/OUTPUT_FILES/'\n",
    "    output_file=dir2+f\"SBZ_nonSBZ_tracked_Area_profiles_{res}_{t_res}_{Np_str}.h5\"\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump(profile_dict1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883d6fa1-7513-4331-abb2-9637d0506b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "#COLD POOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46590190-2a26-454f-9403-ef4f1a9a5813",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "#CALCULATING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c46638-5842-4014-8441-c9fd225983da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'+'ALL')\n",
    "ColdPool_ALL_profile_array_AREA_general=tracked_AREA(type='all',type2='ColdPool',updraft_type='general')\n",
    "ColdPool_ALL_profile_array_AREA_cloudy=tracked_AREA(type='all',type2='ColdPool',updraft_type='cloudy')\n",
    "\n",
    "print('\\n'+'SHALLOW')\n",
    "ColdPool_SHALLOW_profile_array_AREA_general=tracked_AREA(type='shallow',type2='ColdPool',updraft_type='general')\n",
    "ColdPool_SHALLOW_profile_array_AREA_cloudy=tracked_AREA(type='shallow',type2='ColdPool',updraft_type='cloudy')\n",
    "\n",
    "\n",
    "print('\\n'+'DEEP')\n",
    "ColdPool_DEEP_profile_array_AREA_general=tracked_AREA(type='deep',type2='ColdPool',updraft_type='general')\n",
    "ColdPool_DEEP_profile_array_AREA_cloudy=tracked_AREA(type='deep',type2='ColdPool',updraft_type='cloudy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e818e9-74b8-4b5b-a909-d4f26f34fbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVING\n",
    "\n",
    "profile_dict = {}\n",
    "\n",
    "categories = ['ALL', 'SHALLOW', 'DEEP']\n",
    "types2 = ['ColdPool']\n",
    "updraft_types = ['general', 'cloudy']\n",
    "\n",
    "for category in categories:\n",
    "    print(category)  # Print category name\n",
    "    for type2 in types2:\n",
    "        for updraft_type in updraft_types:\n",
    "            var_name = f\"{type2}_{category}_profile_array_AREA_{updraft_type}\"\n",
    "            profile_dict[var_name] = eval(var_name)  # Dynamically reference existing variables\n",
    "\n",
    "dir2=dir+'Project_Algorithms/Tracked_Profiles/job_out2/'\n",
    "output_file=dir2+f\"ColdPool_tracked_Area_profiles_{res}_{t_res}_{Np_str}\"\n",
    "if job_array==True:\n",
    "    output_file+=f\"_{job_id}.h5\"\n",
    "if job_array==False:\n",
    "    output_file+=f\".h5\"\n",
    "with open(output_file, \"wb\") as f:\n",
    "    pickle.dump(profile_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5eaf63-5a65-40c9-8a24-76c3f4b79305",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "#RECOMBINE SEPERATE JOB_ARRAYS AFTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b93b07f-ce0d-4c4a-82d9-18030e205f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "recombine=False #KEEP FALSE WHEN JOBARRAY IS RUNNING\n",
    "recombine=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4775b3-cb72-4d67-be0b-973763b5f605",
   "metadata": {},
   "outputs": [],
   "source": [
    "if recombine==True:\n",
    "    def load_dict(job_id):\n",
    "        dir2=dir+'Project_Algorithms/Tracked_Profiles/job_out2/'\n",
    "        input_file=dir2+f\"ColdPool_tracked_Area_profiles_{res}_{t_res}_{Np_str}_{job_id}.h5\"\n",
    "        with open(input_file, \"rb\") as f:\n",
    "            profile_dict = pickle.load(f)\n",
    "        return profile_dict\n",
    "    \n",
    "    job_id=1\n",
    "    profile_dict1 = load_dict(job_id)\n",
    "    \n",
    "    num_jobs=300\n",
    "    for job_id in np.arange(1,num_jobs+1):\n",
    "        profile_dict2 = load_dict(job_id)\n",
    "        if np.mod(job_id,10)==0: print(f\"job_id = {job_id}\")\n",
    "        for var in profile_dict2:\n",
    "            profile_dict1[var][:,0:1+1]+=profile_dict2[var][:,0:1+1]\n",
    "    \n",
    "    dir2=dir+'Project_Algorithms/Tracked_Profiles/OUTPUT_FILES/'\n",
    "    output_file=dir2+f\"ColdPool_tracked_Area_profiles_{res}_{t_res}_{Np_str}.h5\"\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump(profile_dict1, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
