{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf81bf6-0a82-4533-b015-ee5cca588ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Loading in Packages and Data\n",
    "\n",
    "#Importing Packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import matplotlib.gridspec as gridspec\n",
    "import xarray as xr\n",
    "import os; import time\n",
    "import pickle\n",
    "import h5py\n",
    "###############################################################\n",
    "def coefs(coefficients,degree):\n",
    "    coef=coefficients\n",
    "    coefs=\"\"\n",
    "    for n in range(degree, -1, -1):\n",
    "        string=f\"({coefficients[len(coef)-(n+1)]:.1e})\"\n",
    "        coefs+=string + f\"x^{n}\"\n",
    "        if n != 0:\n",
    "            coefs+=\" + \"\n",
    "    return coefs\n",
    "###############################################################\n",
    "\n",
    "#Importing Model Data\n",
    "check=False\n",
    "dir='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "\n",
    "# dx = 1 km; Np = 1M; Nt = 5 min\n",
    "data=xr.open_dataset(dir+'../cm1r20.3/run/cm1out_1km_5min.nc') #***\n",
    "parcel=xr.open_dataset(dir+'../cm1r20.3/run/cm1out_pdata_1km_5min_1e6.nc') #***\n",
    "res='1km';t_res='5min'\n",
    "Np_str='1e6'\n",
    "\n",
    "# # dx = 1km; Np = 50M\n",
    "# #Importing Model Data\n",
    "# check=False\n",
    "# dir2='/home/air673/koa_scratch/'\n",
    "# data=xr.open_dataset(dir2+'cm1out_1km_1min.nc') #***\n",
    "# parcel=xr.open_dataset(dir2+'cm1out_pdata_1km_1min_50M.nc') #***\n",
    "# res='1km'; t_res='1min'; Np_str='50e6'\n",
    "\n",
    "# # dx = 1km; Np = 100M\n",
    "# #Importing Model Data\n",
    "# check=False\n",
    "# dir2='/home/air673/koa_scratch/'\n",
    "# data=xr.open_dataset(dir2+'cm1out_1km_1min.nc') #***\n",
    "# parcel=xr.open_dataset(dir2+'cm1out_pdata_1km_1min_100M.nc') #***\n",
    "# res='1km'; t_res='1min'; Np_str='100e6'\n",
    "\n",
    "\n",
    "# dx = 250 m\n",
    "# #Importing Model Data\n",
    "# check=False\n",
    "# dir2='/home/air673/koa_scratch/'\n",
    "# data=xr.open_dataset(dir2+'cm1out_250m.nc') #***\n",
    "# parcel=xr.open_dataset(dir2+'cm1out_pdata_250m.nc') #***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd4360b-5993-4c36-87bf-4e5d08d07c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INITIALIZE DATA FUNCTION\n",
    "###############################################################\n",
    "def initiate_array(out_file, vars, t_chunk_size, p_chunk_size, t_size=None, p_size=None):\n",
    "    if t_size is None:\n",
    "        t_size = len(data['time'])  # Number of timesteps\n",
    "    if p_size is None:\n",
    "        p_size = len(parcel['xh'])  # Number of vertical levels\n",
    "\n",
    "    with h5py.File(out_file, 'w') as f:\n",
    "        for var_name in vars:\n",
    "            if var_name not in f:\n",
    "                # Set dtype conditionally\n",
    "                if var_name in ['Z', 'Y', 'X']:\n",
    "                    dtype = np.uint16\n",
    "                elif var_name in ['A_g','A_c','A_g_Processed','A_c_Processed']:\n",
    "                    dtype = np.bool_\n",
    "                else:\n",
    "                    dtype = np.float32  # or whatever your default is\n",
    "\n",
    "                f.create_dataset(\n",
    "                    var_name,\n",
    "                    shape=(t_size, p_size),\n",
    "                    chunks=(t_chunk_size, p_chunk_size),\n",
    "                    dtype=dtype\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a926772d-9f18-4066-b1f5-99eea6a90406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "dir2='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "path=dir2+'../Functions/'\n",
    "sys.path.append(path)\n",
    "\n",
    "import NumericalFunctions\n",
    "from NumericalFunctions import * # import NumericalFunctions \n",
    "import PlottingFunctions\n",
    "from PlottingFunctions import * # import PlottingFunctions\n",
    "\n",
    "\n",
    "# # Get all functions in NumericalFunctions\n",
    "# import inspect\n",
    "# functions = [f[0] for f in inspect.getmembers(NumericalFunctions, inspect.isfunction)]\n",
    "# functions\n",
    "\n",
    "# # Get all functions in NumericalFunctions\n",
    "# import inspect\n",
    "# functions = [f[0] for f in inspect.getmembers(PlottingFunctions, inspect.isfunction)]\n",
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dff7c96-ed40-48b1-abb4-b017f6d84561",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JOB ARRAY SETUP\n",
    "job_array=True\n",
    "if job_array==True:\n",
    "\n",
    "    num_jobs=60 #how many total jobs are being run? i.e. array=1-100 ==> num_jobs=100 #***\n",
    "    total_elements=len(parcel['xh']) #total num of variables\n",
    "\n",
    "    if num_jobs >= total_elements:\n",
    "        raise ValueError(\"Number of jobs cannot be greater than or equal to total elements.\")\n",
    "    \n",
    "    job_range = total_elements // num_jobs  # Base size for each chunk\n",
    "    remaining = total_elements % num_jobs   # Number of chunks with 1 extra \n",
    "    \n",
    "    # Function to compute the start and end for each job_id\n",
    "    def get_job_range(job_id, num_jobs):\n",
    "        job_id-=1\n",
    "        # Add one extra element to the first 'remaining' chunks\n",
    "        start_job = job_id * job_range + min(job_id, remaining)\n",
    "        end_job = start_job + job_range + (1 if job_id < remaining else 0)\n",
    "    \n",
    "        if job_id == num_jobs - 1: \n",
    "            end_job = total_elements #- 1\n",
    "        return start_job, end_job\n",
    "    # def job_testing():\n",
    "    #     #TESTING\n",
    "    #     start=[];end=[]\n",
    "    #     for job_id in range(1,num_jobs+1):\n",
    "    #         start_job, end_job = get_job_range(job_id)\n",
    "    #         print(start_job,end_job)\n",
    "    #         start.append(start_job)\n",
    "    #         end.append(end_job)\n",
    "    #     print(np.all(start!=end))\n",
    "    #     print(len(np.unique(start))==len(start))\n",
    "    #     print(len(np.unique(end))==len(end))\n",
    "    # job_testing()\n",
    "    \n",
    "    job_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0)) #this is the current SBATCH job id\n",
    "    if job_id==0: job_id=10\n",
    "    start_job, end_job = get_job_range(job_id, num_jobs)\n",
    "    index_adjust=start_job\n",
    "    print(f'start_job = {start_job}, end_job = {end_job}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac326886-b29d-466d-b7e8-c513867f50b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "#LOADING IN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01521736-65b1-4ac3-ac1b-12ce2b43a063",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indexing Array with JobArray\n",
    "parcel=parcel.isel(xh=slice(start_job,end_job))\n",
    "#(for 150_000_000 parcels use 500-1000 jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2376c6a0-87b9-4b10-a025-440f6f2daab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Back Data Later\n",
    "##############\n",
    "def make_data_dict(var_names,read_type):\n",
    "    if read_type=='h5py':\n",
    "        with h5py.File(in_file, 'r') as f:\n",
    "            data_dict = {var_name: f[var_name][:,start_job:end_job] for var_name in var_names}\n",
    "            \n",
    "    elif read_type=='xarray':\n",
    "        in_data = xr.open_dataset(\n",
    "            in_file,\n",
    "            engine='h5netcdf',\n",
    "            phony_dims='sort',\n",
    "            chunks={'phony_dim_0': 100, 'phony_dim_1': 1_000_000} \n",
    "        )\n",
    "        data_dict = {k: in_data[k][:,start_job:end_job].compute().data for k in var_names}\n",
    "    return data_dict\n",
    "\n",
    "# read_type='xarray'\n",
    "read_type='h5py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f6be76-90b3-4a05-94af-b163cedc638f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "dir2=dir+'Project_Algorithms/Lagrangian_Arrays/'\n",
    "in_file=dir2+f'lagrangian_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "\n",
    "var_names = ['A_g', 'A_c']\n",
    "data_dict = make_data_dict(var_names,read_type)\n",
    "A_g, A_c = (data_dict[k] for k in var_names)\n",
    "\n",
    "# #Making Time Matrix\n",
    "Nt=len(data['time'])\n",
    "T = np.broadcast_to(np.arange(Nt)[:, None], A_c.shape)  # shape: (Nt, p)\n",
    "\n",
    "check_memory(globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5fb09b-3e09-427c-8432-e76cba791479",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279c30c2-0351-4a31-a8a4-98cced1b8f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_idxs(f,case):\n",
    "    out=np.sort(np.add.outer(f, np.arange(case)).ravel())\n",
    "\n",
    "    # #OLD METHOD (SLOW)\n",
    "    # if np.any(f)==True:\n",
    "    #     out=np.sort(np.concatenate([np.arange(idx, idx + case-1+1) for idx in f]))\n",
    "    # else: \n",
    "    #     out=f\n",
    "    return out\n",
    "\n",
    "def find_sandwiched_patterns(changes, case):\n",
    "    arr=changes\n",
    "    \n",
    "    window_size = case + 1  # e.g., for case=2, window_size = 3\n",
    "    # The interior zeros count is (window_size - 2) which is case - 1\n",
    "    pattern1 = np.array([-1] + [0]*(case - 1) + [1])\n",
    "    pattern2 = np.array([1] + [0]*(case - 1) + [-1])\n",
    "    # print(pattern1,pattern2)\n",
    "    \n",
    "    # Manually construct sliding windows\n",
    "    windows = np.array([arr[i:i + window_size] for i in range(len(arr) - window_size + 1)])\n",
    "    # print(\"Sliding windows:\\n\", windows) #TESTING\n",
    "    \n",
    "    #THE ALGORITHM\n",
    "    turb_d=[]\n",
    "    turb_e=[]\n",
    "    count=0;max_iter=len(data['time']);\n",
    "    while np.any(((windows == pattern1) | (windows == pattern2)).all(axis=1)):\n",
    "        count+=1; \n",
    "        if count>=max_iter: \n",
    "            print(count)\n",
    "            break\n",
    "        \n",
    "        next_ind = np.where(((windows == pattern1) | (windows == pattern2)).all(axis=1))[0][0]\n",
    "        \n",
    "        if (windows[next_ind] == pattern1).all():\n",
    "            turb_d.append(next_ind)\n",
    "        elif (windows[next_ind] == pattern2).all(): \n",
    "            turb_e.append(next_ind) #append to list\n",
    "    \n",
    "        windows[0:next_ind+(case)+1,:] = 0 #removes from windows\n",
    "    \n",
    "    turb_d=np.array(turb_d,dtype=int); turb_e=np.array(turb_e,dtype=int)\n",
    "\n",
    "    #EXTEND REST OF INDEXES TO PROCESS\n",
    "    turb_d=extend_idxs(turb_d,case=case)\n",
    "    turb_e=extend_idxs(turb_e,case=case)\n",
    "    return turb_d,turb_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106dbc65-0e40-4a4b-b0ff-01497659bea4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# TESTING\n",
    "# changes = np.array([0,0,0,-1,1,0,0,-1,0,0,0,1,-1,0,0])\n",
    "# [a,b] = find_sandwiched_patterns(changes, case=1) #<=1 in a row timesteps are removed\n",
    "# print(\"Case matches at indices:\", a,b)\n",
    "\n",
    "# changes = np.array([0,0,0,-1,0,1,0,0,-1,0,0,1,0,-1,0,0])\n",
    "# [a,b] = find_sandwiched_patterns(changes, case=2) #<=2 in a row timesteps are removed\n",
    "# print(\"Case matches at indices:\", a,b)\n",
    "\n",
    "# changes = np.array([0,0,0,-1,0,0,1,0,0,0,0,1,0,0,-1,0,0])\n",
    "# [a,b] = find_sandwiched_patterns(changes, case=3) #<=3 in a row timesteps are removed\n",
    "# print(\"Case matches at indices:\", a,b)\n",
    "\n",
    "# changes = np.array([0,0,0,-1,0,0,0,1,0,0,0,0,1,0,0,-1,0,0])\n",
    "# [a,b] = find_sandwiched_patterns(changes, case=4) #<=4 in a row timesteps are removed\n",
    "# print(\"Case matches at indices:\", a,b)\n",
    "\n",
    "# changes = np.array([0,0,0,-1,0,0,0,0,1,0,0,0,0,1,0,0,-1,0,0])\n",
    "# [a,b] = find_sandwiched_patterns(changes, case=5) #<=5 in a row timesteps are removed\n",
    "# print(\"Case matches at indices:\", a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f56dd61-519d-454f-a230-1a592b3f789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### (amount of time inside/outside of cloud to count as entrainment/detrainment)\n",
    "mins_thresh=5 #5 mins\n",
    "######\n",
    "\n",
    "def get_changes(B):\n",
    "    changes = np.diff(np.concatenate(([B[0]], B)))  # Add 0s to detect edges\n",
    "    return changes\n",
    "def PreProcessing(A,p): #,updraft_type\n",
    "\n",
    "    # if updraft_type=='general':\n",
    "    #     A=A_g.copy()\n",
    "    # elif updraft_type=='cloudy':\n",
    "    #     A=A_c.copy()\n",
    "    B = A[:,p] #UNCOMMENT WHEN NOT TESTING***\n",
    "\n",
    "    # Find the changes in the array\n",
    "    changes=get_changes(B)\n",
    "    # print(f'B = {B}'); print(f'changes = {changes}') \n",
    "\n",
    "    #Determining the Case Number\n",
    "    t_per_mins=1/((data['time'][1]-data['time'][0])/1e9/60).item() #timesteps per minute (<=1)\n",
    "    case=int(t_per_mins*mins_thresh) #UNCOMMENT WHEN NOT TESTING***\n",
    "    \n",
    "    if case>1:\n",
    "        for case_ind in np.arange(case,0,-1): \n",
    "        # for case_ind in [case]:\n",
    "            #Calling Algorithm and Correcting Parcel Data\n",
    "            [turb_d,turb_e]=find_sandwiched_patterns(changes, case=case_ind)\n",
    "            B[turb_d]=1\n",
    "            B[turb_e]=0     \n",
    "            changes=get_changes(B)\n",
    "            # print(B)\n",
    "    elif case==1:\n",
    "        #Calling Algorithm and Correcting Parcel Data\n",
    "        [turb_d,turb_e]=find_sandwiched_patterns(changes, case=case)\n",
    "        B[turb_d]=1\n",
    "        B[turb_e]=0\n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c3de60-1c01-4245-a255-d0380e3dfc1d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #EXAMPLE TESTING #CASE COUNTDOWN\n",
    "# B=np.array([1,1,1,1,0,0,0,1,1,1,1,0,0,0,1,0,0,1,1,1,0,0,0,1])\n",
    "#                     #,#,#,        #,#,#       #,#,#\n",
    "# print(B)\n",
    "\n",
    "# #APPLYING\n",
    "# changes=get_changes(B)\n",
    "# [turb_d,turb_e]=find_sandwiched_patterns(changes,case=3)\n",
    "# print(turb_d,turb_e)\n",
    "\n",
    "# B[turb_d]=1\n",
    "# B[turb_e]=0\n",
    "# print(B)\n",
    "\n",
    "# #APPLYING\n",
    "# changes=get_changes(B)\n",
    "# [turb_d,turb_e]=find_sandwiched_patterns(changes,case=2)\n",
    "# print(turb_d,turb_e)\n",
    "\n",
    "# B[turb_d]=1\n",
    "# B[turb_e]=0\n",
    "# print(B)\n",
    "\n",
    "# #APPLYING\n",
    "# changes=get_changes(B)\n",
    "# [turb_d,turb_e]=find_sandwiched_patterns(changes,case=1)\n",
    "# print(turb_d,turb_e)\n",
    "\n",
    "# B[turb_d]=1\n",
    "# B[turb_e]=0\n",
    "# print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c5f8a2-0ec1-427c-9b42-edf2799f457a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # TESTING\n",
    "# # Case 1\n",
    "# case=1\n",
    "# B=np.array([1,0,1,1,0,0,1,0])\n",
    "# B=np.array([1,0,1,1,0,1,0]) \n",
    "# B=np.array([1,0,1,1,0,1,0,1])\n",
    "\n",
    "# B=np.array([1,0,1,0,1,1,1])\n",
    "# B=np.array([1,0,1,0,1,0,0])\n",
    "# B=np.array([0,1,0,1,0,0,0]) \n",
    "# B=np.array([0,1,0,1,0,1,1])\n",
    "\n",
    "# # Case 2\n",
    "# case=2\n",
    "# B=np.array([1,1,1,0,0,1,1,1,0,0,0,1,1,0,0,0])\n",
    "# B=np.array([1,0,0,1,0,0,1,0,0,0,1,0,1,0,0,0]) #101 should still get removed\n",
    "\n",
    "# # Case 3\n",
    "# case=3\n",
    "# B=np.array([1,1,1,1,0,0,0,1,1,1,1,0,0,0,1,0,0,0,0,1])\n",
    "# print(f'output =  {B}\\n')\n",
    "# p=1234; out=PreProcessing(p,updraft_type='cloudy') #TESTING\n",
    "# print(f'output =  {out}\\n')\n",
    "\n",
    "# # Case 5\n",
    "# case=5\n",
    "# B=np.array([1,1,1,1,0,0,0,0,0,1,1,1,1,0,0,0,1,0,0,0,0,1])\n",
    "# print(f'output =  {B}\\n')\n",
    "# p=1234; out=PreProcessing(p,updraft_type='cloudy') #TESTING\n",
    "# print(f'output =  {out}\\n')\n",
    "\n",
    "# # REAL CASE\n",
    "# count_per_row = (A_c >= 1).sum(axis=0)\n",
    "# where=np.where(count_per_row > 10)[0] # Find row indices where count is greater than 10\n",
    "# # print(where)\n",
    "# ind=12345; p=where[ind]; print(p) \n",
    "\n",
    "# print(A_c[:,p])\n",
    "# out=PreProcessing(p,updraft_type='cloudy') #TESTING\n",
    "# print(f'output =  {out}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0108b97d-3123-41b9-bf21-0c7fc5bcbeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLD NONSIMULTANEOUS VERSION\n",
    "# #RUNNING\n",
    "# A_g_Processed=A_g.copy()\n",
    "# A_c_Processed=A_c.copy()\n",
    "# print('processing parcels for general updrafts')\n",
    "# Np=len(parcel['xh'])\n",
    "# for p in np.arange(Np):\n",
    "#     # if p==1000:break #TESTING\n",
    "#     if np.mod(p,1e3)==0: print(f\"{p}/{len(parcel['xh'])}\")\n",
    "#     out=PreProcessing(p,updraft_type='general'); A_g_Processed[:,p]=out\n",
    "    \n",
    "# print('processing parcels for cloudy updrafts')\n",
    "# for p in np.arange(Np):\n",
    "#     # if p==1000:break #TESTING\n",
    "#     if np.mod(p,1e3)==0: print(f\"{p}/{len(parcel['xh'])}\")\n",
    "#     out=PreProcessing(p,updraft_type='cloudy'); A_c_Processed[:,p]=out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a75682-93cf-4b14-986a-9adca145393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SIMULTANEOUS VERSION\n",
    "#RUNNING\n",
    "A_g_Processed=A_g.copy()\n",
    "A_c_Processed=A_c.copy()\n",
    "print('processing parcels for both general and cloudy updrafts')\n",
    "Np=len(parcel['xh'])\n",
    "for p in np.arange(Np):\n",
    "    # if p==1000:break #TESTING\n",
    "    if np.mod(p,1e3)==0: print(f\"{p}/{len(parcel['xh'])}\")\n",
    "    out1=PreProcessing(A_g,p); A_g_Processed[:,p]=out1\n",
    "    out2=PreProcessing(A_c,p); A_c_Processed[:,p]=out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9098f1f8-eb3d-4ddc-9923-f0239e89cae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVING\n",
    "mins_thresh=5 #5 minutes\n",
    "\n",
    "dir2=dir+'Project_Algorithms/Entrainment/job_out_3/'\n",
    "out_file=dir2+f'processed_binary_arrays_{res}_{t_res}_{Np_str}_{job_id}.h5'\n",
    "\n",
    "vars=['A_g_Processed','A_c_Processed']\n",
    "initiate_array(out_file,vars,t_chunk_size=50,p_chunk_size=1000)\n",
    "\n",
    "with h5py.File(out_file, 'a') as f: \n",
    "    f['A_g_Processed'][:]=A_g_Processed\n",
    "    f['A_c_Processed'][:]=A_c_Processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4489ee-837d-4e5f-91b0-efe502109b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7f87ec-d5b8-48b8-8a46-591df5982b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMBINING JOB_ARRAYS AFTER RUNNING\n",
    "########################################################################\n",
    "recombine=False #KEEP FALSE WHEN JOB ARRAY IS RUNNING\n",
    "recombine=True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e476e7e-f39f-46b5-bf38-a2d83fde7852",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if recombine==True:\n",
    "    dir2=dir+'Project_Algorithms/Entrainment/job_out_3/'\n",
    "    dir3=dir+'Project_Algorithms/Entrainment/'\n",
    "    out_file=dir3+f'processed_binary_arrays_{res}_{t_res}_{Np_str}.h5'\n",
    "    \n",
    "    vars=['A_g_Processed','A_c_Processed']\n",
    "    initiate_array(out_file,vars,t_chunk_size=50,p_chunk_size=100_000)\n",
    "    \n",
    "    with h5py.File(out_file, 'r+') as f_out:\n",
    "        \n",
    "        num_jobs=60\n",
    "        for job_id in np.arange(1,num_jobs+1):\n",
    "            if np.mod(job_id,5)==0: print(f\"job_id = {job_id}\")\n",
    "            [a,b] = get_job_range(job_id,num_jobs)\n",
    "        \n",
    "            in_file=dir2+f'processed_binary_arrays_{res}_{t_res}_{Np_str}_{job_id}.h5'\n",
    "    \n",
    "            with h5py.File(in_file, 'r') as f_in: \n",
    "                for var in vars:\n",
    "                    f_out[var][:,a:b]=f_in[var][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d2f461-3ce7-4022-b1b1-ea3389583c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8834bddf-4143-4f0a-9dd3-5c124171ad3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46476abf-3d03-42de-b0c7-238ba1cff1ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a068f795-4fc8-4185-b957-20574e44821a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#READING BACK IN AND TESTING\n",
    "############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6633ff-c7f0-45ce-a5c7-c5f6ca5758a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_job=0;end_job=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efc7746-ff8b-4979-9d73-2a2d6aabbbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Back Data Later\n",
    "##############\n",
    "def make_data_dict(in_file,var_names,read_type):\n",
    "    if read_type=='h5py':\n",
    "        with h5py.File(in_file, 'r') as f:\n",
    "            data_dict = {var_name: f[var_name][:,start_job:end_job] for var_name in var_names}\n",
    "            \n",
    "    elif read_type=='xarray':\n",
    "        in_data = xr.open_dataset(\n",
    "            in_file,\n",
    "            engine='h5netcdf',\n",
    "            phony_dims='sort',\n",
    "            chunks={'phony_dim_0': 100, 'phony_dim_1': 1_000_000} \n",
    "        )\n",
    "        data_dict = {k: in_data[k][:,start_job:end_job].compute().data for k in var_names}\n",
    "    return data_dict\n",
    "\n",
    "# read_type='xarray'\n",
    "read_type='h5py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4ad837-5fd0-4ac7-ae93-ed9a51de1620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "dir2=dir+'Project_Algorithms/Lagrangian_Arrays/'\n",
    "in_file=dir2+f'lagrangian_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "\n",
    "var_names = ['A_g', 'A_c']\n",
    "data_dict = make_data_dict(in_file,var_names,read_type)\n",
    "[A_g, A_c] = (data_dict[k] for k in var_names) #, W\n",
    "\n",
    "check_memory(globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390b386e-47c9-4faa-b2ca-c70bdd292969",
   "metadata": {},
   "outputs": [],
   "source": [
    "#READING BACK IN\n",
    "dir2=dir+'Project_Algorithms/Entrainment/'\n",
    "in_file=dir2+f'processed_binary_arrays_{res}_{t_res}_{Np_str}.h5'\n",
    "\n",
    "var_names = ['A_g_Processed', 'A_c_Processed']\n",
    "data_dict = make_data_dict(in_file,var_names,read_type)\n",
    "A_g_Processed, A_c_Processed = (data_dict[k] for k in var_names)\n",
    "check_memory(globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565e1c04-32e4-4605-9493-31a7eef08e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=290 #time index\n",
    "where=np.where(A_c[ind]!=A_c_Processed[ind])\n",
    "where #which parcels have differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8af4e01-24fb-46f1-bcdf-0c2d6a5050fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ind2=0 #choose one of those parcels\n",
    "ind2+=1\n",
    "where2=where[0][ind2]\n",
    "def format_array(arr):\n",
    "    arr = np.atleast_2d(arr)  # Ensures it's 2D\n",
    "    return '\\n'.join(''.join('_' if val == 0 else '1' for val in row) for row in arr)\n",
    "\n",
    "print(\"A_c:\")\n",
    "print(format_array(A_c[:, where2]))\n",
    "\n",
    "print(\"A_c_Processed:\")\n",
    "print(format_array(A_c_Processed[:, where2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08a2bf0-9ebb-4277-a89d-4b70d2d27879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #PLUGGING INTO ALGORITHM TO SEE WHERE THINGS WENT WRONG\n",
    "# B=A_c[:,where2]\n",
    "# print(B)\n",
    "# PreProcessing(p=where2,updraft_type='cloudy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2157d4e3-79a0-47d3-a1fe-82fd14fc123e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def extend_idxs(f,case):\n",
    "#     out=np.sort(np.add.outer(f, np.arange(case)).ravel())\n",
    "\n",
    "#     # #OLD METHOD (SLOW)\n",
    "#     # if np.any(f)==True:\n",
    "#     #     out=np.sort(np.concatenate([np.arange(idx, idx + case-1+1) for idx in f]))\n",
    "#     # else: \n",
    "#     #     out=f\n",
    "#     return out\n",
    "\n",
    "# def find_sandwiched_patterns(changes, case):\n",
    "#     arr=changes\n",
    "    \n",
    "#     window_size = case + 1  # e.g., for case=2, window_size = 3\n",
    "#     # The interior zeros count is (window_size - 2) which is case - 1\n",
    "#     pattern1 = np.array([-1] + [0]*(case - 1) + [1])\n",
    "#     pattern2 = np.array([1] + [0]*(case - 1) + [-1])\n",
    "#     # print(pattern1,pattern2)\n",
    "    \n",
    "#     # Manually construct sliding windows\n",
    "#     windows = np.array([arr[i:i + window_size] for i in range(len(arr) - window_size + 1)])\n",
    "#     # print(\"Sliding windows:\\n\", windows) #TESTING\n",
    "    \n",
    "#     #THE ALGORITHM\n",
    "#     turb_d=[]\n",
    "#     turb_e=[]\n",
    "#     count=0;max_iter=len(data['time']);\n",
    "#     while np.any(((windows == pattern1) | (windows == pattern2)).all(axis=1)):\n",
    "#         count+=1; \n",
    "#         if count>=max_iter: \n",
    "#             print(count)\n",
    "#             break\n",
    "        \n",
    "#         next_ind = np.where(((windows == pattern1) | (windows == pattern2)).all(axis=1))[0][0]\n",
    "        \n",
    "#         if (windows[next_ind] == pattern1).all():\n",
    "#             turb_d.append(next_ind)\n",
    "#         elif (windows[next_ind] == pattern2).all(): \n",
    "#             turb_e.append(next_ind) #append to list\n",
    "    \n",
    "#         windows[0:next_ind+(case)+1,:] = 0 #removes from windows\n",
    "    \n",
    "#     turb_d=np.array(turb_d,dtype=int); turb_e=np.array(turb_e,dtype=int)\n",
    "\n",
    "#     #EXTEND REST OF INDEXES TO PROCESS\n",
    "#     turb_d=extend_idxs(turb_d,case=case)\n",
    "#     turb_e=extend_idxs(turb_e,case=case)\n",
    "#     return turb_d,turb_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7222237-9b8f-4fd7-b02b-48474f252db7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ###### (amount of time inside/outside of cloud to count as entrainment/detrainment)\n",
    "# mins_thresh=5 #5 mins\n",
    "# ######\n",
    "\n",
    "# def get_changes(B):\n",
    "#     changes = np.diff(np.concatenate(([B[0]], B)))  # Add 0s to detect edges\n",
    "#     return changes\n",
    "# def PreProcessing(p,updraft_type):\n",
    "\n",
    "#     if updraft_type=='general':\n",
    "#         A=A_g.copy()\n",
    "#     elif updraft_type=='cloudy':\n",
    "#         A=A_c.copy()\n",
    "#     # B = A[:,p] #UNCOMMENT WHEN NOT TESTING***\n",
    "\n",
    "#     # Find the changes in the array\n",
    "#     changes=get_changes(B)\n",
    "#     # print(f'B = {B}'); print(f'changes = {changes}') \n",
    "\n",
    "#     #Determining the Case Number\n",
    "#     t_per_mins=1/((data['time'][1]-data['time'][0])/1e9/60).item() #timesteps per minute (<=1)\n",
    "#     case=int(t_per_mins*mins_thresh) #UNCOMMENT WHEN NOT TESTING***\n",
    "    \n",
    "#     if case>1:\n",
    "#         for case_ind in np.arange(case,0,-1): \n",
    "#         # for case_ind in [case]:\n",
    "#             #Calling Algorithm and Correcting Parcel Data\n",
    "#             [turb_d,turb_e]=find_sandwiched_patterns(changes, case=case_ind)\n",
    "#             B[turb_d]=1\n",
    "#             B[turb_e]=0     \n",
    "#             changes=get_changes(B)\n",
    "#             # print(B)\n",
    "#     elif case==1:\n",
    "#         #Calling Algorithm and Correcting Parcel Data\n",
    "#         [turb_d,turb_e]=find_sandwiched_patterns(changes, case=case)\n",
    "#         B[turb_d]=1\n",
    "#         B[turb_e]=0\n",
    "#     return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eacde93-34b1-4690-a4fa-523ac6fa70d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
