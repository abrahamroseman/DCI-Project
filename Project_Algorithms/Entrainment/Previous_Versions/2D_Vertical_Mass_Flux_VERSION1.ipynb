{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a13b10a-9f17-40ec-b1f5-1e56aa532b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in Packages and Data\n",
    "\n",
    "#Importing Packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import matplotlib.gridspec as gridspec\n",
    "import xarray as xr\n",
    "import os; import time\n",
    "import pickle\n",
    "import h5py\n",
    "###############################################################\n",
    "def coefs(coefficients,degree):\n",
    "    coef=coefficients\n",
    "    coefs=\"\"\n",
    "    for n in range(degree, -1, -1):\n",
    "        string=f\"({coefficients[len(coef)-(n+1)]:.1e})\"\n",
    "        coefs+=string + f\"x^{n}\"\n",
    "        if n != 0:\n",
    "            coefs+=\" + \"\n",
    "    return coefs\n",
    "###############################################################\n",
    "\n",
    "#Importing Model Data\n",
    "check=False\n",
    "dir='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "\n",
    "# # dx = 1 km; Np = 1M; Nt = 5 min\n",
    "# data=xr.open_dataset(dir+'../cm1r20.3/run/cm1out_1km_5min.nc') #***\n",
    "# parcel=xr.open_dataset(dir+'../cm1r20.3/run/cm1out_pdata_1km_5min_1e6.nc') #***\n",
    "# res='1km'\n",
    "# Np_str='1e6'\n",
    "\n",
    "# dx = 1km; Np = 50M\n",
    "#Importing Model Data\n",
    "check=False\n",
    "dir2='/home/air673/koa_scratch/'\n",
    "data=xr.open_dataset(dir2+'cm1out_1km_1min.nc') #***\n",
    "parcel=xr.open_dataset(dir2+'cm1out_pdata_1km_1min_50M.nc') #***\n",
    "res='1km'; t_res='1min'; Np_str='50e6'\n",
    "\n",
    "# # dx = 1km; Np = 100M\n",
    "# #Importing Model Data\n",
    "# check=False\n",
    "# dir2='/home/air673/koa_scratch/'\n",
    "# data=xr.open_dataset(dir2+'cm1out_1km_1min.nc') #***\n",
    "# parcel=xr.open_dataset(dir2+'cm1out_pdata_1km_1min_100M.nc') #***\n",
    "# res='1km'; t_res='1min'; Np_str='100e6'\n",
    "\n",
    "\n",
    "# dx = 250 m\n",
    "# #Importing Model Data\n",
    "# check=False\n",
    "# dir2='/home/air673/koa_scratch/'\n",
    "# data=xr.open_dataset(dir2+'cm1out_250m.nc') #***\n",
    "# parcel=xr.open_dataset(dir2+'cm1out_pdata_250m.nc') #***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0ab1d3-0431-4ced-80dd-1593f34ada57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "dir2='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "path=dir2+'../Functions/'\n",
    "sys.path.append(path)\n",
    "\n",
    "import NumericalFunctions\n",
    "from NumericalFunctions import * # import NumericalFunctions \n",
    "import PlottingFunctions\n",
    "from PlottingFunctions import * # import PlottingFunctions\n",
    "\n",
    "\n",
    "# # Get all functions in NumericalFunctions\n",
    "# import inspect\n",
    "# functions = [f[0] for f in inspect.getmembers(NumericalFunctions, inspect.isfunction)]\n",
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cd51d3-1a9d-499b-8784-a544e1c9c258",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INITIALIZE DATA FUNCTION\n",
    "###############################################################\n",
    "def initiate_array(out_file,vars,t_chunk_size,z_chunk_size,t_size=None,z_size=None):\n",
    "    # Define array dimensions (adjust based on your data)\n",
    "\n",
    "    if t_size==None:\n",
    "        t_size = len(data['time'])  # Number of timesteps\n",
    "    if z_size==None:\n",
    "        z_size = len(data['zh'])    # Number of vertical levels\n",
    "    \n",
    "    with h5py.File(out_file, 'w') as f: \n",
    "        # Check if the dataset 'theta_e' already exists\n",
    "        for var_name in vars:\n",
    "            if var_name not in f:\n",
    "                # Create a dataset with the full size for all time steps (initially empty)\n",
    "                f.create_dataset(var_name, \n",
    "                                 (t_size, z_size),  # Full size for all timesteps\n",
    "                                 chunks=(t_chunk_size, z_chunk_size))  # Chunks for time axis to allow resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa779832-7402-4686-badc-b119dc7f8c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JOB ARRAY SETUP\n",
    "job_array=False\n",
    "job_array=True\n",
    "\n",
    "if job_array==True:\n",
    "\n",
    "    num_jobs=180 #how many total jobs are being run? i.e. array=1-100 ==> num_jobs=100 #***\n",
    "    total_elements=len(data['time']) #total num of variables\n",
    "\n",
    "    if num_jobs >= total_elements:\n",
    "        raise ValueError(\"Number of jobs cannot be greater than or equal to total elements.\")\n",
    "    \n",
    "    job_range = total_elements // num_jobs  # Base size for each chunk\n",
    "    remaining = total_elements % num_jobs   # Number of chunks with 1 extra \n",
    "    \n",
    "    # Function to compute the start and end for each job_id\n",
    "    def get_job_range(job_id, num_jobs):\n",
    "        job_id-=1\n",
    "        # Add one extra element to the first 'remaining' chunks\n",
    "        start_job = job_id * job_range + min(job_id, remaining)\n",
    "        end_job = start_job + job_range + (1 if job_id < remaining else 0)\n",
    "    \n",
    "        if job_id == num_jobs - 1: \n",
    "            end_job = total_elements #- 1\n",
    "        return start_job, end_job\n",
    "    # def job_testing():\n",
    "    #     #TESTING\n",
    "    #     start=[];end=[]\n",
    "    #     for job_id in range(1,num_jobs+1):\n",
    "    #         start_job, end_job = get_job_range(job_id)\n",
    "    #         print(start_job,end_job)\n",
    "    #         start.append(start_job)\n",
    "    #         end.append(end_job)\n",
    "    #     print(np.all(start!=end))\n",
    "    #     print(len(np.unique(start))==len(start))\n",
    "    #     print(len(np.unique(end))==len(end))\n",
    "    # job_testing()\n",
    "    \n",
    "    job_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0)) #this is the current SBATCH job id\n",
    "    if job_id==0: job_id=135\n",
    "    start_job, end_job = get_job_range(job_id, num_jobs)\n",
    "    index_adjust=start_job\n",
    "    print(f'start_job = {start_job}, end_job = {end_job}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cd22c2-5e79-42e5-a1de-b42a742a9383",
   "metadata": {},
   "outputs": [],
   "source": [
    "if job_array==True:\n",
    "    #Indexing Array with JobArray\n",
    "    data=data.isel(time=slice(start_job,end_job))\n",
    "    parcel=parcel.isel(time=slice(start_job,end_job))\n",
    "    #(for 150_000_000 parcels use 500-1000 jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173dde25-4073-4cfb-b831-fe04eaf1c9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Back Data Later\n",
    "##############\n",
    "def make_data_dict(in_file,var_names,read_type):\n",
    "    if read_type=='h5py':\n",
    "        with h5py.File(in_file, 'r') as f:\n",
    "            if job_array==True:\n",
    "                data_dict = {var_name: f[var_name][start_job:end_job] for var_name in var_names}\n",
    "            elif job_array==False:\n",
    "                data_dict = {var_name: f[var_name][:] for var_name in var_names}\n",
    "            \n",
    "    elif read_type=='xarray':\n",
    "        in_data = xr.open_dataset(\n",
    "            in_file,\n",
    "            engine='h5netcdf',\n",
    "            phony_dims='sort',\n",
    "            chunks={'phony_dim_0': 100, 'phony_dim_1': 1_000_000} \n",
    "        )\n",
    "        if job_array==True:\n",
    "            data_dict = {k: in_data[k][start_job:end_job].compute().data for k in var_names}\n",
    "        elif job_array==False:\n",
    "            data_dict = {k: in_data[k][:].compute().data for k in var_names}\n",
    "    return data_dict\n",
    "# read_type='xarray'\n",
    "read_type='h5py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f850b5ea-a193-47c8-b469-cb87db099a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "dir2=dir+'Project_Algorithms/Lagrangian_Arrays/'\n",
    "in_file=dir2+f'lagrangian_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "\n",
    "var_names = ['A_g', 'A_c', 'W', 'Z', 'Y', 'X']\n",
    "data_dict = make_data_dict(in_file,var_names,read_type)\n",
    "A_g, A_c, W, Z, Y, X = (data_dict[k] for k in var_names)\n",
    "\n",
    "# #Making Time Matrix\n",
    "Nt=len(data['time'])\n",
    "T = np.broadcast_to(np.arange(Nt)[:, None], A_c.shape)  # shape: (Nt, p)\n",
    "\n",
    "check_memory(globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e12d46-35a9-489c-824d-0734042243d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#READING BACK IN\n",
    "dir2=dir+'Project_Algorithms/Entrainment/'\n",
    "in_file=dir2+f'processed_binary_arrays_{res}_{t_res}_{Np_str}.h5'\n",
    "\n",
    "var_names = ['A_g_Processed', 'A_c_Processed']\n",
    "data_dict = make_data_dict(in_file,var_names,read_type)\n",
    "A_g_Processed, A_c_Processed = (data_dict[k] for k in var_names)\n",
    "check_memory(globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff695397-158c-43ec-9c74-04a10c46e289",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458f6c37-6d32-45df-8f86-5de29b9524b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VMF2d(A, T, Z):\n",
    "    start_time = time.time()\n",
    "    \"\"\"\n",
    "    Function to compute 2D Mass Flux and update result array based on provided inputs.\n",
    "    \n",
    "    Returns a 2D (t,z) array containing the sum of the D array represented by parcels in cloudy updrafts by 1.\n",
    "    The finally array is then ordered by the appropiate index using the np.add.at function.\n",
    "    \n",
    "    Parameters:\n",
    "    - A: The (t,p) lagrangian binary array.\n",
    "    - T: The (t,p) lagrangian time index array.\n",
    "    - Z: The (t,p) Lagrangian z index array.\n",
    "\n",
    "    \"\"\"\n",
    "    # Compute the difference between neighboring elements along the first axis\n",
    "    D = A * W\n",
    "    \n",
    "    # # Update D for entrainment/detrainment\n",
    "    # if type=='e':\n",
    "    #     D[D <= 0] = 0\n",
    "    # elif type=='d':\n",
    "    #     D[D >= 0] = 0\n",
    "    \n",
    "    # Initialize time and vertical dimension arrays\n",
    "    Nt = len(data['time']); Nz = len(data['zh'])\n",
    "    \n",
    "    # Initialize result array\n",
    "    result = np.zeros((Nt, Nz))\n",
    "    \n",
    "    # Use np.add.at to accumulate values in the result array\n",
    "    np.add.at(result, (T, Z), D)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Execution time: {(end_time - start_time)} seconds\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa8e37b-5e77-4b00-9552-5aaf902ddf6f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #TESTING TESTING TESTING\n",
    "# from collections import defaultdict\n",
    "# def find_repeated_columns(arr):\n",
    "#     \"\"\"\n",
    "#     Given a 2D array of shape (4, N), find indices of columns that repeat (are identical).\n",
    "#     Returns a list of index arrays, where each array contains indices of repeated columns.\n",
    "#     \"\"\"\n",
    "#     arr_T = arr.T  # shape becomes (N, 4)\n",
    "    \n",
    "#     # Dictionary to store unique rows and their corresponding indices\n",
    "#     row_dict = defaultdict(list)\n",
    "    \n",
    "#     for idx, row in enumerate(arr_T):\n",
    "#         row_tuple = tuple(row)\n",
    "#         row_dict[row_tuple].append(idx)\n",
    "    \n",
    "#     # Return only groups with repeated columns (length >= 2)\n",
    "#     repeated_indices = [np.array(indices) for indices in row_dict.values() if len(indices) > 1]\n",
    "    \n",
    "#     return repeated_indices\n",
    "\n",
    "# # Example usage\n",
    "# arr1 = np.array([1, 2,2, 3, 4, 4, 5, 6,6,6])\n",
    "# arr2 = np.array([1, 3,3, 3, 4, 4, 5, 6,6,6])\n",
    "# arr3 = np.array([1, 4,4, 3, 4, 4, 5, 6,6,6])\n",
    "# arr4 = np.array([1, 5,5, 3, 4, 4, 5, 6,6,6])\n",
    "# arr = np.array([arr1, arr2, arr3, arr4])\n",
    "\n",
    "# result = find_repeated_columns(arr)\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48004fb2-c7ed-42b3-8174-ef9942649f9e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #TESTING TESTING TESTING #*#*\n",
    "# where=np.where(A==1)\n",
    "# arr1=T[where]\n",
    "# arr2=Z[where]\n",
    "# arr3=Y[where]\n",
    "# arr4=X[where] \n",
    "# arrs=np.array([arr2,arr3,arr4]) \n",
    "# arrs\n",
    "\n",
    "# out=find_repeated_columns(arrs)\n",
    "# print(out)\n",
    "\n",
    "# # one=where[0][1],where[1][1]\n",
    "# # two=where[0][2],where[1][2]\n",
    "# # one,two\n",
    "# # T[one],Z[one],Y[one],X[one]\n",
    "# # T[two],Z[two],Y[two],X[two]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04ac09f-5afb-42b1-b187-2574f69a10bd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #TESTING TESTING TESTING\n",
    "# def apply_function(A):\n",
    "#     # A1=A.copy() #TESTING\n",
    "\n",
    "#     where=np.where(A==1)\n",
    "#     arr1=T[where]\n",
    "#     arr2=Z[where]\n",
    "#     arr3=Y[where]\n",
    "#     arr4=X[where] \n",
    "#     arr=np.array([arr1,arr2,arr3,arr4]) \n",
    "\n",
    "#     out = find_repeated_columns(arr)\n",
    "#     # print(out)\n",
    "\n",
    "#     for ind in np.arange(len(out)):\n",
    "#         extras=out[ind][1:]\n",
    "#         A[(where[0][extras],where[1][extras])]=0\n",
    "\n",
    "#     # A2=A.copy() #TESTING\n",
    "#     # print(np.all(A1!=A2)) #TESTING\n",
    "\n",
    "#     #TESTING \n",
    "#     # for k in range(len(out)):\n",
    "#     #     ind1=out[k][0]\n",
    "#     #     ind2=out[k][1]\n",
    "#     #     print((arr1[ind1],arr2[ind1],arr3[ind1],arr4[ind1])==(arr1[ind2],arr2[ind2],arr3[ind2],arr4[ind2]))\n",
    "#     return A\n",
    "\n",
    "# # apply_function(A) #TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99593da-48c3-4c27-a1c4-89c3aa5c54a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TURN PROCESSING ON OR OFF\n",
    "PROCESSING=False\n",
    "PROCESSING=True\n",
    "\n",
    "# Set A based on PROCESSING state\n",
    "print('Calculating 2D VMF for General Updrafts')\n",
    "A = A_g if (PROCESSING==False) else A_g_Processed\n",
    "# A=apply_function(A) #TESTING TESTING TESTING\n",
    "profile_array_VMF_g = VMF2d(A, T, Z)\n",
    "\n",
    "\n",
    "# Set A for the second block\n",
    "print('Calculating 2D VMF for Cloudy Updrafts')\n",
    "A = A_c if (PROCESSING==False) else A_c_Processed\n",
    "# A=apply_function(A) #TESTING TESTING TESTING\n",
    "profile_array_VMF_c = VMF2d(A, T, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f839b7a9-cf0b-48dc-a575-73c481fa575c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if job_array==True:\n",
    "    dir2=dir+'Project_Algorithms/Entrainment/job_out_4/'\n",
    "elif job_array==False:\n",
    "    dir2=dir+'Project_Algorithms/Entrainment/'\n",
    "\n",
    "#SAVING\n",
    "if PROCESSING==False:\n",
    "    out_file=dir2+f'2D_VMF_profiles_{res}_{t_res}_{Np_str}'\n",
    "elif PROCESSING==True:\n",
    "    out_file=dir2+f'2D_VMF_profiles_PREPROCESSING_{res}_{t_res}_{Np_str}'\n",
    "    \n",
    "if job_array==True:\n",
    "    out_file+=f'_{job_id}.h5'\n",
    "elif job_array==False:\n",
    "    out_file+=f'.h5'\n",
    "\n",
    "vars=[\"profile_array_VMF_g\",\"profile_array_VMF_c\"]\n",
    "initiate_array(out_file,vars,t_chunk_size=1,z_chunk_size=17)\n",
    "\n",
    "with h5py.File(out_file, 'a') as f: \n",
    "    f['profile_array_VMF_g'][:]=profile_array_VMF_g\n",
    "    f['profile_array_VMF_c'][:]=profile_array_VMF_c\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b76c897-4012-41c3-8100-5bc1b0e3c196",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_memory(globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485d7329-d52c-4dfe-85ca-d8f633d7fb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "#RECOMBINE SEPERATE JOB_ARRAYS AFTER\n",
    "recombine=False #KEEP FALSE DURING JOB ARRAY RUN\n",
    "# recombine=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368341e7-d858-43d4-94e0-e80862f8c557",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if recombine==True:\n",
    "    PROCESSING=False\n",
    "    PROCESSING=True\n",
    "    \n",
    "    dir2=dir+'Project_Algorithms/Entrainment/job_out_4/'\n",
    "    dir3=dir+'Project_Algorithms/Entrainment/'\n",
    "    \n",
    "    if PROCESSING==False:\n",
    "        out_file=dir3+f'2D_VMF_profiles_{res}_{t_res}_{Np_str}.h5'\n",
    "    elif PROCESSING==True:\n",
    "        out_file=dir3+f'2D_VMF_profiles_PREPROCESSING_{res}_{t_res}_{Np_str}.h5'\n",
    "    \n",
    "    vars=[\"profile_array_VMF_g\",\"profile_array_VMF_c\"]\n",
    "    initiate_array(out_file,vars,t_chunk_size=50,z_chunk_size=17)\n",
    "    \n",
    "    with h5py.File(out_file, 'r+') as f_out:\n",
    "        num_jobs=180\n",
    "        for job_id in np.arange(1,num_jobs+1):\n",
    "            if np.mod(job_id,5)==0: print(f\"job_id = {job_id}\")\n",
    "            [a,b] = get_job_range(job_id,num_jobs)\n",
    "    \n",
    "            if PROCESSING==False:\n",
    "                in_file=dir2+f'2D_VMF_profiles_{res}_{t_res}_{Np_str}_{job_id}.h5'\n",
    "            elif PROCESSING==True:\n",
    "                in_file=dir2+f'2D_VMF_profiles_PREPROCESSING_{res}_{t_res}_{Np_str}_{job_id}.h5'\n",
    "            with h5py.File(in_file, 'r') as f_in: \n",
    "                for var in vars:\n",
    "                    f_out[var][a:b]=f_in[var][:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
