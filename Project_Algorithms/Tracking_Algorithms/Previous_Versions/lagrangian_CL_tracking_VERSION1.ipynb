{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969c6cb7-1643-4447-88ec-a5330c725265",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in Packages and Data\n",
    "\n",
    "#Importing Packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import matplotlib.gridspec as gridspec\n",
    "import xarray as xr\n",
    "import os; import time\n",
    "import pickle\n",
    "import h5py\n",
    "###############################################################\n",
    "def coefs(coefficients,degree):\n",
    "    coef=coefficients\n",
    "    coefs=\"\"\n",
    "    for n in range(degree, -1, -1):\n",
    "        string=f\"({coefficients[len(coef)-(n+1)]:.1e})\"\n",
    "        coefs+=string + f\"x^{n}\"\n",
    "        if n != 0:\n",
    "            coefs+=\" + \"\n",
    "    return coefs\n",
    "###############################################################\n",
    "start_time = time.time();\n",
    "\n",
    "#Importing Model Data\n",
    "check=False\n",
    "dir='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "job_array=False;index_adjust=0\n",
    "\n",
    "# dx = 1 km; Np = 1M; Nt = 5 min\n",
    "data=xr.open_dataset(dir+'../cm1r20.3/run/cm1out_1km_5min.nc') #***\n",
    "parcel=xr.open_dataset(dir+'../cm1r20.3/run/cm1out_pdata_1km_5min_1e6.nc') #***\n",
    "t_res='5min'; res='1km'\n",
    "Np_str='1e6'\n",
    "\n",
    "# # dx = 1km; Np = 50M\n",
    "# #Importing Model Data\n",
    "# check=False\n",
    "# dir2='/home/air673/koa_scratch/'\n",
    "# data=xr.open_dataset(dir2+'cm1out_1km_1min.nc') #***\n",
    "# parcel=xr.open_dataset(dir2+'cm1out_pdata_1km_1min_50M.nc') #***\n",
    "# res='1km'; t_res='1min'; Np_str='50e6'\n",
    "\n",
    "# # dx = 1km; Np = 100M\n",
    "# #Importing Model Data\n",
    "# check=False\n",
    "# dir2='/home/air673/koa_scratch/'\n",
    "# data=xr.open_dataset(dir2+'cm1out_1km_1min.nc') #***\n",
    "# parcel=xr.open_dataset(dir2+'cm1out_pdata_1km_1min_100M.nc') #***\n",
    "# res='1km'; t_res='1min'; Np_str='100e6'\n",
    "\n",
    "# # dx = 250 m\n",
    "# #Importing Model Data\n",
    "# check=False\n",
    "# dir2='/home/air673/koa_scratch/'\n",
    "# data=xr.open_dataset(dir2+'cm1out_250m.nc') #***\n",
    "# parcel=xr.open_dataset(dir2+'cm1out_pdata_250m.nc') #***\n",
    "# res='250m'\n",
    "# Np_str='150e6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11162238-d04e-47e9-aab8-74fddfc51c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_memory():\n",
    "    import sys\n",
    "    ipython_vars = [\"In\", \"Out\", \"exit\", \"quit\", \"get_ipython\", \"ipython_vars\"]\n",
    "    print(\"Top 10 objects with highest memory usage\")\n",
    "    # Get a sorted list of the objects and their sizes\n",
    "    mem = {\n",
    "        key: round(value/1e6,2)\n",
    "        for key, value in sorted(\n",
    "            [\n",
    "                (x, sys.getsizeof(globals().get(x)))\n",
    "                for x in globals()\n",
    "                if not x.startswith(\"_\") and x not in sys.modules and x not in ipython_vars\n",
    "            ],\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True)[:10]\n",
    "    }\n",
    "    print({key:f\"{value} MB\" for key,value in mem.items()})\n",
    "    print(f\"\\n{round(sum(mem.values()),2)/1000} GB in use overall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4430f783-c58e-4585-ad82-e8a9bd3b018b",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "\n",
    "times=data['time'].values/(1e9 * 60); times=times.astype(float);\n",
    "minutes=1/times[1] #1 / minutes per timestep = timesteps per minute\n",
    "kms=np.argmax(data['xh'].values-data['xh'][0].values >= 1)\n",
    "\n",
    "#LOADING CL MAXS FROM CL TRACKING ALGORITHM\n",
    "folder = '/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/Project_Algorithms/Tracking_Algorithms/'\n",
    "open_name = folder+f'whereCL_{res}_{t_res}_ALL_CLS.nc'\n",
    "whereCL=xr.open_dataset(open_name).load()\n",
    "whereCL=whereCL.isel(time=slice(0,len(data['time'])))\n",
    "whereCL=whereCL['maxconv_x']\n",
    "def Get_Conv_X(t,z,y):\n",
    "    Conv_X_Max=whereCL[t,z,y,:].values\n",
    "    return Conv_X_Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95408a4c-65cd-4a3c-8953-edc972bb5d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JOB ARRAY SETUP\n",
    "job_array=True\n",
    "if job_array==True:\n",
    "\n",
    "    num_jobs=60 #how many total jobs are being run? i.e. array=1-100 ==> num_jobs=100 #***\n",
    "    total_elements=len(parcel['xh']) #total num of variables\n",
    "\n",
    "    if num_jobs >= total_elements:\n",
    "        raise ValueError(\"Number of jobs cannot be greater than or equal to total elements.\")\n",
    "    \n",
    "    job_range = total_elements // num_jobs  # Base size for each chunk\n",
    "    remaining = total_elements % num_jobs   # Number of chunks with 1 extra \n",
    "    \n",
    "    # Function to compute the start and end for each job_id\n",
    "    def get_job_range(job_id):\n",
    "        job_id-=1\n",
    "        # Add one extra element to the first 'remaining' chunks\n",
    "        start_job = job_id * job_range + min(job_id, remaining)\n",
    "        end_job = start_job + job_range + (1 if job_id < remaining else 0)\n",
    "    \n",
    "        if job_id == num_jobs - 1: \n",
    "            end_job = total_elements #- 1\n",
    "        return start_job, end_job\n",
    "    # def job_testing():\n",
    "    #     #TESTING\n",
    "    #     start=[];end=[]\n",
    "    #     for job_id in range(1,num_jobs+1):\n",
    "    #         start_job, end_job = get_job_range(job_id)\n",
    "    #         print(start_job,end_job)\n",
    "    #         start.append(start_job)\n",
    "    #         end.append(end_job)\n",
    "    #     print(np.all(start!=end))\n",
    "    #     print(len(np.unique(start))==len(start))\n",
    "    #     print(len(np.unique(end))==len(end))\n",
    "    # job_testing()\n",
    "    \n",
    "    job_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0)) #this is the current SBATCH job id\n",
    "    if job_id==0: job_id=60\n",
    "    start_job, end_job = get_job_range(job_id)\n",
    "    index_adjust=start_job\n",
    "    print(f'start_job = {start_job}, end_job = {end_job}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106afd28-31a6-4821-aba8-8bc6295bceca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SUBSETTING PARCEL DATA\n",
    "parcel=parcel.isel(xh=slice(start_job,end_job))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0b814b-8426-4310-a5e1-465fdb1aecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Back Data Later\n",
    "##############\n",
    "import h5py\n",
    "dir2=dir+'Project_Algorithms/Lagrangian_Arrays/'\n",
    "open_file=dir2+f'lagrangian_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "with h5py.File(open_file, 'r') as f:\n",
    "    parcel_z = f['z'][:,start_job:end_job]\n",
    "    parcel_x = f['x'][:,start_job:end_job] #*#\n",
    "    \n",
    "    # Load the dataset by its name\n",
    "\n",
    "    W = f['W'][:,start_job:end_job]\n",
    "    Z = f['Z'][:,start_job:end_job]\n",
    "    Y = f['Y'][:,start_job:end_job]\n",
    "    X = f['X'][:,start_job:end_job]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e428e62d-8d62-4411-b2a4-f6e84aebec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reading Back Data Later\n",
    "# ##############\n",
    "# import h5py\n",
    "# dir2=dir+'Project_Algorithms/Lagrangian_Binary_Array/'\n",
    "# with h5py.File(dir2+f'LFC_LCL_binary_array.h5', 'r') as f:\n",
    "#     # Load the dataset by its name\n",
    "#     LFC = f['LFC'][:]\n",
    "#     LCL = f['LCL'][:]\n",
    "\n",
    "# Reading Back Data Later\n",
    "##############\n",
    "import h5py\n",
    "dir2=dir+'Project_Algorithms/Lagrangian_Arrays/'\n",
    "open_file=dir2+f'LFC_LCL_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "with h5py.File(open_file, 'r') as f:\n",
    "    # Load the dataset by its name\n",
    "    LFC = f['LFC'][:,start_job:end_job]\n",
    "    LCL = f['LCL'][:,start_job:end_job]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7574d52f-94ec-4678-a84b-454c1d6e2efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc9aece-9953-47b6-a4a5-0e03463efcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e02e69-4333-411f-8340-6781385925b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updated Lagrangian Tracking Algorithm\n",
    "\n",
    "#Algorithm Steps:\n",
    "#(1) Find the first time a parcel is above the LFC:\n",
    "#(2) First check if the parcel ascends (w>=0.1) for another 20 minutes\n",
    "#(3) If so, find first time, the parcel slows down (w<0.1)\n",
    "#(4) If that time is when the parcel is above 750m, save it, \"forget\", and move on to next parcel\n",
    "#(5) If that time is when the parcel is below 750m, check if it is within 2km of the CL_Max found from the CL Tracking Algorithm\n",
    "#(6) If the parcel is near the CL, store in, otherwise save it, \"forget\", and move on to next parcel\n",
    "#(7) Continue to next parcel\n",
    "\n",
    "#(Also, if during, traceback, the parcel escapes the x or z boundary, \"forget\" parcel, and move on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e645e76-ec24-4464-bc25-f4d9632cf0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numerical Settings\n",
    "Nt=len(data['time'])\n",
    "Np=len(parcel['xh'])\n",
    "dt=times[1]*60\n",
    "print(f\"(Nt,Np,dt) {Nt,Np,dt}\")\n",
    "\n",
    "#For saving ascend-after-LFC info\n",
    "ascend_lst=[]\n",
    "CLmaxheight=750 #750m\n",
    "\n",
    "#BL slow-down-threshold\n",
    "w_thresh=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f84c8e-4813-4d84-b6ad-3aa7419ef274",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #OLD CODE (USE BELOW INSTEAD)\n",
    "# t_by_t=False#LOAD IN ALL AT ONCE (TOO SLOW)\n",
    "# # t_by_t=True\n",
    "\n",
    "# if t_by_t==False:\n",
    "#     print('loading in lagrangian u and w')\n",
    "#     parcel_u=parcel['u'].data \n",
    "#     parcel_w=parcel['w'].data\n",
    "#     print('done')\n",
    "\n",
    "# elif t_by_t==True:\n",
    "#     Nt = len(parcel['time'])\n",
    "#     Np = len(parcel['xh'])  \n",
    "    \n",
    "#     # Initialize final output array\n",
    "#     def create_empty():\n",
    "#         out=np.empty((Nt, Np), dtype=np.float32)\n",
    "#         return out\n",
    "#     parcel_u = create_empty()\n",
    "#     parcel_w = create_empty()\n",
    "    \n",
    "#     # Load and process timestep-by-timestep\n",
    "#     for t in range(Nt):\n",
    "#         if np.mod(t,50)==0: print(f'time = {t}')\n",
    "#         parcel_u[t, :] = parcel['u'].isel(time=t).data\n",
    "#         parcel_w[t, :] = parcel['w'].isel(time=t).data\n",
    "#     print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7906d1e1-78a2-445f-a880-5560a248055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#u,w are now added to Lagrangian_Binary_Array results, no need for code above\n",
    "# Reading Back Data Later\n",
    "##############\n",
    "import h5py\n",
    "dir2=dir+'Project_Algorithms/Lagrangian_Arrays/'\n",
    "open_file=dir2+f'lagrangian_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "with h5py.File(open_file, 'r') as f:\n",
    "    parcel_u = f['u'][:,start_job:end_job]\n",
    "    parcel_w = f['w'][:,start_job:end_job] #*#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d007b1c-cec8-46e1-8507-677ab58459a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_memory() #Needs about 3.25G per job ==> request 8G (actually 12G needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f5e82e-6419-49c5-901e-dc7de91030da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if ((x + dt*u)==0) or ((z + dt*w)==0)\n",
    "# u=u[t,Z[t,p],Y[t,p],X[t,p]]; W=W[t,p]\n",
    "# [u[t,Z[t,p],Y[t,p],X[t,p]] for t in time_arr] >np.max(data['xf'].values) or < np.min(data['xf'].values)\n",
    "# similarly for w\n",
    "################################################################################################################\n",
    "#BOUNDARY-ESCAPE CONDITION\n",
    "xmin=np.min(data['xf'].values)*1e3\n",
    "xmax=np.max(data['xf'].values)*1e3\n",
    "zmin=np.min(data['zf'].values)*1e3\n",
    "zmax=np.max(data['zf'].values)*1e3\n",
    "\n",
    "def check_boundary(p,where_BL,above_LFC):\n",
    "    time_arr=np.arange(where_BL,above_LFC)\n",
    "\n",
    "    def get_x(t,p):\n",
    "        # return parcel['x'][t,p].item()\n",
    "        return parcel_x[t,p] \n",
    "    def get_u(t,p):\n",
    "        # return data['uinterp'].isel(time=t,zh=Z[t,p],yh=Y[t,p],xh=X[t,p]).item() #TESTING\n",
    "        # return parcel['u'][t,p].item() \n",
    "        return parcel_u[t,p]\n",
    "    def get_z(t,p):\n",
    "        # return parcel['z'][t,p].item()\n",
    "        return parcel_z[t,p]\n",
    "    def get_w(t,p):\n",
    "        # return data['winterp'].isel(time=t,zh=Z[t,p],yh=Y[t,p],xh=X[t,p]).item()\n",
    "        # return parcel['w'][t,p].item()\n",
    "        return parcel_w[t,p]\n",
    "        \n",
    "\n",
    "    # x_tend = [get_x(t, p) + dt * get_u(t, z, y, x)   #THIS IS OLD, LESS IDEAL\n",
    "    #       for (t, z, y, x) in zip(time_arr, Z[time_arr, p], Y[time_arr, p], X[time_arr, p])] \n",
    "    # z_tend = [get_z(t, p) + dt * get_w(t, z, y, x)  \n",
    "    #       for (t, z, y, x) in zip(time_arr, Z[time_arr, p], Y[time_arr, p], X[time_arr, p])] \n",
    "    \n",
    "    x_tend = [get_x(t, p) + dt * get_u(t,p)   \n",
    "          for (t, z, y, x) in zip(time_arr, Z[time_arr, p], Y[time_arr, p], X[time_arr, p])] \n",
    "    z_tend = [get_z(t, p) + dt * get_w(t,p)  \n",
    "          for (t, z, y, x) in zip(time_arr, Z[time_arr, p], Y[time_arr, p], X[time_arr, p])] \n",
    "\n",
    "    x_bound=any(val < xmin or val > xmax for val in x_tend)*1\n",
    "    z_bound=any(val < zmin or val > zmax for val in z_tend)*1\n",
    "\n",
    "    out=(x_bound,z_bound)\n",
    "    if out[0]==1:\n",
    "        print(f'parcel {p} crossed x-boundary between t={where_BL} and t={above_LFC}')\n",
    "    elif out[1]==1:\n",
    "        print(f'parcel {p} crossed z-boundary between t={where_BL} and t={above_LFC}')\n",
    "    return out\n",
    "#############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0eed3f-6b8c-4230-9abc-37e560986dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################################\n",
    "#The Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe292a9e-8f61-403c-8206-7cfd130c18c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Output Storage Vector\n",
    "\n",
    "#int 32 can store up to the number 2,147,483,647 \n",
    "#int 32 has 4 bytes per number, so needs (Np*3)*4 bytes of memory\n",
    "#Np=125000 ==> (125000*3*4)/(1024**3) = 0.001 GB\n",
    "#Np=50e6 ==> (50e6*3*4)/(1024**3) = 0.56 GB\n",
    "\n",
    "out_arr=np.zeros((Np,3),dtype=np.int32) \n",
    "save_arr=np.zeros((Np,3),dtype=np.int32) #This one is for saving continued-ascent, slow-below-750m parcels that are not with 2 km of CL\n",
    "save2_arr=np.zeros((Np,3),dtype=np.int32) #This one is for saving continued-ascent, slow-above-750m parcels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1f6fe0-558b-4087-8ba1-f6ca642d5b4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#1--------------Looping over each parcel\n",
    "for count,p in enumerate(np.arange(Np)): \n",
    "\n",
    "    if np.mod(p,5e4)==0: print(f'current parcel: {p}/{Np}')\n",
    "    \n",
    "    W_p = W[:,p]\n",
    "    LFC_p = LFC[:,p] \n",
    "   \n",
    "    #----FIND WHERE PARCEL IS ABOVE LFC----\n",
    "    indices = np.where(LFC_p == 1)[0]; above_LFC = indices[0] if indices.size > 0 else -999; #FIRST TIME ABOVE LFC\n",
    "    if above_LFC ==-999:\n",
    "        # print(f'parcel {p} never above LFC')\n",
    "        continue #if the parcel is never above the LFC, skip the parcel\n",
    "    \n",
    "    #----CHECK IF ASCENDS FOR >= 20 minutes AFTER LFC----\n",
    "    ascend_array=W_p[above_LFC+1:]\n",
    "    indices=np.where(ascend_array==0)[0]; ascend_stop=indices[0] if indices.size > 0 else 10000; #location of where parcel stops ascending (labeled 10000 to mark for future analysis)\n",
    "    ascend_lst.append(ascend_stop) #(also store for histogram)\n",
    "    if ascend_stop>=20*minutes:\n",
    "    \n",
    "        #----FIND THE FIRST TIME W_p<=w_thresh----\n",
    "        indices=np.where(W_p[0:above_LFC]<w_thresh)[0]\n",
    "        where_BL=indices[-1] if indices.size > 0 else -999 #FIRST PRIOR TIME W<0.1 (IN THE BL) (ADDED 1 TO GET TIME RIGHT AFTER INTERACTION)\n",
    "        if where_BL ==-999:\n",
    "            # print(f'parcel {p} w is never below threshold prior to t={above_LFC}')\n",
    "            continue #if the parcel never slows down backwards in time (unlikely), skip the parcel\n",
    "            \n",
    "        #check for boundary escapes\n",
    "        ################################\n",
    "        future_location=check_boundary(p,where_BL,above_LFC)\n",
    "        if (future_location[0]+future_location[1]>=1): continue #if parcel crosses boundary, skips current parcel\n",
    "        ################################\n",
    "        \n",
    "        #----CHECK IF PARCEL SLOWED DOWN LOW ENOUGH----\n",
    "        if parcel_z[where_BL,p]<=CLmaxheight: #PARCEL MUST BE BELOW 750m WHEN CONTACTING CL #***\n",
    "        # if LCL[where_BL,p]==0: #PARCEL MUST BE BELOW LCL WHEN CONTACTING CL (not recommended)\n",
    "    \n",
    "            #----CHECK IF CL IS WITHIN 2km----\n",
    "            #Find the CL-max x-location\n",
    "            t=where_BL; z=Z[where_BL,p]; y=Y[where_BL,p]; x=X[where_BL,p]\n",
    "            CONV_X=Get_Conv_X(t,z,y)\n",
    "            within_CL=np.any(np.isin(CONV_X, np.arange(x-2*kms,x+3*kms)))\n",
    "            \n",
    "            if within_CL==True:\n",
    "                #save X's (t,p) \n",
    "                print(f'Parcel {p} is success at time {where_BL}')\n",
    "                out_arr[p,0]=p\n",
    "                out_arr[p,1]=where_BL\n",
    "                out_arr[p,2]=above_LFC \n",
    "            else: #continued-ascent, slow-below-750m parcels that are not with 2 km of CL\n",
    "                #SAVE PARCEL\n",
    "                # print(f'Parcel {p} not near CL at t={where_BL}')\n",
    "                save_arr[p,0]=p\n",
    "                save_arr[p,1]=where_BL\n",
    "                save_arr[p,2]=above_LFC \n",
    "    \n",
    "        else: #continued-ascent, slow-above-750m parcels\n",
    "            #SAVE PARCEL\n",
    "            # print(f'Parcel {p} above {CLmaxheight}m at t={where_BL}')\n",
    "            save2_arr[p,0]=p\n",
    "            save2_arr[p,1]=where_BL\n",
    "            save2_arr[p,2]=above_LFC         \n",
    "            \n",
    "        #END OF LOOP, THEN WE MOVE ON TO NEXT PARCEL p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eade392-bb34-4c0f-a93a-1cc1da5fea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CORRECTING DATA PARCEL ID BASED ON JOB NUMBER\n",
    "#####################################################\n",
    "out_arr[np.where(np.any(out_arr != 0, axis=1))[0],0]+=index_adjust #*needed for job array*+=index_adjust #*needed for job array*\n",
    "save_arr[np.where(np.any(save_arr != 0, axis=1))[0],0]+=index_adjust #*needed for job array*+=index_adjust #*needed for job array*\n",
    "save2_arr[np.where(np.any(save2_arr != 0, axis=1))[0],0]+=index_adjust #*needed for job array*+=index_adjust #*needed for job array*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b12a89-f869-4b35-81b5-5f1b56cacbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVING BLANK ROWS\n",
    "#####################################################\n",
    "def remove_zero_rows(arr):\n",
    "    arr = arr[~np.all(arr == 0, axis=1)]\n",
    "    return arr\n",
    "out_arr=remove_zero_rows(out_arr)\n",
    "save_arr=remove_zero_rows(save_arr)\n",
    "save2_arr=remove_zero_rows(save2_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbefa819-624e-4353-bd13-ed131d46539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Storing output and save data\n",
    "#USES H5 FILE\n",
    "###################################################################################################################################\n",
    "out_file = dir + f'Project_Algorithms/Tracking_Algorithms/trackout/parcel_tracking_{res}_{t_res}_{Np_str}_{job_id}.h5'\n",
    "with h5py.File(out_file, 'w') as hf:\n",
    "    hf.create_dataset('out_arr', data=out_arr, compression=\"gzip\")\n",
    "    hf.create_dataset('save_arr', data=save_arr, compression=\"gzip\")\n",
    "    hf.create_dataset('save2_arr', data=save2_arr, compression=\"gzip\")\n",
    "\n",
    "end_time = time.time(); elapsed_time = end_time - start_time; print(f\"Total Elapsed Time: {elapsed_time} seconds\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcf334b-8a14-47d9-b72a-c6a74104646a",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ea6027-6d89-4401-8b1c-7dd289b2a093",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #Storing output and save data\n",
    "# #OLD, USES NETCDF, H5 IS RECOMMENDED INSTEAD HERE\n",
    "# ###################################################################################################################################\n",
    "# ds=xr.Dataset({\n",
    "#     'out_arr': (['rows', 'columns'], out_arr.astype(float)),\n",
    "#     'save_arr': (['rows', 'columns'], save_arr.astype(float)),\n",
    "#     'save2_arr': (['rows', 'columns'], save2_arr.astype(float)),\n",
    "# })\n",
    "\n",
    "\n",
    "# out_file = dir+f'Project_Algorithms/Tracking_Algorithms/trackout/parcel_tracking_{res}_{t_res}_{Np_str}_{job_id}.nc'\n",
    "# ds.to_netcdf(out_file)\n",
    "# ###########################################################################################################################\n",
    "# end_time = time.time(); elapsed_time = end_time - start_time; print(f\"Total Elapsed Time: {elapsed_time} seconds\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7a78cc-b1ba-4495-a482-dc791048a1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "#Run after finishing job_array\n",
    "recombine=False #KEEP FALSE WHEN JOB_ARRAY IS RUNNING\n",
    "# recombine=True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6514fd3-550b-402d-8fde-3d23a4ac1319",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #OLD USING NC\n",
    "# if recombine==True:\n",
    "#     #combine all job output arrays \n",
    "#     import numpy as np\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     import xarray as xr\n",
    "#     import os; import time\n",
    "#     dir='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "    \n",
    "#     num_jobs=60#350 #***\n",
    "#     for job_id in range(1, num_jobs+1):\n",
    "#         if np.mod(job_id,50)==0: print(f'job_id = {job_id}')\n",
    "#         # Open the dataset and append it to the list\n",
    "#         if job_id == 1: \n",
    "#             in_file=dir+f'Project_Algorithms/Tracking_Algorithms/trackout/parcel_tracking_{res}_{t_res}_{Np_str}_{job_id}.h5'\n",
    "            \n",
    "#             out_arr = xr.open_dataset(in_file)['out_arr']\n",
    "#             save_arr = xr.open_dataset(in_file)['save_arr']\n",
    "#         elif job_id >= 2: \n",
    "#             in_file = dir+f'Project_Algorithms/Tracking_Algorithms/trackout/parcel_tracking_{res}_{t_res}_{Np_str}_{job_id}.h5'\n",
    "            \n",
    "#             out2 = xr.open_dataset(in_file)['out_arr']\n",
    "#             save2 = xr.open_dataset(in_file)['save_arr']\n",
    "#             out_arr=np.concatenate((out_arr, out2), axis=0)\n",
    "#             save_arr=np.concatenate((save_arr, save2), axis=0)\n",
    "#     ds=xr.Dataset({\n",
    "#         'out_arr': (['rows', 'columns'], out_arr.astype(float)),\n",
    "#         'save_arr': (['rows', 'columns'], save_arr.astype(float)),\n",
    "#     })\n",
    "#     out_file=dir+f'Project_Algorithms/Tracking_Algorithms/parcel_tracking_{res}_{t_res}_{Np_str}.h5'\n",
    "#     ds.to_netcdf(out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0c83bc-0063-4eae-80af-317f43baf3a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#combine all job output arrays \n",
    "if recombine == True:\n",
    "    num_jobs = 350 #60\n",
    "    var_names = ['out_arr', 'save_arr', 'save2_arr']\n",
    "    recombined_arrays = {}  # Store final arrays here\n",
    "\n",
    "    def MakeUnique(arr):\n",
    "        return np.unique(arr, axis=0)\n",
    "    def get_total_count(var_name):\n",
    "        return sum(\n",
    "            h5py.File(dir + f'Project_Algorithms/Tracking_Algorithms/trackout/parcel_tracking_{res}_{t_res}_{Np_str}_{job_id}.h5', 'r')[var_name].shape[0]\n",
    "            for job_id in range(1, num_jobs + 1)\n",
    "        )\n",
    "\n",
    "    def load_file(job_id, var_name):\n",
    "        in_file = dir + f'Project_Algorithms/Tracking_Algorithms/trackout/parcel_tracking_{res}_{t_res}_{Np_str}_{job_id}.h5'\n",
    "        with h5py.File(in_file, 'r') as hf:\n",
    "            arr = hf[var_name][:]\n",
    "        return arr\n",
    "\n",
    "    # Preallocate arrays\n",
    "    for var_name in var_names:\n",
    "        total_count = get_total_count(var_name)\n",
    "        recombined_arrays[var_name] = np.zeros((total_count, 3), dtype=np.int32)\n",
    "\n",
    "    # Fill arrays\n",
    "    for var_name in var_names:\n",
    "        print(f\"Combining data for {var_name}\")\n",
    "        left_ind = 0\n",
    "        for job_id in range(1, num_jobs + 1):\n",
    "            if job_id % 50 == 0:\n",
    "                print(f\"{var_name}: processing job {job_id}\")\n",
    "            arr = load_file(job_id, var_name)\n",
    "            n_rows = arr.shape[0]; right_ind = left_ind + n_rows\n",
    "            recombined_arrays[var_name][left_ind:right_ind, :] = arr\n",
    "            left_ind = right_ind\n",
    "\n",
    "    #Make Unique\n",
    "    for var_name in var_names:\n",
    "        recombined_arrays[var_name]=MakeUnique(recombined_arrays[var_name])\n",
    "        \n",
    "    # Write to file\n",
    "    out_file = dir + f'Project_Algorithms/Tracking_Algorithms/parcel_tracking_{res}_{t_res}_{Np_str}.h5'\n",
    "    with h5py.File(out_file, 'w') as hf:\n",
    "        for var_name in var_names:\n",
    "            \n",
    "            hf.create_dataset(var_name, data=recombined_arrays[var_name], compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1b74d1-ba85-4b12-9e14-3100ba52fe1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ea522e-734b-407b-9cfa-081d43743c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #LOADING BACK IN\n",
    "# def load_file():\n",
    "#     in_file=dir+f'Project_Algorithms/Tracksing_Algorithms/parcel_tracking_{res}_{t_res}_{Np_str}.h5'\n",
    "#     with h5py.File(in_file, 'r') as hf:\n",
    "#         out_arr=hf['out_arr'][:]\n",
    "#         save_arr=hf['save_arr'][:]\n",
    "#         save2_arr=hf['save2_arr'][:]\n",
    "#     return out_arr,save_arr,save2_arr\n",
    "# [out_arr,save_arr,save2_arr]=load_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87562d0-b7f7-430d-845e-1239e42ad061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TESTING\n",
    "# out_file = dir + f'Project_Algorithms/Tracking_Algorithms/parcel_tracking_{res}_{t_res}_{Np_str}.h5'\n",
    "# with h5py.File(out_file, 'r') as hf:\n",
    "#     test=hf['out_arr'][0:232]\n",
    "# np.all(test==out_arr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
