{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e661fd2-d578-41ef-b048-a4b882a9ba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in Packages and Data\n",
    "\n",
    "#Importing Packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import matplotlib.gridspec as gridspec\n",
    "import xarray as xr\n",
    "import os; import time\n",
    "import pickle\n",
    "import h5py\n",
    "###############################################################\n",
    "def coefs(coefficients,degree):\n",
    "    coef=coefficients\n",
    "    coefs=\"\"\n",
    "    for n in range(degree, -1, -1):\n",
    "        string=f\"({coefficients[len(coef)-(n+1)]:.1e})\"\n",
    "        coefs+=string + f\"x^{n}\"\n",
    "        if n != 0:\n",
    "            coefs+=\" + \"\n",
    "    return coefs\n",
    "###############################################################\n",
    "start_time = time.time();\n",
    "\n",
    "#Importing Model Data\n",
    "check=False\n",
    "dir='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "job_array=False;index_adjust=0\n",
    "ocean_fraction=2/8\n",
    "\n",
    "# dx = 1 km; Np = 1M; Nt = 5 min\n",
    "data1=xr.open_dataset(dir+'../cm1r20.3/run/cm1out_1km_5min.nc', decode_timedelta=True) #***\n",
    "parcel1=xr.open_dataset(dir+'../cm1r20.3/run/cm1out_pdata_1km_5min_1e6.nc', decode_timedelta=True) #***\n",
    "res='1km';t_res='5min'\n",
    "Np_str='1e6'\n",
    "\n",
    "# # dx = 1km; Np = 50M\n",
    "# #Importing Model Data\n",
    "# dir2='/home/air673/koa_scratch/'\n",
    "# data1=xr.open_dataset(dir2+'cm1out_1km_1min.nc', decode_timedelta=True) #***\n",
    "# parcel1=xr.open_dataset(dir2+'cm1out_pdata_1km_1min_50M.nc', decode_timedelta=True) #***\n",
    "# res='1km'; t_res='1min'; Np_str='50e6'\n",
    "\n",
    "# # dx = 1km; Np = 50M; Nz = 95\n",
    "# #Importing Model Data\n",
    "# dir2='/home/air673/koa_scratch/'\n",
    "# data1=xr.open_dataset(dir2+'cm1out_1km_1min_95nz.nc', decode_timedelta=True) #***\n",
    "# parcel1=xr.open_dataset(dir2+'cm1out_pdata_1km_1min_95nz.nc', decode_timedelta=True) #***\n",
    "# res='1km'; t_res='1min_95nz'; Np_str='50e6'\n",
    "\n",
    "# # dx = 250m; Np = 50M\n",
    "# #Importing Model Data\n",
    "# dir2='/home/air673/koa_scratch/'\n",
    "# data1=xr.open_dataset(dir2+'cm1out_250m_1min_50M.nc', decode_timedelta=True) #***\n",
    "# parcel1=xr.open_dataset(dir2+'cm1out_pdata_250m_1min_50M.nc', decode_timedelta=True) #***\n",
    "# res='250m'; t_res='1min'; Np_str='50e6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d759de-8dc1-428e-8d11-01d4da2e92a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "#MODEL AND ALGORITHM NUMERICAL PARAMETERS\n",
    "times=data1['time'].values/(1e9 * 60); times=times.astype(float);\n",
    "minutes=1/times[1] #1 / minutes per timestep = timesteps per minute\n",
    "kms=np.argmax(data1['xh'].values-data1['xh'][0].values >= 1) #finds how many x grids is 1 km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f759a2a1-cd92-4b97-953a-443424b74f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JOB ARRAY SETUP\n",
    "def StartJobArray(num_jobs):\n",
    "    total_elements=len(data1['time']) #total num of variables\n",
    "    \n",
    "    if num_jobs >= total_elements:\n",
    "        raise ValueError(\"Number of jobs cannot be greater than or equal to total elements.\")\n",
    "    \n",
    "    job_range = total_elements // num_jobs  # Base size for each chunk\n",
    "    remaining = total_elements % num_jobs   # Number of chunks with 1 extra \n",
    "    \n",
    "    # Function to compute the start and end for each job_id\n",
    "    def get_job_range(job_id):\n",
    "        job_id-=1\n",
    "        # Add one extra element to the first 'remaining' chunks\n",
    "        start_job = job_id * job_range + min(job_id, remaining)\n",
    "        end_job = start_job + job_range + (1 if job_id < remaining else 0)\n",
    "    \n",
    "        if job_id == num_jobs - 1: \n",
    "            end_job = total_elements #- 1\n",
    "        return start_job, end_job\n",
    "    # def job_testing():\n",
    "    #     #TESTING\n",
    "    #     start=[];end=[]\n",
    "    #     for job_id in range(1,num_jobs+1):\n",
    "    #         start_job, end_job = get_job_range(job_id)\n",
    "    #         print(start_job,end_job)\n",
    "    #         start.append(start_job)\n",
    "    #         end.append(end_job)\n",
    "    #     print(np.all(start!=end))\n",
    "    #     print(len(np.unique(start))==len(start))\n",
    "    #     print(len(np.unique(end))==len(end))\n",
    "    # job_testing()\n",
    "    \n",
    "    job_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0)) #this is the current SBATCH job id\n",
    "    if job_id==0: job_id=1\n",
    "    start_job, end_job = get_job_range(job_id)\n",
    "    index_adjust=start_job\n",
    "    # print(f'start_job = {start_job}, end_job = {end_job}')\n",
    "    return start_job,end_job,index_adjust,job_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61501f68-925f-4a6e-b305-ae30a441e367",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################################################\n",
    "#GENERAL FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee609b3-9a9a-473b-9335-818b3f14f225",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Data Functions\n",
    "def get_2dtime_data(data,varname,tlev,zlev):\n",
    "    cloud_var=data[varname].isel(time=tlev,zh=zlev).values\n",
    "    return cloud_var\n",
    "def get_3dtime_data(data,varname,tlev):\n",
    "    cloud_var=data[varname].isel(time=tlev).values\n",
    "    return cloud_var\n",
    "\n",
    "def get_conv1(t,z):\n",
    "    import h5py\n",
    "    # print('calculating convergence and taking mean')\n",
    "    if res=='1km':\n",
    "        dir2='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "    elif res=='250m':\n",
    "        dir2='/home/air673/koa_scratch/'\n",
    "    file_path = dir2 + 'Variable_Calculation/OUTPUT/' + 'Convergence' + f'_{res}_{t_res}.h5'\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        Conv = f['conv'][t+index_adjust,z] #*#*#* For JobArray\n",
    "    return Conv\n",
    "\n",
    "def get_conv2(data,t):\n",
    "    Nz=len(data['zh'])\n",
    "    import h5py\n",
    "    # print('calculating convergence and taking mean')\n",
    "    if res=='1km':\n",
    "        dir2='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "    elif res=='250m':\n",
    "        dir2='/home/air673/koa_scratch/'\n",
    "    file_path = dir2 + 'Variable_Calculation/OUTPUT/' + 'Convergence' + f'_{res}_{t_res}.h5'\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        Conv = f['conv'][t+index_adjust,0:Nz] #*#*#* For JobArray\n",
    "    return Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8487c5fc-e03c-4a03-af50-fd1c8c42b609",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################################################\n",
    "#ALGORITHM FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289c262e-108b-442b-bb57-cfee87490dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for taking x and y derivatives (Gradient)\n",
    "def cd2d(f,dx,dy): #size not compatible, cant calculate adjacent gradient\n",
    "    ddx = (\n",
    "            f[:,:, 1:  ]\n",
    "            -\n",
    "            f[:,:, 0:-1]\n",
    "        ) / (\n",
    "        2 * dx\n",
    "    )\n",
    "    \n",
    "    ddy = (\n",
    "        f[:,1:, :]\n",
    "        -\n",
    "        f[:,0:-1, :]\n",
    "    ) / (\n",
    "        2 * dy\n",
    "    )\n",
    "    \n",
    "    return ddx, ddy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7276b41b-5527-4d71-9772-42669b80c85d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def find_SBZ_xmaxs():\n",
    "\n",
    "    \n",
    "#     # Define the vertical level you are interested in\n",
    "#     if res=='1km':\n",
    "#         zlev = 4 #534m\n",
    "#     elif res=='250m':\n",
    "#         zlev= 10 #525m\n",
    "    \n",
    "#     # Initialize a list to store the xmaxs for each time step\n",
    "#     xmaxs_list = []\n",
    "\n",
    "#     # Loop over each time step (axis=0 corresponds to time)\n",
    "#     for t in range(len(data1['time'])):  # conv_dataset.shape[0] is the time dimension size\n",
    "#         if t % 60 ==0: print(f\"current time {t}\")\n",
    "#         # Read the relevant slice for this time step and vertical level\n",
    "#         Conv_t_zlev = get_conv1(t, zlev)  # Shape should be (y_size, x_size)\n",
    "        \n",
    "#         # Calculate the mean across the y-axis\n",
    "#         Conv_ymean = np.mean(Conv_t_zlev, axis=0)  # Mean across the y-axis\n",
    "        \n",
    "#         # Find the index of the maximum value along the x-axis\n",
    "#         xmax = np.argmax(Conv_ymean)\n",
    "        \n",
    "#         # Append the result for this time step\n",
    "#         xmaxs_list.append(xmax)\n",
    "    \n",
    "#     # Convert the list of xmaxs to a numpy array (optional)\n",
    "#     xmaxs = np.array(xmaxs_list)\n",
    "\n",
    "#     return xmaxs #returns SBZ x location for each timestep\n",
    "# SBZ_MAXS=find_SBZ_xmaxs()\n",
    "# print('done')\n",
    "# print(SBZ_MAXS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0709efcf-c03e-4a84-bcf0-fd12825aed85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds all local maximums (from Calculus) along each y level for a specific z level (~0.28km in this case)\n",
    "def find_local_maxes(conv_z,t,yind,conv_thresh,ONLY_SBZ):\n",
    "    xf=data['xf'].values\n",
    "    dx=np.round(data['xf'][1]-data['xf'][0],2).item() #grid resolution (in km) (can also be set to meters, since this function only finds the inflection points)\n",
    "\n",
    "    #indexes convergence in y\n",
    "    yconv=conv_z[yind,:]\n",
    "    \n",
    "    #takes dconv/dx\n",
    "    f=yconv\n",
    "    ddx = (\n",
    "            f[1:  ]\n",
    "            -\n",
    "            f[0:-1]\n",
    "        ) / (\n",
    "        2 * dx\n",
    "    )\n",
    "\n",
    "    ########################################################\n",
    "    #RUNNING\n",
    "    ########################################################\n",
    "    \n",
    "    #finds local max where dconv/dx sign changes\n",
    "    signs = np.sign(ddx)\n",
    "    signs_diff=np.diff(signs)\n",
    "    local_maxes=np.where((signs_diff != 0) & (signs_diff < 0))[0]+1 #make sure +1 is here (it corrects the location of the derivative)\n",
    "    local_maxes=local_maxes[np.where(yconv[local_maxes]>conv_thresh)] #check if convergence is greater than convergence threshold (1s-1)\n",
    "    local_maxes=local_maxes[(local_maxes>50*kms)&(local_maxes<len(xf)-50*kms)] #removes maxes that are with 50 km of y boundary\n",
    "    # local_maxes=local_maxes[local_maxes>int(len(xf)*ocean_fraction)] #restricts to right land side\n",
    "    # if ONLY_SBZ==True:\n",
    "    #     local_maxes=local_maxes[(local_maxes>=SBZ_MAXS[t-index_adjust]-10*kms)&(local_maxes<=SBZ_MAXS[t]+10*kms)] #removes maxes that are with 50 km of y boundary\n",
    "\n",
    "    # ################################################################################\n",
    "    # #second round maxes (not 100% necessary, only if missing many convergence maximums that are visually there)\n",
    "    # yconv2=yconv.copy()\n",
    "    # yconv2[local_maxes]=0\n",
    "    # #takes dconv/dx\n",
    "    # f=yconv2\n",
    "    # ddx = (\n",
    "    #         f[1:  ]\n",
    "    #         -\n",
    "    #         f[0:-1]\n",
    "    #     ) / (\n",
    "    #     2 * dx\n",
    "    # )\n",
    "    # signs = np.sign(ddx)\n",
    "    # signs_diff=np.diff(signs)\n",
    "    # local_maxes2=np.where((signs_diff != 0) & (signs_diff < 0))[0]+1 #make sure +1 is here\n",
    "    # local_maxes2=local_maxes2[np.where(yconv2[local_maxes2]>conv_thresh)] #remove local maxes less than zero\n",
    "    # local_maxes2=local_maxes2[(local_maxes2>50*kms)&(local_maxes2<len(xf)-50*kms)] #removes maxes that are with 50 km of y boundary\n",
    "    # local_maxes2=local_maxes2[local_maxes2>int(len(xf)/2)] #restricts to right land side\n",
    "    # local_maxes=np.concatenate((local_maxes,local_maxes2))\n",
    "    # ################################################################################\n",
    "    return ddx,local_maxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a21482-5da9-43d8-83eb-5b0c0684d35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################################################\n",
    "#Calculation Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241f9b09-ed1e-4cc5-9e42-a227eaeccf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find_Local_Maxes Function\n",
    "#(1) At a single time and z level, runs through each y-level\n",
    "#(2) At each y-level, takes the x-derivative\n",
    "#(3) Take sign(x_derivative)\n",
    "#(4) Take diff(x_derivative)\n",
    "#(5) Max is located one index to the right of where derivative changes from positive to negative or diff is +1\n",
    "#[(6) Optional: the algorithm can run a second time over the leftover maxes after removing previous maxes from temporary variable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe7f568-51fa-4f85-8463-ad445e0deaa9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#SBZ Convergence Line Search Algorithm (levels are seperate) (python version 3.10.9) (All Max Algorithm)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr; import time as time\n",
    "\n",
    "def layermax(data,t,ONLY_SBZ): #finds max convergence along y for multiple z location (5 is good)\n",
    "    Nz=len(data['zh'])\n",
    "    \n",
    "    #making data to fill\n",
    "    maxconv_x=np.full((Nz,len(data['yh']),len(data['xh'])), -1, dtype=np.int16)\n",
    "    #RUNNING AGAIN FOR ALL LEVELS\n",
    "    for zlev in range(0,Nz):\n",
    "        #Taking Convergence of current timesftep\n",
    "        conv=get_conv2(data,t)\n",
    "        conv_z=conv[zlev,:,:] #current z level for convergence\n",
    "\n",
    "        for yind in range(0,len(data['yh'])): #plot maximums for each row\n",
    "            #setting convergence threshold\n",
    "            if res=='1km':\n",
    "                conv_thresh=1.0/1000\n",
    "            elif res==\"250m\":\n",
    "                conv_thresh=1.5/1000\n",
    "\n",
    "            #finds all local maxes\n",
    "            [ddx,local_maxes]=find_local_maxes(conv_z,t,yind,conv_thresh,ONLY_SBZ) #convergence threshold (in 1/s)\n",
    "            \n",
    "            #storing data\n",
    "            maxconv_x[zlev,yind,local_maxes] = local_maxes\n",
    "    return maxconv_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb86d1e-d09e-4a03-9596-3563a446d223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetOutputName(job_id):\n",
    "    base_folder = '/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/Project_Algorithms/Tracking_Algorithms/'\n",
    "    if job_array:\n",
    "        base_folder += 'CL_Tracking_Out/'\n",
    "    \n",
    "    subfolder = 'ALL_CLS/' if not ONLY_SBZ else 'ONLY_SBZS/'\n",
    "    \n",
    "    out_folder = base_folder + subfolder\n",
    "\n",
    "    fname = f'whereCL_{res}_{t_res}_'\n",
    "    fname += 'ONLY_SBZS' if ONLY_SBZ else 'ALL_CLS'\n",
    "    \n",
    "    fname += f'_{job_id}'\n",
    "    fname += '.h5'\n",
    "\n",
    "    full_path = out_folder + fname\n",
    "    return full_path\n",
    "\n",
    "\n",
    "def initiate_array(VarNames, data, job_id):\n",
    "    t_size = len(data['time'])\n",
    "    z_size = len(data['zh'])\n",
    "    y_size = len(data['yh'])\n",
    "    x_size = len(data['xh'])\n",
    "\n",
    "    out_file = GetOutputName(job_id)\n",
    "\n",
    "    with h5py.File(out_file, 'a') as f:\n",
    "        for var_name in VarNames:\n",
    "            if var_name not in f:\n",
    "                f.create_dataset(\n",
    "                    var_name,\n",
    "                    shape=(t_size, z_size, y_size, x_size),\n",
    "                    maxshape=(None, z_size, y_size, x_size),\n",
    "                    dtype='float64',\n",
    "                    chunks=(1, z_size, y_size, x_size)\n",
    "                )\n",
    "    return out_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffd3426-8d1b-4947-81c2-cf8dc6bc45e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUNNING ALGORITHM\n",
    "ONLY_SBZ=False\n",
    "# ONLY_SBZ=True\n",
    "def SubsetZ(data):\n",
    "    # Find the last index where zh <= 0.775 and add 1 to get count\n",
    "    num_zlevs = np.where(data['zh'].data <= 0.775)[0][-1] + 1  \n",
    "    # Select vertical levels from 0 up to num_zlevs (not including num_zlevs)\n",
    "    out = data.isel(zh=slice(0, num_zlevs))\n",
    "    return out\n",
    "def RunAlgorithm(data, job_id):\n",
    "    out_file = initiate_array(['maxconv_x'], data, job_id)\n",
    "\n",
    "    with h5py.File(out_file, 'a') as f:\n",
    "        for t in range(len(data['time'])):\n",
    "            if t % 1 == 0: print(f'Processing timestep {t}/{len(data[\"time\"])}')\n",
    "\n",
    "            # Compute maxconv_x for this timestep (z,y,x)\n",
    "            maxconv_x = layermax(data, t, ONLY_SBZ)\n",
    "\n",
    "            # Directly write into HDF5 dataset at index t\n",
    "            SaveData(f, maxconv_x, 'maxconv_x', t)\n",
    "\n",
    "    print(f'Data saved to {out_file}')\n",
    "    return out_file\n",
    "\n",
    "\n",
    "#SAVING DATA\n",
    "def SaveData(h5file, data_array, var_name, t):\n",
    "    h5file[var_name][t, :, :, :] = data_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2b08e1-3501-46ac-a798-82d31763327f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JOB_ARRAY SETUP\n",
    "########################################\n",
    "# job_array=False\n",
    "job_array=True\n",
    "if res=='1km':\n",
    "    num_jobs=60\n",
    "elif res=='250m':\n",
    "    num_jobs=150\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c63948-f8dc-4dc5-a49e-ef770787661f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#RUNNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f5ea5e-6297-41cf-9685-fb486f31f0af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time=time.time()\n",
    "\n",
    "if job_array==False:\n",
    "    start_job=0;end_job=len(data1['time']);index_adjust=0;job_id=0\n",
    "    data=data1.copy()\n",
    "if job_array==True:\n",
    "    [start_job,end_job,index_adjust,job_id]=StartJobArray(num_jobs=num_jobs)\n",
    "    print(f\"job_id = {job_id} ==> Running for t = [{start_job},{end_job}]\")\n",
    "    data=data1.isel(time=slice(start_job,end_job))\n",
    "\n",
    "data=SubsetZ(data)\n",
    "output=RunAlgorithm(data, job_id)\n",
    "end_time = time.time(); elapsed_time = end_time - start_time; print(f\"Total Elapsed Time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0d02ec-ee91-44bd-b3fc-29ef5af5b123",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RECOMBINING\n",
    "recombine=False\n",
    "recombine=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b39958-2f96-46fd-8261-11bacbac4d28",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def RecombineDask(ONLY_SBZ, num_jobs):\n",
    "    import xarray as xr\n",
    "    from dask.diagnostics import ProgressBar\n",
    "\n",
    "    base_path = '/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/Project_Algorithms/Tracking_Algorithms/CL_Tracking_Out/'\n",
    "    base_path += 'ALL_CLS/' if not ONLY_SBZ else 'ONLY_SBZS/'\n",
    "    varname = 'ALL_CLS' if not ONLY_SBZ else 'ONLY_SBZS'\n",
    "\n",
    "    filepaths = [f'{base_path}whereCL_{res}_{t_res}_{varname}_{job_id}.h5'\n",
    "                 for job_id in range(1, num_jobs + 1)]\n",
    "    \n",
    "    # print(f'Combining files:\\n' + \"\\n\".join(filepaths))\n",
    "\n",
    "    ds = xr.open_mfdataset(\n",
    "        filepaths,\n",
    "        concat_dim='phony_dim_0',  # phony_dim_0 is first dimension = time\n",
    "        combine='nested',\n",
    "        parallel=True,\n",
    "        engine='h5netcdf',\n",
    "        phony_dims='access'\n",
    "    )\n",
    "\n",
    "    # Optional: rename phony dims to meaningful ones\n",
    "    ds = ds.rename({'phony_dim_0': 'time', 'phony_dim_1': 'z', 'phony_dim_2': 'y', 'phony_dim_3': 'x'})\n",
    "\n",
    "    out_path = f'{base_path}../whereCL_{res}_{t_res}_{varname}.h5'\n",
    "    print(f\"Saving combined dataset to: {out_path}\")\n",
    "\n",
    "    with ProgressBar():\n",
    "        ds.to_netcdf(out_path, engine='netcdf4', compute=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84799891-c9ab-44c4-9fd7-d67565c544ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if recombine == True:\n",
    "    RecombineDask(ONLY_SBZ, num_jobs=num_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75cce72-a316-4012-8da2-dba19b049926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d221ac9f-bfb2-47b4-acc1-aa6196b7bd88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de768e0-a680-4fce-aa4c-83354e9b2c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #################################\n",
    "# # READING BACK IN\n",
    "\n",
    "# import xarray as xr\n",
    "\n",
    "# # Define the path to your output file\n",
    "# ONLY_SBZ = False  # or True\n",
    "# varname = 'ONLY_SBZS' if ONLY_SBZ else 'ALL_CLS'\n",
    "\n",
    "# # File path\n",
    "# file_path = f'/mnt/lustre/koa/koastore/torri_group/air_directory/' \\\n",
    "#             f'DCI-Project/Project_Algorithms/Tracking_Algorithms/' \\\n",
    "#             f'CL_Tracking_Out/' \\\n",
    "#             f'whereCL_{res}_{t_res}_{varname}.h5'\n",
    "\n",
    "# # Open dataset (as it's valid NetCDF)\n",
    "# ds = xr.open_dataset(file_path, engine='netcdf4')['maxconv_x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24251668-c1d0-48c1-8f08-c034643f8a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae33178-a866-48aa-9a91-1a060bf5d7b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f85eec-eb07-4b73-926e-73654eff066c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6098bda2-34d4-4493-b375-1d325b24249f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77ee748-7b30-495d-99e4-0ae611bd0b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TESTING\n",
    "#################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9d6dfb-4f70-48d7-b0d1-fad07acfdc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #SIMPLE PLOT\n",
    "\n",
    "# out=ds.isel(time=100,z=3).data\n",
    "# plt.contourf(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1f1bf2-7e57-4cc9-9513-216a2ee3e338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TESTING COMPARING TO VERSION2\n",
    "# ######\n",
    "# load_dir = '/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/Project_Algorithms/Tracking_Algorithms/CL_Tracking_Out/'\n",
    "# open_name = load_dir+f'whereCL_{res}_{t_res}_ALL_CLS_50.nc'\n",
    "# hey=xr.open_dataset(open_name)['maxconv_x']\n",
    "# print(np.where(hey!=-1))\n",
    "# hey.isel(time=0,z=7).plot()\n",
    "\n",
    "# ######\n",
    "# #TESTING COMPARING TO VERSION2\n",
    "# load_dir = '/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/Project_Algorithms/Tracking_Algorithms/'\n",
    "# open_name = load_dir+f'whereCL_{res}_{t_res}_ALL_CLS.nc'\n",
    "# out=xr.open_dataset(open_name)['maxconv_x']\n",
    "# print(np.all(hey==out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5a71a2-ab9a-496f-9ee7-fc7f381db6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TESTING COMPARING TO VERSION2 (FULL DATA)\n",
    "# ######\n",
    "# #TESTING COMPARING TO VERSION2\n",
    "# load_dir = '/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/Project_Algorithms/Tracking_Algorithms/'\n",
    "# open_name = load_dir+f'whereCL_{res}_{t_res}_ALL_CLS.nc'\n",
    "# out1=xr.open_dataset(open_name)['maxconv_x']\n",
    "# ######\n",
    "# #TESTING COMPARING TO VERSION2\n",
    "# load_dir = '/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/Project_Algorithms/Tracking_Algorithms/CL_Tracking_Out/'\n",
    "# open_name = load_dir+f'whereCL_{res}_{t_res}_ALL_CLS.nc'\n",
    "# out2=xr.open_dataset(open_name)['maxconv_x']\n",
    "\n",
    "# #####\n",
    "# t=10\n",
    "# for t in np.arange(1,661,5):\n",
    "#     print(np.all(out1[t]==out2[t]).data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b262e6bf-25f4-4434-b640-2ae1863618ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "work"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
