{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa3c3ed-dfcc-4845-96c2-f586b0019619",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in Packages and Data\n",
    "\n",
    "#Importing Packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import matplotlib.gridspec as gridspec\n",
    "import xarray as xr\n",
    "import os; import time\n",
    "import pickle\n",
    "import h5py\n",
    "###############################################################\n",
    "def coefs(coefficients,degree):\n",
    "    coef=coefficients\n",
    "    coefs=\"\"\n",
    "    for n in range(degree, -1, -1):\n",
    "        string=f\"({coefficients[len(coef)-(n+1)]:.1e})\"\n",
    "        coefs+=string + f\"x^{n}\"\n",
    "        if n != 0:\n",
    "            coefs+=\" + \"\n",
    "    return coefs\n",
    "###############################################################\n",
    "\n",
    "#Importing Model Data\n",
    "check=False\n",
    "dir='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "\n",
    "# dx = 1 km; Np = 1M; Nt = 5 min\n",
    "data=xr.open_dataset(dir+'../cm1r20.3/run/cm1out_1km_1e6.nc', decode_timedelta=True) #***\n",
    "parcel=xr.open_dataset(dir+'../cm1r20.3/run/cm1out_pdata_1km_1e6.nc', decode_timedelta=True) #***\n",
    "res='1km'\n",
    "Np_str='1e6'\n",
    "\n",
    "# dx = 1km; Np = 50M\n",
    "#Importing Model Data\n",
    "check=False\n",
    "dir2='/home/air673/koa_scratch/'\n",
    "data=xr.open_dataset(dir2+'cm1out_1km_1min.nc', decode_timedelta=True) #***\n",
    "parcel=xr.open_dataset(dir2+'cm1out_pdata_1km_1min_50M.nc', decode_timedelta=True) #***\n",
    "res='1km'; t_res='1min'; Np_str='50e6'\n",
    "\n",
    "# # dx = 1km; Np = 100M\n",
    "# #Importing Model Data\n",
    "# check=False\n",
    "# dir2='/home/air673/koa_scratch/'\n",
    "# data=xr.open_dataset(dir2+'cm1out_1km_1min.nc', decode_timedelta=True) #***\n",
    "# parcel=xr.open_dataset(dir2+'cm1out_pdata_1km_1min_100M.nc', decode_timedelta=True) #***\n",
    "# res='1km'; t_res='1min'; Np_str='100e6'\n",
    "\n",
    "\n",
    "# dx = 250 m\n",
    "# #Importing Model Data\n",
    "# check=False\n",
    "# dir2='/home/air673/koa_scratch/'\n",
    "# data=xr.open_dataset(dir2+'cm1out_250m.nc', decode_timedelta=True) #***\n",
    "# parcel=xr.open_dataset(dir2+'cm1out_pdata_250m.nc', decode_timedelta=True) #***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4561fcd-74cc-4ec3-bf62-c6e82253e43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_memory():\n",
    "    import sys\n",
    "    ipython_vars = [\"In\", \"Out\", \"exit\", \"quit\", \"get_ipython\", \"ipython_vars\"]\n",
    "    print(\"Top 10 objects with highest memory usage\")\n",
    "    # Get a sorted list of the objects and their sizes\n",
    "    mem = {\n",
    "        key: round(value/1e6,2)\n",
    "        for key, value in sorted(\n",
    "            [\n",
    "                (x, sys.getsizeof(globals().get(x)))\n",
    "                for x in globals()\n",
    "                if not x.startswith(\"_\") and x not in sys.modules and x not in ipython_vars\n",
    "            ],\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True)[:10]\n",
    "    }\n",
    "    print({key:f\"{value} MB\" for key,value in mem.items()})\n",
    "    print(f\"\\n{round(sum(mem.values()),2)/1000} GB in use overall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad0ebca-caa2-4635-8ded-476acaa57e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "dir2='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "path=dir2+'../Functions/'\n",
    "sys.path.append(path)\n",
    "\n",
    "import NumericalFunctions\n",
    "from NumericalFunctions import * # import NumericalFunctions \n",
    "import PlottingFunctions\n",
    "from PlottingFunctions import * # import PlottingFunctions\n",
    "\n",
    "\n",
    "# # Get all functions in NumericalFunctions\n",
    "# import inspect\n",
    "# functions = [f[0] for f in inspect.getmembers(NumericalFunctions, inspect.isfunction)]\n",
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017eb2a4-301a-444f-9bb9-efe29eefd550",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JOB ARRAY SETUP\n",
    "job_array=True\n",
    "if job_array==True:\n",
    "\n",
    "    num_jobs=60 #how many total jobs are being run? i.e. array=1-100 ==> num_jobs=100 #***\n",
    "    total_elements=len(parcel['xh']) #total num of variables\n",
    "\n",
    "    if num_jobs >= total_elements:\n",
    "        raise ValueError(\"Number of jobs cannot be greater than or equal to total elements.\")\n",
    "    \n",
    "    job_range = total_elements // num_jobs  # Base size for each chunk\n",
    "    remaining = total_elements % num_jobs   # Number of chunks with 1 extra \n",
    "    \n",
    "    # Function to compute the start and end for each job_id\n",
    "    def get_job_range(job_id, num_jobs):\n",
    "        job_id-=1\n",
    "        # Add one extra element to the first 'remaining' chunks\n",
    "        start_job = job_id * job_range + min(job_id, remaining)\n",
    "        end_job = start_job + job_range + (1 if job_id < remaining else 0)\n",
    "    \n",
    "        if job_id == num_jobs - 1: \n",
    "            end_job = total_elements #- 1\n",
    "        return start_job, end_job\n",
    "    # def job_testing():\n",
    "    #     #TESTING\n",
    "    #     start=[];end=[]\n",
    "    #     for job_id in range(1,num_jobs+1):\n",
    "    #         start_job, end_job = get_job_range(job_id)\n",
    "    #         print(start_job,end_job)\n",
    "    #         start.append(start_job)\n",
    "    #         end.append(end_job)\n",
    "    #     print(np.all(start!=end))\n",
    "    #     print(len(np.unique(start))==len(start))\n",
    "    #     print(len(np.unique(end))==len(end))\n",
    "    # job_testing()\n",
    "    \n",
    "    job_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0)) #this is the current SBATCH job id\n",
    "    if job_id==0: job_id=1\n",
    "    start_job, end_job = get_job_range(job_id, num_jobs)\n",
    "    index_adjust=start_job\n",
    "    print(f'start_job = {start_job}, end_job = {end_job}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da371877-78f2-451a-bfe8-b6a4b8a49975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indexing Array with JobArray\n",
    "parcel=parcel.isel(xh=slice(start_job,end_job))\n",
    "#(for 150_000_000 parcels use 500-1000 jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85508e99-9279-4d27-9c14-475ae4b8cef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RESIDENCE CODE\n",
    "########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe66549-0f72-4097-84f9-5f16242ac25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Back Data Later\n",
    "##############\n",
    "def make_data_dict(in_file,var_names,read_type):\n",
    "    if read_type=='h5py':\n",
    "        with h5py.File(in_file, 'r') as f:\n",
    "            data_dict = {var_name: f[var_name][:,start_job:end_job] for var_name in var_names}\n",
    "            \n",
    "    elif read_type=='xarray':\n",
    "        in_data = xr.open_dataset(\n",
    "            in_file,\n",
    "            engine='h5netcdf',\n",
    "            phony_dims='sort',\n",
    "            chunks={'phony_dim_0': 100, 'phony_dim_1': 1_000_000} \n",
    "        )\n",
    "        data_dict = {k: in_data[k][:,start_job:end_job].compute().data for k in var_names}\n",
    "    return data_dict\n",
    "\n",
    "# read_type='xarray'\n",
    "read_type='h5py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9464854f-490c-4af4-97e6-b04ee161abc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "dir2=dir+'Project_Algorithms/Lagrangian_Binary_Array/'\n",
    "in_file=dir2+f'lagrangian_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "\n",
    "var_names = ['A_g', 'A_c', 'Z', 'Y', 'X']\n",
    "data_dict = make_data_dict(in_file,var_names,read_type)\n",
    "A_g, A_c, Z, Y, X = (data_dict[k] for k in var_names)\n",
    "\n",
    "# #Making Time Matrix\n",
    "# rows, cols = A.shape[0], A.shape[1]\n",
    "# T = np.arange(rows).reshape(-1, 1) * np.ones((1, cols), dtype=int)\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd1931e-ec0d-4bb0-8eee-a8efbcd62c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #READING BACK IN\n",
    "# dir2=dir+'Project_Algorithms/Entrainment/'\n",
    "# in_file=dir2+f'processed_binary_arrays_{res}_{t_res}_{Np_str}.h5'\n",
    "\n",
    "# var_names = ['A_g_Processed', 'A_c_Processed']\n",
    "# data_dict = make_data_dict(in_file,var_names,read_type)\n",
    "# A_g_Processed, A_c_Processed = (data_dict[k] for k in var_names)\n",
    "# check_memory(globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42382a9a-4d80-4bb3-ad75-a6a0ec2462c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CALCULATING RESIDENCE TIMES\n",
    "#############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c8863a-1a6d-40e0-bfbd-688dd9d4c072",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir2=dir+'Project_Algorithms/Lagrangian_Binary_Array/'\n",
    "def residence_times(p,type,updraft_type):\n",
    "\n",
    "    if updraft_type=='general':\n",
    "        A=A_g\n",
    "    elif updraft_type=='cloudy':\n",
    "        A=A_c\n",
    "    \n",
    "    B = A[:,p]\n",
    "\n",
    "    # B=np.array([0,1,0,1,0,0,1,1,0,1,1]) #TESTING\n",
    "    \n",
    "    T=np.arange(len(B))\n",
    "    \n",
    "    if np.any(B)==True:\n",
    "        if type=='e':\n",
    "            C=B.copy()\n",
    "        elif type=='d':\n",
    "            C=1-B\n",
    "    \n",
    "        \n",
    "        # Find the changes in the array\n",
    "        changes = np.diff(np.concatenate(([0], C, [0])))  # Add 0s to detect edges\n",
    "            \n",
    "        start_ind = np.where(changes == 1)[0]  # Start of sequences\n",
    "        end_ind = np.where(changes == -1)[0]  # End of sequences\n",
    "        \n",
    "        # Calculate the lengths of sequences\n",
    "        lengths = end_ind - start_ind\n",
    "\n",
    "        sequences = [(start) for start, end, length in zip(start_ind, end_ind, lengths) if length >= 1] #only records en/detrainment time\n",
    "        # sequences = [(start, *range(start + 1, end+1)) for start, end, length in zip(start_ind, end_ind, lengths) if length >= 1]\n",
    "        lens=[(end-start) for start, end, length in zip(start_ind, end_ind, lengths) if length >= 1] #residence times\n",
    "\n",
    "        #Remove the last one to get rid of entrainments that reach end of simulation\n",
    "        sequences=sequences[:-1];lens=lens[:-1]\n",
    "\n",
    "        #Initial Entrainment/Detrainment Times\n",
    "        ts=np.array(sequences.copy()) #only records en/detrainment time \n",
    "        # ts=np.array(tuple(item for seq in sequences for item in seq))\n",
    "\n",
    "         #Finds Last Time Parcel is in Cloudy Updraft before Initial Entrainment\n",
    "        last=[None]+[np.where(C[:ind + 1] == 1)[0][-2] for ind in ts[1:]]\n",
    "        last_lens=ts[1:]-last[1:]; \n",
    "        last_lens=np.insert(last_lens, 0, -1e5) #if never in cloudy updraft add -1e5 for nan\n",
    "\n",
    "        if np.any(ts):\n",
    "            zs=Z[ts,p]\n",
    "            ys=Y[ts,p]\n",
    "            xs=X[ts,p]\n",
    "            return [np.array(lens),ts,zs,ys,xs,last_lens]\n",
    "        else:\n",
    "            return []\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "p=1234; out=residence_times(p,type='e',updraft_type='cloudy')\n",
    "out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cb3614-0d9d-4714-8a70-ef333da9553b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ENTRAINMENT/DETRAINMENT PROFILES\n",
    "Nx=len(data['xh']);Ny=len(data['yh']);Nz=len(data['zh'])\n",
    "yx_array=np.zeros((Ny,Nx));yx_counter=np.zeros_like(yx_array)\n",
    "zx_array=np.zeros((Nz,Nx));zx_counter=np.zeros_like(zx_array)\n",
    "\n",
    "Np=len(parcel['xh'])-1\n",
    "for p in np.arange(Np): \n",
    "    if np.mod(p,4000)==0: print(p)\n",
    "    out=residence_times(p,type='e',updraft_type='cloudy')\n",
    "    \n",
    "\n",
    "    if np.any(out):\n",
    "        np.add.at(yx_array, (out[3], out[4]), out[0])  \n",
    "        np.add.at(yx_counter, (out[3], out[4]), 1)\n",
    "\n",
    "        np.add.at(zx_array, (out[2], out[4]), out[0])  # Add residence times to (x, z) positions\n",
    "        np.add.at(zx_counter, (out[2], out[4]), 1)\n",
    "\n",
    "\n",
    "# #Divide by Counts (MOVED TO JOB_ARRAY COMPILE STEP)\n",
    "# mask = yx_counter != 0\n",
    "# yx_array[mask]/=yx_counter[mask]\n",
    "# mask = zx_counter != 0\n",
    "# zx_array[mask]/=zx_counter[mask]\n",
    "\n",
    "# #Convert to Minutes\n",
    "# mins=((data['time'][1]-data['time'][0])/1e9/60).item()\n",
    "# yx_array*=mins\n",
    "# zx_array*=mins\n",
    "\n",
    "\n",
    "#SAVING\n",
    "dir2=dir+'Project_Algorithms/Entrainment/'\n",
    "output_file = dir2+f'job_out/e_residence_time_arrays_{res}_{t_res}_{Np_str}_{job_id}.h5' \n",
    "with h5py.File(output_file, 'w') as f:\n",
    "    f.create_dataset('yx_array', data=yx_array, compression=\"gzip\")\n",
    "    f.create_dataset('yx_counter', data=yx_counter, compression=\"gzip\")\n",
    "    f.create_dataset('zx_array', data=zx_array, compression=\"gzip\")\n",
    "    f.create_dataset('zx_counter', data=zx_counter, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50565e6a-86f5-48ff-b7d5-f04d315ed392",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTRAINMENT/DETRAINMENT PROFILES\n",
    "Nx=len(data['xh']);Ny=len(data['yh']);Nz=len(data['zh'])\n",
    "yx_array=np.zeros((Ny,Nx));yx_counter=np.zeros_like(yx_array)\n",
    "zx_array=np.zeros((Nz,Nx));zx_counter=np.zeros_like(zx_array)\n",
    "\n",
    "Np=len(parcel['xh'])-1\n",
    "for p in np.arange(Np): \n",
    "    if np.mod(p,4000)==0: print(p)\n",
    "    out=residence_times(p,type='d',updraft_type='cloudy')\n",
    "    \n",
    "\n",
    "    if np.any(out):\n",
    "        np.add.at(yx_array, (out[3], out[4]), out[0])  \n",
    "        np.add.at(yx_counter, (out[3], out[4]), 1)\n",
    "\n",
    "        np.add.at(zx_array, (out[2], out[4]), out[0])  # Add residence times to (x, z) positions\n",
    "        np.add.at(zx_counter, (out[2], out[4]), 1)\n",
    "\n",
    "\n",
    "# #Divide by Counts (MOVED TO JOB_ARRAY COMPILE STEP)\n",
    "# mask = yx_counter != 0\n",
    "# yx_array[mask]/=yx_counter[mask]\n",
    "# mask = zx_counter != 0\n",
    "# zx_array[mask]/=zx_counter[mask]\n",
    "\n",
    "# #Convert to Minutes\n",
    "# mins=((data['time'][1]-data['time'][0])/1e9/60).item()\n",
    "# yx_array*=mins\n",
    "# zx_array*=mins\n",
    "\n",
    "\n",
    "#SAVING\n",
    "dir2=dir+'Project_Algorithms/Entrainment/'\n",
    "output_file = dir2+f'job_out/d_residence_time_arrays_{res}_{t_res}_{Np_str}_{job_id}.h5' \n",
    "with h5py.File(output_file, 'w') as f:\n",
    "    f.create_dataset('yx_array', data=yx_array, compression=\"gzip\")\n",
    "    f.create_dataset('yx_counter', data=yx_counter, compression=\"gzip\")\n",
    "    f.create_dataset('zx_array', data=zx_array, compression=\"gzip\")\n",
    "    f.create_dataset('zx_counter', data=zx_counter, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702e1982-34f3-4afd-9ab2-2e526acd4ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "#RECOMBINE SEPERATE JOB_ARRAYS AFTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678617c5-7fac-4428-ab0f-69aff3932d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir2=dir+'Project_Algorithms/Entrainment/'\n",
    "output_file = dir2+f'job_out/e_residence_time_arrays_{res}_{t_res}_{Np_str}.h5' \n",
    "\n",
    "Nz=len(data['zh'])\n",
    "Ny=len(data['yh'])\n",
    "Nx=len(data['xh'])\n",
    "yx_array=np.zeros((Ny,Nx))\n",
    "yx_counter=yx_array.copy()                  \n",
    "zx_array=np.zeros((Nz,Nx))\n",
    "zx_counter=zx_array.copy()                  \n",
    "\n",
    "num_jobs=60\n",
    "for job_id in np.arange(1,num_jobs+1):\n",
    "    if np.mod(job_id,20)==0: print(f\"{job_id}/{num_jobs}\")\n",
    "    input_file = dir2+f'job_out/e_residence_time_arrays_{res}_{t_res}_{Np_str}_{job_id}.h5' \n",
    "    with h5py.File(input_file,'r') as f:\n",
    "        yx_array+=f['yx_array']\n",
    "        yx_counter+=f['yx_counter']\n",
    "        zx_array+=f['zx_array']\n",
    "        zx_counter+=f['zx_counter']\n",
    "\n",
    "######################################################\n",
    "#Divide by Counts\n",
    "print('dividing by counts')\n",
    "mask = yx_counter != 0\n",
    "yx_array[mask]/=yx_counter[mask]\n",
    "mask = zx_counter != 0\n",
    "zx_array[mask]/=zx_counter[mask]\n",
    "\n",
    "#Convert to Minutes\n",
    "mins=((data['time'][1]-data['time'][0])/1e9/60).item()\n",
    "yx_array*=mins\n",
    "zx_array*=mins\n",
    "######################################################\n",
    "\n",
    "#SAVING INTO FINAL FORM\n",
    "print('saving')\n",
    "with h5py.File(output_file, 'w') as f:\n",
    "    f.create_dataset('yx_array', data=yx_array, compression=\"gzip\")\n",
    "    f.create_dataset('yx_counter', data=yx_counter, compression=\"gzip\")\n",
    "    f.create_dataset('zx_array', data=zx_array, compression=\"gzip\")\n",
    "    f.create_dataset('zx_counter', data=zx_counter, compression=\"gzip\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ee3401-c0b1-4131-a491-66a346aa2433",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir2=dir+'Project_Algorithms/Entrainment/'\n",
    "output_file = dir2+f'job_out/d_residence_time_arrays_{res}_{t_res}_{Np_str}.h5' \n",
    "\n",
    "Nz=len(data['zh'])\n",
    "Ny=len(data['yh'])\n",
    "Nx=len(data['xh'])\n",
    "yx_array=np.zeros((Ny,Nx))\n",
    "yx_counter=yx_array.copy()                  \n",
    "zx_array=np.zeros((Nz,Nx))\n",
    "zx_counter=zx_array.copy()                  \n",
    "\n",
    "num_jobs=60\n",
    "for job_id in np.arange(1,num_jobs+1):\n",
    "    if np.mod(job_id,20)==0: print(f\"{job_id}/{num_jobs}\")\n",
    "    input_file = dir2+f'job_out/d_residence_time_arrays_{res}_{t_res}_{Np_str}_{job_id}.h5' \n",
    "    with h5py.File(input_file,'r') as f:\n",
    "        yx_array+=f['yx_array']\n",
    "        yx_counter+=f['yx_counter']\n",
    "        zx_array+=f['zx_array']\n",
    "        zx_counter+=f['zx_counter']\n",
    "\n",
    "######################################################\n",
    "#Divide by Counts\n",
    "print('dividing by counts')\n",
    "mask = yx_counter != 0\n",
    "yx_array[mask]/=yx_counter[mask]\n",
    "mask = zx_counter != 0\n",
    "zx_array[mask]/=zx_counter[mask]\n",
    "\n",
    "#Convert to Minutes\n",
    "mins=((data['time'][1]-data['time'][0])/1e9/60).item()\n",
    "yx_array*=mins\n",
    "zx_array*=mins\n",
    "######################################################\n",
    "\n",
    "#SAVING INTO FINAL FORM\n",
    "print('saving')\n",
    "with h5py.File(output_file, 'w') as f:\n",
    "    f.create_dataset('yx_array', data=yx_array, compression=\"gzip\")\n",
    "    f.create_dataset('yx_counter', data=yx_counter, compression=\"gzip\")\n",
    "    f.create_dataset('zx_array', data=zx_array, compression=\"gzip\")\n",
    "    f.create_dataset('zx_counter', data=zx_counter, compression=\"gzip\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104c9629-7e2d-4a67-ab85-64255feffa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "#PLOTTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1de1e2-1ba9-4232-8a7b-5c15c3352254",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = dir2+f'job_out/e_residence_time_arrays_{res}_{t_res}_{Np_str}.h5' \n",
    "with h5py.File(output_file, 'r') as f:\n",
    "    yx_array = f['yx_array'][:]\n",
    "    yx_counter = f['yx_counter'][:]\n",
    "    zx_array = f['zx_array'][:]\n",
    "    zx_counter = f['zx_counter'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a216879-504b-40d1-bf9a-ac2e33f9f263",
   "metadata": {},
   "outputs": [],
   "source": [
    "yx_array[yx_array==0]=np.nan\n",
    "zx_array[zx_array==0]=np.nan\n",
    "\n",
    "fig = plt.figure(figsize=(8*(512/34)/5, 8))\n",
    "gs = gridspec.GridSpec(2, 1)  # 1 row, 2 columns\n",
    "cmap='seismic'\n",
    "levels1=15;levels2=15\n",
    "\n",
    "# First subplot (yx_array contour)\n",
    "ax1 = fig.add_subplot(gs[0, 0])  # First column\n",
    "contour1 = ax1.contourf(yx_array,levels=levels1,cmap=cmap)\n",
    "cbar1 = plt.colorbar(contour1, ax=ax1);cbar1.set_label(\"mins / parcel\")\n",
    "ax1.set_title(\"XY Entrainment Residence Time (Plotted by Entrainment Time)\")\n",
    "\n",
    "# Second subplot (zx_array contour)\n",
    "ax2 = fig.add_subplot(gs[1, 0])  # Second column\n",
    "contour2 = ax2.contourf(zx_array, levels=levels2, cmap=cmap)\n",
    "cbar2 = plt.colorbar(contour2, ax=ax2);cbar2.set_label(\"mins / parcel\")\n",
    "ax2.set_title(\"XZ Entrainment Residence Time (Plotted by Entrainment Time)\")\n",
    "\n",
    "\n",
    "#COASTLINE\n",
    "ocean_fraction=2/8\n",
    "ax1.axvline(yx_array.shape[1]*ocean_fraction,color='green',linewidth=3)\n",
    "ax2.axvline(yx_array.shape[1]*ocean_fraction,color='green',linewidth=3)\n",
    "\n",
    "#THICKEN COLOR LINES\n",
    "\n",
    "for edge in cbar1.ax.collections:  # Loop over individual elements in each list\n",
    "    edge.set_linewidth(10)\n",
    "for edge in cbar2.ax.collections:  # Loop over individual elements in each list\n",
    "    edge.set_linewidth(8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1f333e-a971-46c7-ad29-7824b68cb748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# currently i look for runs of being in cloudy updraft that are at least 5 minutes (1 timestep). i add the total length of that run at the location of initial entrainment/detrainment. then i average to get vertical profile. \n",
    "\n",
    "# For entrainment, particles should stay in a cloud for 30/60 minutes\n",
    "# I’m very surprised by a) the extremely low values for entrainment time and b) the extremely high values for detainment times\n",
    "# another thing: this is only cloudy updrafts, not clouds as well. is that 30/60 minutes true for the “cloud updrafts”?\n",
    "# for detrainment then, there are some parcels that don’t interact with any “cloudy updrafts” for a very long time.\n",
    "# also, if you look at the contour plot we do have higher values than the vertical profile \n",
    "\n",
    "plt.plot(np.nanmean(zx_array[:,:],axis=(1)),data['zh'],label='everywhere')\n",
    "plt.ylabel('z (km)');plt.xlabel('residence time (mins)')\n",
    "\n",
    "\n",
    "# plt.plot(np.nanmean(zx_array[:,int(512/2):512],axis=(1)),data['zh'],label='over land')\n",
    "# plt.ylabel('z (km)');plt.xlabel('preconditioning time (mins)')\n",
    "\n",
    "# plt.plot(np.nanmean(zx_array[:,0:int(512/2)],axis=(1)),data['zh'],label='over ocean')\n",
    "# plt.ylabel('z (km)');plt.xlabel('preconditioning time (mins)')\n",
    "\n",
    "plt.ylim(top=20)\n",
    "plt.title('preconditioning time')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff2ba32-1fea-481a-afa8-eefd6ede9bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = dir2+f'job_out/d_residence_time_arrays_{res}_{t_res}_{Np_str}.h5' \n",
    "with h5py.File(output_file, 'r') as f:\n",
    "    yx_array = f['yx_array'][:]\n",
    "    yx_counter = f['yx_counter'][:]\n",
    "    zx_array = f['zx_array'][:]\n",
    "    zx_counter = f['zx_counter'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a649fb31-0fb5-456f-853b-ebc90b0d7e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "yx_array[yx_array==0]=np.nan\n",
    "zx_array[zx_array==0]=np.nan\n",
    "\n",
    "fig = plt.figure(figsize=(8*(512/34)/5, 8))\n",
    "gs = gridspec.GridSpec(2, 1)  # 1 row, 2 columns\n",
    "cmap='seismic'\n",
    "levels1=15;levels2=15\n",
    "\n",
    "# First subplot (yx_array contour)\n",
    "ax1 = fig.add_subplot(gs[0, 0])  # First column\n",
    "contour1 = ax1.contourf(yx_array,levels=levels1,cmap=cmap)\n",
    "cbar1 = plt.colorbar(contour1, ax=ax1);cbar1.set_label(\"mins / parcel\")\n",
    "ax1.set_title(\"XY Detrainment Residence Time (Plotted by Detrainment Time)\")\n",
    "\n",
    "# Second subplot (zx_array contour)\n",
    "ax2 = fig.add_subplot(gs[1, 0])  # Second column\n",
    "contour2 = ax2.contourf(zx_array, levels=levels2, cmap=cmap)\n",
    "cbar2 = plt.colorbar(contour2, ax=ax2);cbar2.set_label(\"mins / parcel\")\n",
    "ax2.set_title(\"XZ Detrainment Residence Time (Plotted by Detrainment Time)\")\n",
    "\n",
    "\n",
    "#COASTLINE\n",
    "ocean_fraction=2/8\n",
    "ax1.axvline(yx_array.shape[1]*ocean_fraction,color='green',linewidth=3)\n",
    "ax2.axvline(yx_array.shape[1]*ocean_fraction,color='green',linewidth=3)\n",
    "\n",
    "#THICKEN COLOR LINES\n",
    "\n",
    "for edge in cbar1.ax.collections:  # Loop over individual elements in each list\n",
    "    edge.set_linewidth(10)\n",
    "for edge in cbar2.ax.collections:  # Loop over individual elements in each list\n",
    "    edge.set_linewidth(8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6d0137-f81b-4cce-8eef-8737733ce80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# currently i look for runs of being in cloudy updraft that are at least 5 minutes (1 timestep). i add the total length of that run at the location of initial entrainment/detrainment. then i average to get vertical profile. \n",
    "\n",
    "# For entrainment, particles should stay in a cloud for 30/60 minutes\n",
    "# I’m very surprised by a) the extremely low values for entrainment time and b) the extremely high values for detainment times\n",
    "# another thing: this is only cloudy updrafts, not clouds as well. is that 30/60 minutes true for the “cloud updrafts”?\n",
    "# for detrainment then, there are some parcels that don’t interact with any “cloudy updrafts” for a very long time.\n",
    "# also, if you look at the contour plot we do have higher values than the vertical profile \n",
    "\n",
    "plt.plot(np.nanmean(zx_array[:,:],axis=(1)),data['zh'],label='everywhere')\n",
    "plt.ylabel('z (km)');plt.xlabel('nonresidence time (mins)')\n",
    "\n",
    "\n",
    "# plt.plot(np.nanmean(zx_array[:,int(512/2):512],axis=(1)),data['zh'],label='over land')\n",
    "# plt.ylabel('z (km)');plt.xlabel('preconditioning time (mins)')\n",
    "\n",
    "# plt.plot(np.nanmean(zx_array[:,0:int(512/2)],axis=(1)),data['zh'],label='over ocean')\n",
    "# plt.ylabel('z (km)');plt.xlabel('preconditioning time (mins)')\n",
    "\n",
    "plt.ylim(top=20)\n",
    "plt.title('nonresidence time')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcbb952-503b-4712-a1a8-13764fc2657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTANT FOR PLOTTING\n",
    "\n",
    "# xticks/yticks\n",
    "# xticks = plt.gca().get_xticks()\n",
    "# new_labels = [str(int(tick * 5)) for tick in xticks]\n",
    "# plt.gca().set_xticklabels(new_labels);\n",
    "\n",
    "# cbar-ticks\n",
    "# cbar_ticks = cbar.get_ticks()  # Get the current ticks\n",
    "# new_ticks = [str(int(tick * 5)) for tick in cbar_ticks]  # Modify ticks (multiply by 5 and convert to string)\n",
    "# cbar.set_ticks(cbar_ticks)  # Set the original ticks again to avoid resetting\n",
    "# cbar.set_ticklabels(new_ticks)\n",
    "\n",
    "\n",
    "#imshow\n",
    "# plt.yticks(np.arange(Nz))\n",
    "# new_ytick_labels = np.round(data['zf'].values[:Nz], 2)\n",
    "# plt.gca().set_yticklabels(new_ytick_labels, fontsize=8, rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0facb6e5-9016-4ccb-af5b-1c54db8303c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d46026-6386-4ac8-8c24-e594be9c58e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ab3109-514a-49d5-884f-38723ae7fbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "#OTHER TESTING FOR POSSIBLY ANALYSISES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4664da-80dc-40cd-b1dc-bad97bead02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKING 2D HISTOGRAM OF RESIDENCE TIME VS Z (COLOR: VARIABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7774861-7716-4531-96a6-ce083c2cac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def averaged_profiles(profile): \n",
    "    out_var=profile[ (profile[:, 1] != 0)]; #gets rid of rows that have no data\n",
    "    out_var=np.array([out_var[:, 0] / out_var[:, 1], out_var[:, 2]]).T #divides the data column by the counter column\n",
    "    return out_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb08040-9920-4e93-8d7c-887eb97faf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst=[]\n",
    "for p in np.arange(Np):\n",
    "    out=residence_times(p,type='e',updraft_type='cloudy')\n",
    "    if np.any(out)==True:\n",
    "        lens=out[0]\n",
    "        lst.append(lens)\n",
    "print(f'max entrainment time: {max(arr.max() for arr in lst)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feef2197-982f-4891-b9fd-c776d6036c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nz=len(data['zh'])\n",
    "profile=np.zeros((Nz,18)); #residence time by Z levels\n",
    "counter=np.zeros_like(profile)\n",
    "\n",
    "Np=125000-1\n",
    "for p in np.arange(Np):\n",
    "    out=residence_times(p,type='e',updraft_type='cloudy')\n",
    "    if np.any(out)==True:\n",
    "        lens=out[0]\n",
    "        # print(lens)\n",
    "        \n",
    "        ts=out[1]\n",
    "        zs=out[2]\n",
    "        ys=out[3]\n",
    "        xs=out[4]\n",
    "\n",
    "        for ind,(z,l) in enumerate(zip(zs,lens)):\n",
    "            profile[z,l]+=1\n",
    "\n",
    "# #NORMALIZATION\n",
    "row_averages = np.nansum(profile, axis=1)\n",
    "mask = row_averages!=0\n",
    "profile[mask] /= row_averages[mask,np.newaxis]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ecaac8-4d07-4e7d-af0c-546bdca32fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one=profile.copy()\n",
    "two=profile.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98cd31b-4af3-47f8-9007-70d02a604360",
   "metadata": {},
   "outputs": [],
   "source": [
    "residence_profile=profile.copy() #save for comparing with TKE later\n",
    "\n",
    "\n",
    "#Nan out zeros\n",
    "cmap='plasma'\n",
    "profile2=profile.copy()\n",
    "profile2[profile2==0]=np.nan\n",
    "\n",
    "\n",
    "#PLOTTING\n",
    "# plt.imshow(profile.T);plt.gca().invert_yaxis()\n",
    "plt.contourf(profile2*100,cmap=cmap,levels=50)\n",
    "\n",
    "cbar=plt.colorbar(label='normalized count (%)')\n",
    "plt.ylabel('z (km)');plt.xlabel('total entrainment residence time (mins)')\n",
    "plt.title('Entrainment Count')\n",
    "\n",
    "#FIXING TICKS\n",
    "\n",
    "plt.yticks(np.arange(Nz));\n",
    "new_ytick_labels = np.round(data['zf'].values[:Nz], 2);\n",
    "plt.gca().set_yticklabels(new_ytick_labels, fontsize=8, rotation=0);\n",
    "\n",
    "xticks = plt.gca().get_xticks()\n",
    "new_labels = [str(int(tick * 5)) for tick in xticks]\n",
    "plt.gca().set_xticklabels(new_labels);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44ea838-0643-4004-8f1a-680ea88efa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts\n",
    "for ind,(z,l) in enumerate(zip(zs,lens)):\n",
    "    print(ind,z,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea82606-94d0-49e4-b788-55d1f13a0b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading Important Variables\n",
    "# ##############\n",
    "# if 'emptylike' not in globals():\n",
    "#     print('loading neccessary variables')\n",
    "#     variable='w'; w_data=data[variable] #get w data\n",
    "#     w_data=w_data.interp(zf=data['zh']).data #interpolation w data z coordinate from zh to zf\n",
    "#     variable='qv'; qv_data=data[variable].data # get qc data\n",
    "#     variable='qc'; qc_data=data[variable].data # get qc data\n",
    "#     variable='qi'; qi_data=data[variable].data # get qc data\n",
    "#     qc_plus_qi=qc_data+qi_data\n",
    "#     buoyancy_data=data['buoyancy'].data\n",
    "\n",
    "#     import h5py\n",
    "#     with h5py.File(dir + 'Variable_Calculation/' + 'theta_e'+f'_{res}_{t_res}'+'.h5', 'r') as f:\n",
    "#         theta_e_data = f['theta_e'][:]\n",
    "    \n",
    "#     print('done')\n",
    "#     empty_like=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a805137a-73da-4ebb-88d4-58abf5a2ade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def call_variables(t): \n",
    "#     if np.mod(t,25)==0: print(f'loading variables for time {t}')\n",
    "#     variable='w'; w_data=data[variable].isel(time=t).interp(zf=data['zh']).data #get w_data and interpolation w data z coordinate from zh to zf\n",
    "#     variable='qv'; qv_data=data[variable].isel(time=t).data # get qc data\n",
    "#     variable='qc'; qc_data=data[variable].isel(time=t).data # get qc data\n",
    "#     variable='qi'; qi_data=data[variable].isel(time=t).data # get qc data\n",
    "#     qc_plus_qi=qc_data+qi_data\n",
    "#     variable='th'; th_data=data[variable].isel(time=t).data # get qc data\n",
    "#     variable='buoyancy'; buoyancy_data=data[variable].isel(time=t).data # get qc data\n",
    "    \n",
    "#     import h5py\n",
    "#     with h5py.File(dir + 'Variable_Calculation/' + 'theta_e'+f'_{res}_{t_res}'+'.h5', 'r') as f:\n",
    "#         theta_e_data = f['theta_e'][t]\n",
    "        \n",
    "#     if np.mod(t,25)==0:print(f'done loading')\n",
    "\n",
    "#     return w_data,qv_data,qc_data,qi_data,qc_plus_qi,th_data,buoyancy_data,theta_e_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7b24d5-5b64-48b7-84c1-c3259720f739",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nz=len(data['zh'])\n",
    "profile=np.zeros((Nz,18)); #residence time by Z levels\n",
    "counter=np.zeros_like(profile)\n",
    "\n",
    "Np=125000-1\n",
    "for p in np.arange(Np):\n",
    "    out=residence_times(p,type='e',updraft_type='cloudy')\n",
    "    if np.any(out)==True:\n",
    "        lens=out[0]\n",
    "        # print(lens)\n",
    "        \n",
    "        ts=out[1]\n",
    "        zs=out[2]\n",
    "        ys=out[3]\n",
    "        xs=out[4]\n",
    "\n",
    "        \n",
    "        for ind,(z,l) in enumerate(zip(zs,lens)): \n",
    "            profile[z,l]+=w_data[ts[ind],zs[ind],ys[ind],xs[ind]] #USE IF LOADING IN FULL VARIABLE\n",
    "            # t=ts[ind];variable='w'; w_data=data[variable].isel(time=t).interp(zf=data['zh']).data #get w_data and interpolation w data z coordinate from zh to zf #THIS IS WAY TOO SLOW\n",
    "            # profile[z,l]+=w_data[zs[ind],ys[ind],xs[ind]]\n",
    "            counter[z,l]+=1\n",
    "\n",
    "#averaging by number of parcel\n",
    "mask=profile!=0\n",
    "profile[mask]/=counter[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edbb097-cb74-467a-9a65-1b65eca4c690",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap='plasma'\n",
    "profile[profile==0]=np.nan\n",
    "plt.contourf(profile,cmap=cmap,levels=50)\n",
    "plt.colorbar(label='w (m/s) per parcel')\n",
    "plt.xlabel('total entrainment residence time (mins)');plt.ylabel('z (km)')\n",
    "plt.title('Entrained W Profile')\n",
    "\n",
    "xticks = plt.gca().get_xticks()\n",
    "new_labels = [str(int(tick * 5)) for tick in xticks]\n",
    "plt.gca().set_xticklabels(new_labels);\n",
    "\n",
    "\n",
    "plt.yticks(np.arange(Nz));\n",
    "new_ytick_labels = np.round(data['zf'].values[:Nz], 2);\n",
    "plt.gca().set_yticklabels(new_ytick_labels, fontsize=8, rotation=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ffbe1f-9579-451d-8ca1-803952631d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nz=len(data['zh'])\n",
    "profile=np.zeros((Nz,18)); #residence time by Z levels\n",
    "counter=np.zeros_like(profile)\n",
    "\n",
    "Np=125000-1\n",
    "for p in np.arange(Np):\n",
    "    out=residence_times(p,type='e',updraft_type='cloudy')\n",
    "    if np.any(out)==True:\n",
    "        lens=out[0]\n",
    "        # print(lens)\n",
    "        \n",
    "        ts=out[1]\n",
    "        zs=out[2]\n",
    "        ys=out[3]\n",
    "        xs=out[4]\n",
    "\n",
    "        \n",
    "        for ind,(z,l) in enumerate(zip(zs,lens)):\n",
    "            profile[z,l]+=theta_e_data[ts[ind],zs[ind],ys[ind],xs[ind]] #USE IF LOADING IN FULL VARIABLE\n",
    "            # t=ts[ind];import h5py #     with h5py.File(dir + 'Variable_Calculation/' + 'theta_e'+f'_{res}_{t_res}'+'.h5', 'r') as f: #         theta_e_data = f['theta_e'][t] #THIS IS WAY TOO SLOW\n",
    "            # profile[z,l]+=theta_e_data[zs[ind],ys[ind],xs[ind]]\n",
    "            counter[z,l]+=1\n",
    "\n",
    "#averaging by number of parcel\n",
    "mask=profile!=0\n",
    "profile[mask]/=counter[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2567d02-5414-4294-b110-de66134ef919",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap='viridis'\n",
    "profile[profile==0]=np.nan\n",
    "\n",
    "plt.contourf(profile,cmap=cmap,levels=50)#,vmin=200)\n",
    "plt.colorbar(label='theta_e (K) per parcel')\n",
    "plt.xlabel('total entrainment residence time (mins)');plt.ylabel('z (km)')\n",
    "plt.title(r'Entrained $\\theta_e$ Profile')\n",
    "\n",
    "xticks = plt.gca().get_xticks()\n",
    "new_labels = [str(int(tick * 5)) for tick in xticks]\n",
    "plt.gca().set_xticklabels(new_labels);\n",
    "\n",
    "plt.yticks(np.arange(Nz));\n",
    "new_ytick_labels = np.round(data['zf'].values[:Nz], 2);\n",
    "plt.gca().set_yticklabels(new_ytick_labels, fontsize=8, rotation=0);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ff3826-ee75-47d2-b108-11686e40a0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Some Data\n",
    "tke_data=data['tke'].interp(zf=data['zh']).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c963c03a-b369-4793-8f1b-7ae3fd1e0905",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nz=len(data['zh'])\n",
    "profile=np.zeros((Nz,18)); #residence time by Z levels\n",
    "counter=np.zeros_like(profile)\n",
    "\n",
    "Np=125000-1\n",
    "for p in np.arange(Np):\n",
    "    out=residence_times(p,type='e',updraft_type='cloudy')\n",
    "    if np.any(out)==True:\n",
    "        lens=out[0]\n",
    "        # print(lens)\n",
    "        \n",
    "        ts=out[1]\n",
    "        zs=out[2]\n",
    "        ys=out[3]\n",
    "        xs=out[4]\n",
    "\n",
    "        \n",
    "        for ind,(z,l) in enumerate(zip(zs,lens)):\n",
    "            profile[z,l]+=tke_data[ts[ind],zs[ind],ys[ind],xs[ind]] #USE IF LOADING IN FULL VARIABLE\n",
    "            # t=ts[ind];tke_data=data['tke'].isel(time=t).interp(zf=data['zh']).data\n",
    "            # profile[z,l]+=tke_data[zs[ind],ys[ind],xs[ind]]\n",
    "            counter[z,l]+=1\n",
    "\n",
    "#averaging by number of parcel\n",
    "mask=profile!=0\n",
    "profile[mask]/=counter[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de93dc66-8c56-46bf-b713-9e6c8d6a6c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "TKE_profile=profile.copy()\n",
    "\n",
    "cmap='plasma'\n",
    "profile[profile==0]=np.nan\n",
    "\n",
    "plt.contourf(profile,cmap=cmap, levels=50)\n",
    "plt.colorbar(label=r'TKE ($m^2/s^2$) per parcel')\n",
    "plt.xlabel('total entrainment residence time (mins)');plt.ylabel('z (km)')\n",
    "plt.title('Entrained TKE Profile')\n",
    "\n",
    "xticks = plt.gca().get_xticks()\n",
    "new_labels = [str(int(tick * 5)) for tick in xticks]\n",
    "plt.gca().set_xticklabels(new_labels);\n",
    "\n",
    "plt.yticks(np.arange(Nz));\n",
    "new_ytick_labels = np.round(data['zf'].values[:Nz], 2);\n",
    "plt.gca().set_yticklabels(new_ytick_labels, fontsize=8, rotation=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533d2174-898c-42af-a00f-ea3648bb7cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING TESTING TESTING\n",
    "\n",
    "\n",
    "TKE_norm = (TKE_profile - np.min(TKE_profile)) / (np.max(TKE_profile) - np.min(TKE_profile))\n",
    "residence_norm = (residence_profile - np.min(residence_profile)) / (np.max(residence_profile) - np.min(residence_profile))\n",
    "\n",
    "compare_array = (1-TKE_norm) - (1-residence_norm)\n",
    "\n",
    "\n",
    "\n",
    "plt.contourf(compare_array)\n",
    "plt.colorbar(label='Difference of Norms')\n",
    "plt.xlabel('total entrainment residence time (mins)');plt.ylabel('z (km)')\n",
    "plt.title('Entrainment Compared with Entrained TKE Profile')\n",
    "\n",
    "####\n",
    "\n",
    "xticks = plt.gca().get_xticks()\n",
    "new_labels = [str(int(tick * 5)) for tick in xticks]\n",
    "plt.gca().set_xticklabels(new_labels);\n",
    "\n",
    "plt.yticks(np.arange(Nz));\n",
    "new_ytick_labels = np.round(data['zf'].values[:Nz], 2);\n",
    "plt.gca().set_yticklabels(new_ytick_labels, fontsize=8, rotation=0);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
