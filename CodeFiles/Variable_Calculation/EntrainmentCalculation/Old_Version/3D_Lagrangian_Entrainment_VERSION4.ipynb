{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3411e523-2126-4767-aae1-fbef4b65f0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS FUNCTION IS FOR RUNNING WITH SLURM JOB ARRAY\n",
    "#(SPLITS UP JOB_ARRAY BELOW INTO EVEN MORE TASKS)\n",
    "def StartSlurmJobArray(num_jobs,num_slurm_jobs, ISRUN):\n",
    "    job_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0)) #this is the current SBATCH job id\n",
    "    if job_id==0: job_id=1\n",
    "    if ISRUN==False:\n",
    "        start_job=1;end_job=num_jobs+1\n",
    "        return start_job,end_job\n",
    "    total_elements=num_jobs #total num of variables\n",
    "\n",
    "    job_range = total_elements // num_slurm_jobs  # Base size for each chunk\n",
    "    remaining = total_elements % num_slurm_jobs   # Number of chunks with 1 extra \n",
    "    \n",
    "    # Function to compute the start and end for each job_id\n",
    "    def get_job_range(job_id, num_slurm_jobs):\n",
    "        job_id-=1\n",
    "        # Add one extra element to the first 'remaining' chunks\n",
    "        start_job = job_id * job_range + min(job_id, remaining)\n",
    "        end_job = start_job + job_range + (1 if job_id < remaining else 0)\n",
    "    \n",
    "        if job_id == num_slurm_jobs - 1: \n",
    "            end_job = total_elements \n",
    "        return start_job, end_job\n",
    "    # def job_testing():\n",
    "    #     #TESTING\n",
    "    #     start=[];end=[]\n",
    "    #     for job_id in range(1,num_slurm_jobs+1):\n",
    "    #         start_job, end_job = get_job_range(job_id)\n",
    "    #         print(start_job,end_job)\n",
    "    #         start.append(start_job)\n",
    "    #         end.append(end_job)\n",
    "    #     print(np.all(start!=end))\n",
    "    #     print(len(np.unique(start))==len(start))\n",
    "    #     print(len(np.unique(end))==len(end))\n",
    "    # job_testing()\n",
    "    # if sbatch==True:\n",
    "        \n",
    "    start_job, end_job = get_job_range(job_id, num_slurm_jobs)\n",
    "    index_adjust=start_job\n",
    "    # print(f'start_job = {start_job}, end_job = {end_job}')\n",
    "    if start_job==0: start_job=1\n",
    "    if end_job==total_elements: end_job+=1\n",
    "    return start_job,end_job\n",
    "\n",
    "# job_id=1\n",
    "# [start_slurm_job,end_slurm_job,slurm_index_adjust]=StartSlurmJobArray(num_jobs,num_slurm_jobs,ISRUN)\n",
    "# parcel=parcel1.isel(xh=slice(start_job,end_job))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7218e3-f689-4e05-a8f2-3895d6925849",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADING LIBRARIES\n",
    "import os\n",
    "os.environ[\"DASK_WORKER_MEMORY_LIMIT\"] = \"25GB\"  #\"160GB\" for full node\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import xarray as xr\n",
    "import dask  # import after setting env var\n",
    "import time\n",
    "import pickle\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f937c71-cf2c-4c9a-9dbc-3d87d5470d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAIN DIRECTORIES\n",
    "mainDirectory='/mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/'\n",
    "scratchDirectory='/home/air673/koa_scratch/'\n",
    "codeDirectory='/mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/Project_Algorithms/Entrainment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2700651c-da4d-44b1-a454-80717cf3c100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADING DATA\n",
    "def GetDataDirectories(simulationNumber):\n",
    "    if simulationNumber == 1:\n",
    "        Directory=os.path.join(mainDirectory,'Model/cm1r20.3/run')\n",
    "        res='1km'; t_res='5min'; Np_str='1e6'; Nz_str='34'\n",
    "    elif simulationNumber == 2:\n",
    "        Directory=scratchDirectory\n",
    "        res='1km'; t_res='1min'; Np_str='50e6'; Nz_str='95'\n",
    "    elif simulationNumber == 3:\n",
    "        Directory=scratchDirectory\n",
    "        res='250m'; t_res='1min'; Np_str='50e6'; Nz_str='95'\n",
    "        \n",
    "    dataDirectory = os.path.join(Directory, f\"cm1out_{res}_{t_res}_{Nz_str}nz.nc\")\n",
    "    parcelDirectory = os.path.join(Directory,f\"cm1out_pdata_{res}_{t_res}_{Np_str}np.nc\")\n",
    "    return dataDirectory, parcelDirectory, res,t_res,Np_str,Nz_str\n",
    "    \n",
    "def GetData(dataDirectory, parcelDirectory):\n",
    "    dataNC = xr.open_dataset(dataDirectory, decode_timedelta=True) \n",
    "    parcelNC = xr.open_dataset(parcelDirectory, decode_timedelta=True) \n",
    "    return dataNC,parcelNC\n",
    "\n",
    "def SubsetDataVars(dataNC):\n",
    "    varList = [\"thflux\", \"qvflux\", \"tsk\", \"cape\", \n",
    "               \"cin\", \"lcl\", \"lfc\", \"th\",\n",
    "               \"prs\", \"rho\", \"qv\", \"qc\",\n",
    "               \"qr\", \"qi\", \"qs\",\"qg\", \n",
    "               \"buoyancy\", \"uinterp\", \"vinterp\", \"winterp\",]\n",
    "    \n",
    "    varList += [\"ptb_hadv\", \"ptb_vadv\", \"ptb_hidiff\", \"ptb_vidiff\",\n",
    "                \"ptb_hturb\", \"ptb_vturb\", \"ptb_mp\", \"ptb_rdamp\", \n",
    "                \"ptb_rad\", \"ptb_div\", \"ptb_diss\",]\n",
    "    \n",
    "    varList += [\"qvb_hadv\", \"qvb_vadv\", \"qvb_hidiff\", \"qvb_vidiff\", \n",
    "                \"qvb_hturb\", \"qvb_vturb\", \"qvb_mp\",]\n",
    "    \n",
    "    varList += [\"wb_hadv\", \"wb_vadv\", \"wb_hidiff\", \"wb_vidiff\",\n",
    "                \"wb_hturb\", \"wb_vturb\", \"wb_pgrad\", \"wb_rdamp\", \"wb_buoy\",]\n",
    "\n",
    "    return dataNC[varList]\n",
    "\n",
    "[dataDirectory,parcelDirectory, res,t_res,Np_str,Nz_str] = GetDataDirectories(simulationNumber=1)\n",
    "[data1,parcel1] = GetData(dataDirectory, parcelDirectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341304a7-523a-4aa8-aa10-71c6651ba5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir='/mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a9d525-e790-4970-8944-a618286b46e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e2aac7-33e0-4d8e-9044-75b63f1149b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "dir2='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "path=dir2+'../Functions/'\n",
    "sys.path.append(path)\n",
    "\n",
    "import NumericalFunctions\n",
    "from NumericalFunctions import * # import NumericalFunctions \n",
    "import PlottingFunctions\n",
    "from PlottingFunctions import * # import PlottingFunctions\n",
    "\n",
    "\n",
    "# # Get all functions in NumericalFunctions\n",
    "# import inspect\n",
    "# functions = [f[0] for f in inspect.getmembers(NumericalFunctions, inspect.isfunction)]\n",
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26efa6d-772f-4968-9cc2-b69bf7549776",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8d0b03-65e9-47d5-bab9-03fc3594c6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "#ENTRAINMENT DATA_TYPE SETUP\n",
    "PROCESSING=False #if using full entrainment calculations\n",
    "PROCESSING=True #if using turbulence-filtered entrainment calculations\n",
    "################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4e893e-cf10-4aba-bf93-2f1b1b2c1ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "#JOB ARRAY SETUP\n",
    "################################\n",
    "# how many total jobs are being run? i.e. array=1-100 ==> num_jobs=100\n",
    "if Np_str=='1e6':\n",
    "    num_jobs=60 #1M parcels\n",
    "    num_slurm_jobs=10\n",
    "elif Np_str=='50e6':\n",
    "    num_jobs=600 #50M parcels\n",
    "    num_slurm_jobs=100\n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d8bbed-46c9-4bd9-9613-88baed084348",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "#DATA LOADING FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5fb140-365f-4b07-b49a-529f37efd183",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INITIALIZE DATA FUNCTION\n",
    "###############################################################\n",
    "#INITIALIZE DATA FUNCTION\n",
    "###############################################################\n",
    "def InitiateArray(out_file,vars,t_chunk_size,z_chunk_size,y_chunk_size,x_chunk_size,t_size=None,z_size=None,y_size=None,x_size=None):\n",
    "    # Define array dimensions (adjust based on your data)\n",
    "\n",
    "    if t_size==None:\n",
    "        t_size = len(data1['time'])  # Number of timesteps\n",
    "    if z_size==None:\n",
    "        z_size = len(data1['zh'])    # Number of vertical levels\n",
    "    if y_size==None:\n",
    "        y_size = len(data1['yh'])    # Number of vertical levels\n",
    "    if x_size==None:\n",
    "        x_size = len(data1['xh'])    # Number of vertical levels\n",
    "    \n",
    "    with h5py.File(out_file, 'w') as f: \n",
    "        # Check if the dataset 'theta_e' already exists\n",
    "        for var_name in vars:\n",
    "            if var_name not in f:\n",
    "                # Create a dataset with the full size for all time steps (initially empty)\n",
    "                f.create_dataset(var_name, \n",
    "                                 (t_size, z_size, y_size, x_size),  # Full size for all timesteps\n",
    "                                 chunks=(t_chunk_size, z_chunk_size, y_chunk_size, x_chunk_size))  # Chunks for time axis to allow resizing\n",
    "\n",
    "def InitiateArray_Job(data,out_file,vars,t_chunk_size,z_chunk_size,y_chunk_size,x_chunk_size,t_size=None,z_size=None,y_size=None,x_size=None):\n",
    "    # Define array dimensions (adjust based on your data)\n",
    "\n",
    "    if t_size==None:\n",
    "        t_size = len(data['time'])  # Number of timesteps\n",
    "    if z_size==None:\n",
    "        z_size = len(data1['zh'])    # Number of vertical levels\n",
    "    if y_size==None:\n",
    "        y_size = len(data1['yh'])    # Number of vertical levels\n",
    "    if x_size==None:\n",
    "        x_size = len(data1['xh'])    # Number of vertical levels\n",
    "    \n",
    "    with h5py.File(out_file, 'w') as f: \n",
    "        # Check if the dataset 'theta_e' already exists\n",
    "        for var_name in vars:\n",
    "            if var_name not in f:\n",
    "                # Create a dataset with the full size for all time steps (initially empty)\n",
    "                f.create_dataset(var_name, \n",
    "                                 (t_size, z_size, y_size, x_size),  # Full size for all timesteps\n",
    "                                 chunks=(t_chunk_size, z_chunk_size, y_chunk_size, x_chunk_size))  # Chunks for time axis to allow resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5ca6cc-cb55-4925-850e-d57777d332f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JOB ARRAY SETUP\n",
    "def StartJobArray(job_id,num_jobs):\n",
    "    total_elements=len(data1['time']) #total num of variables\n",
    "\n",
    "    if num_jobs >= total_elements:\n",
    "        raise ValueError(\"Number of jobs cannot be greater than or equal to total elements.\")\n",
    "    \n",
    "    job_range = total_elements // num_jobs  # Base size for each chunk\n",
    "    remaining = total_elements % num_jobs   # Number of chunks with 1 extra \n",
    "    \n",
    "    # Function to compute the start and end for each job_id\n",
    "    def get_job_range(job_id, num_jobs):\n",
    "        job_id-=1\n",
    "        # Add one extra element to the first 'remaining' chunks\n",
    "        start_job = job_id * job_range + min(job_id, remaining)\n",
    "        end_job = start_job + job_range + (1 if job_id < remaining else 0)\n",
    "    \n",
    "        if job_id == num_jobs - 1: \n",
    "            end_job = total_elements #- 1\n",
    "        return start_job, end_job\n",
    "    # def job_testing():\n",
    "    #     #TESTING\n",
    "    #     start=[];end=[]\n",
    "    #     for job_id in range(1,num_jobs+1):\n",
    "    #         start_job, end_job = get_job_range(job_id)\n",
    "    #         print(start_job,end_job)\n",
    "    #         start.append(start_job)\n",
    "    #         end.append(end_job)\n",
    "    #     print(np.all(start!=end))\n",
    "    #     print(len(np.unique(start))==len(start))\n",
    "    #     print(len(np.unique(end))==len(end))\n",
    "    # job_testing()\n",
    "\n",
    "    # if sbatch==True:\n",
    "    #     job_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0)) #this is the current SBATCH job id\n",
    "    #     if job_id==0: job_id=1\n",
    "        \n",
    "    start_job, end_job = get_job_range(job_id, num_jobs)\n",
    "    index_adjust=start_job\n",
    "    # print(f'start_job = {start_job}, end_job = {end_job}')\n",
    "    return start_job,end_job,index_adjust\n",
    "# job_id=1\n",
    "# [start_job,end_job,index_adjust]=StartJobArray(job_id,num_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a417ea3-ee3e-45bb-8605-3a13087bdaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Back Data Later\n",
    "##############\n",
    "def make_data_dict(in_file,var_names,read_type,start_job,end_job):\n",
    "    if read_type=='h5py':\n",
    "        with h5py.File(in_file, 'r') as f:\n",
    "            data_dict = {var_name: f[var_name][start_job:end_job] for var_name in var_names}\n",
    "\n",
    "            \n",
    "    elif read_type=='xarray':\n",
    "        in_data = xr.open_dataset(\n",
    "            in_file,\n",
    "            engine='h5netcdf',\n",
    "            phony_dims='sort',\n",
    "            chunks={'phony_dim_0': 100, 'phony_dim_1': 1_000_000} \n",
    "        )\n",
    "        data_dict = {k: in_data[k][start_job:end_job].compute().data for k in var_names}\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "# read_type='xarray'\n",
    "read_type='h5py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f18fd4-650e-4c23-ac9d-0e6be41f2eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetData(data1,parcel1,start_job,end_job):\n",
    "    #Indexing Array with JobArray\n",
    "    data=data1.isel(time=slice(start_job,end_job))\n",
    "    parcel=parcel1.isel(time=slice(start_job,end_job))\n",
    "    return data,parcel\n",
    "# [data,parcel]=GetData(data1,parcel1,start_job,end_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112e89b9-690d-4a85-81f2-1900fcf2a948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetSpatialData(data,start_job,end_job):\n",
    "    import h5py\n",
    "    dir2=dir+'Project_Algorithms/Lagrangian_Arrays/OUTPUT/'\n",
    "    in_file=dir2+f'lagrangian_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "    \n",
    "    var_names = ['Z', 'Y', 'X']\n",
    "    data_dict = make_data_dict(in_file,var_names,read_type,start_job,end_job)\n",
    "    Z,Y,X = (data_dict[k] for k in var_names)\n",
    "    \n",
    "    # #Making Time Matrix\n",
    "    Nt=len(data['time'])\n",
    "    T = np.broadcast_to(np.arange(Nt)[:, None], Z.shape)  # shape: (Nt, p) #A_c==>Z\n",
    "    \n",
    "    # check_memory(globals())\n",
    "    return T,Z,Y,X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d6d252-a5c3-420f-8a03-90e6cf68a230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetAData(PROCESSING,start_job,end_job):\n",
    "    if PROCESSING==False:\n",
    "        dir2=dir+'Project_Algorithms/Lagrangian_Arrays/OUTPUT/'\n",
    "        in_file=dir2+f'lagrangian_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "        \n",
    "        var_names = ['A_g', 'A_c']\n",
    "        data_dict = make_data_dict(in_file,var_names,read_type,start_job,end_job)\n",
    "        A_g, A_c = (data_dict[k] for k in var_names) #, W\n",
    "    \n",
    "    elif PROCESSING==True:\n",
    "        dir2=dir+'Project_Algorithms/Entrainment/OUTPUT/'\n",
    "        in_file=dir2+f'processed_binary_arrays_{res}_{t_res}_{Np_str}.h5'\n",
    "        \n",
    "        var_names = ['A_g_Processed', 'A_c_Processed']\n",
    "        data_dict = make_data_dict(in_file,var_names,read_type,start_job,end_job)\n",
    "        A_g,A_c = (data_dict[k] for k in var_names)\n",
    "        # check_memory(globals())\n",
    "    return A_g,A_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc74b8ac-c06b-4b93-bf8a-87fd25b59a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetAPrior(PROCESSING,start_job): #FIX FOR JOB_ARRAY (SO FIRST TIMESTEP WILL NOT BE BLANK)\n",
    "    if start_job!=0:\n",
    "        A_g_Prior,A_c_Prior=GetAData(PROCESSING,start_job-1,start_job);\n",
    "        return A_g_Prior,A_c_Prior\n",
    "    else:\n",
    "        return 0,0\n",
    "def GetAPost(PROCESSING,end_job): #*#*\n",
    "    if end_job!=len(data1['time']):\n",
    "        [A_g_Post,A_c_Post]=GetAData(PROCESSING,end_job-1,end_job+1)\n",
    "        return A_g_Post,A_c_Post\n",
    "    else:\n",
    "        return 0,0\n",
    "        \n",
    "def SubtractA(A,A_Prior):\n",
    "    D = np.zeros_like(A,dtype=np.int8)\n",
    "    D[1:, :] = A[1:, :]*1 - A[:-1, :]*1\n",
    "    if start_job!=0:\n",
    "        D[0, :] = A[0, :]*1-A_Prior*1 #FIX FOR FIRST TIMESTEP\n",
    "    return D\n",
    "def AddAs(A1,A2,A1_Prior):\n",
    "    S = np.zeros_like(A2,dtype=np.int8)\n",
    "    S[1:, :] = A2[1:, :]*1 + A1[:-1, :]*1\n",
    "    if start_job!=0:\n",
    "        S[0, :] = A2[0, :]*1 + A1_Prior*1 #FIX FOR FIRST TIMESTEP\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bc6da3-e010-45d7-968b-9319a6ea4c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FixDetrainment(A_Post): #*#*\n",
    "    #when using slurm_job_array, last timestep is not calculated\n",
    "    #since data is outside of access scope\n",
    "    #THIS FUNCTION FIXES THAT\n",
    "    D3 = np.zeros_like(A_Post.copy(),dtype=np.int8)\n",
    "    D3[1:, :] = A_Post[1:, :]*1 - A_Post[:-1, :]*1\n",
    "    D3[D3 > 0] = 0\n",
    "    return D3[1:]\n",
    "def Apply_FixDetrainment(D, A_Post):\n",
    "    D2 = np.zeros_like(D.copy())\n",
    "    D2[:-1] = D[1:]  # shift D back in time\n",
    "    D = D2.copy()\n",
    "    if not isinstance(A_Post, int):\n",
    "        D3 = FixDetrainment(A_Post)\n",
    "        D[-1] = D3.copy()\n",
    "    return D\n",
    "\n",
    "def FixDetrainment_S(A1_Post,A2_Post): #*#*\n",
    "    #when using slurm_job_array, last timestep is not calculated\n",
    "    #since data is outside of access scope\n",
    "    #THIS FUNCTION FIXES THAT\n",
    "    S3 = np.zeros_like(A2_Post.copy(),dtype=np.int8)\n",
    "    S3[1:, :] = A2_Post[1:, :]*1 + A1_Post[:-1, :]*1\n",
    "    return S3[1:]\n",
    "def Apply_FixDetrainment_S(S, A1_Post,A2_Post):\n",
    "    S2 = np.zeros_like(S.copy())\n",
    "    S2[:-1] = S[1:]  # shift D back in time\n",
    "    S = S2.copy()\n",
    "    if not isinstance(A1_Post, int) and not isinstance(A2_Post, int):\n",
    "        S3 = FixDetrainment_S(A1_Post,A2_Post)\n",
    "        S[-1] = S3.copy()\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b1507a-889f-4a89-a045-17fb3b4b76c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db75b3bc-5bdf-4a1f-91f1-543d8e3425aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "#Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13b53c9-f8ec-4ecb-beff-d474cec0beb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ed3d(data, T,Z,Y,X, A1,A2, A1_Prior,A2_Prior,A1_Post,A2_Post, type1): #*#*\n",
    "    # start_time = time.time()\n",
    "    \"\"\"\n",
    "    Function to compute 3D entrainment and update result array based on provided inputs.\n",
    "    \n",
    "    Returns a 3D (t,z) array containing the sum of the D array representing entrained parcels, by 1, and detrained parcels, by -1.\n",
    "    The finally array is then ordered by the appropiate index using the np.add.at function\n",
    "    \n",
    "    Parameters:\n",
    "    - A: The (t,p) lagrangian binary array.\n",
    "    - T: The (t,p) lagrangian time index array.\n",
    "    - Z: The (t,p) Lagrangian z index array.\n",
    "    - Y: The (t,p) Lagrangian y index array.\n",
    "    - X: The (t,p) Lagrangian x index array.\n",
    "\n",
    "    \"\"\"\n",
    "    # Compute the difference between neighboring elements along the first axis\n",
    "    D = SubtractA(A2,A2_Prior) #Entrainment/Detrainment\n",
    "    #General <==> Cloudy Updraft-Transfer Entrainment\n",
    "    if type1=='e':\n",
    "        S_init = AddAs(A1,A2,A1_Prior) \n",
    "    elif type1=='d':\n",
    "        S_init = AddAs(A2,A1,A2_Prior)\n",
    "        \n",
    "    # Update D for entrainment/detrainment\n",
    "    if type1=='e':\n",
    "        D[D < 0] = 0\n",
    "    elif type1=='d':\n",
    "        D[D > 0] = 0\n",
    "        D = Apply_FixDetrainment(D, A2_Post) #NEED TO SHIFT TO PREVIOUS TIME TO STORE D CORRECTLY \n",
    "        S_init = Apply_FixDetrainment_S(S_init, A2_Post, A1_Post) #NEED TO SHIFT TO PREVIOUS TIME TO STORE D CORRECTLY \n",
    "\n",
    "    #FINDING LOCATIONS WITH TRANSFER ENTRAINMENT\n",
    "    S = (S_init==2)*1\n",
    "    #Removing Transfer-Entrainment where D==0 (just in case)\n",
    "    S[D==0]=0 \n",
    "    \n",
    "    # Initialize time and vertical dimension arrays\n",
    "    Nt = len(data['time']); Nz = len(data1['zh']); Ny = len(data1['yh']); Nx = len(data1['xh'])\n",
    "    # Initialize result array\n",
    "    result1 = np.zeros((Nt, Nz, Ny, Nx))\n",
    "    result2 = np.zeros((Nt, Nz, Ny, Nx))\n",
    "\n",
    "    # Use np.add.at to accumulate values in the result array\n",
    "    np.add.at(result1, (T, Z, Y, X), D)\n",
    "    np.add.at(result2, (T, Z, Y, X), S)\n",
    "\n",
    "    # end_time = time.time()\n",
    "    # print(f\"Execution time: {(end_time - start_time)} seconds\")\n",
    "    return result1,result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1c1ede-07a6-41b4-9424-26a60b7cc496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeDictionary(**vars): #*#*\n",
    "    return vars\n",
    "def RunCalculation(data, A_g,A_c, T,Z,Y,X, A_g_Prior,A_c_Prior,A_g_Post,A_c_Post): #*#*  \n",
    "    # Set A based on PROCESSING state\n",
    "    # print('Calculating 3D Entrainment for General Updrafts')\n",
    "    [profile_array_e_g, profile_array_c_to_g_E] = ed3d(data, T,Z,Y,X, A1=A_c,A2=A_g, A1_Prior=A_c_Prior,A2_Prior=A_g_Prior,A1_Post=A_c_Post,A2_Post=A_g_Post, type1='e')\n",
    "    [profile_array_d_g, profile_array_g_to_c_D] = ed3d(data, T,Z,Y,X, A1=A_c,A2=A_g, A1_Prior=A_c_Prior,A2_Prior=A_g_Prior,A1_Post=A_c_Post,A2_Post=A_g_Post, type1='d')\n",
    "    \n",
    "    # Set A for the second block\n",
    "    # print('Calculating 3D Entrainment for Cloudy Updrafts')\n",
    "    [profile_array_e_c, profile_array_g_to_c_E] = ed3d(data, T,Z,Y,X, A1=A_g,A2=A_c, A1_Prior=A_g_Prior,A2_Prior=A_c_Prior,A1_Post=A_g_Post,A2_Post=A_c_Post, type1='e')\n",
    "    [profile_array_d_c, profile_array_c_to_g_D] = ed3d(data, T,Z,Y,X, A1=A_g,A2=A_c, A1_Prior=A_g_Prior,A2_Prior=A_c_Prior,A1_Post=A_g_Post,A2_Post=A_c_Post, type1='d')\n",
    "\n",
    "    VARs=MakeDictionary(profile_array_e_g=profile_array_e_g,profile_array_e_c=profile_array_e_c,\n",
    "                        profile_array_d_g=profile_array_d_g,profile_array_d_c=profile_array_d_c,\n",
    "                        profile_array_g_to_c_E=profile_array_g_to_c_E,profile_array_g_to_c_D=profile_array_g_to_c_D,\n",
    "                        profile_array_c_to_g_E=profile_array_c_to_g_E,profile_array_c_to_g_D=profile_array_c_to_g_D)\n",
    "    return VARs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9e8cf5-85a3-40d3-b13d-7a80d79d2d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveProfile(VARs,job_id,PROCESSING):\n",
    "    dir2=dir+'Project_Algorithms/Entrainment/SBATCH/job_out_3/'\n",
    "    \n",
    "    #SAVING\n",
    "    # print('SAVING')\n",
    "    if PROCESSING==False:\n",
    "        out_file=dir2+f'3D_entrainmentdetrainment_profiles_{res}_{t_res}_{Np_str}'\n",
    "    elif PROCESSING==True:\n",
    "        out_file=dir2+f'3D_entrainmentdetrainment_profiles_PREPROCESSING_{res}_{t_res}_{Np_str}'\n",
    "    out_file+=f'_{job_id}.h5'\n",
    "    \n",
    "    vars=VARs.keys()\n",
    "    t_chunk_size=end_job-start_job\n",
    "    z_chunk_size=2\n",
    "    InitiateArray_Job(data,out_file,vars,t_chunk_size=1,z_chunk_size=17,y_chunk_size=25,x_chunk_size=64)\n",
    "    \n",
    "    with h5py.File(out_file, 'a') as f: \n",
    "        for var in VARs:\n",
    "            f[f'{var}'][:]=VARs[var]\n",
    "    # print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990d7643-09a8-4ccf-9c8d-e8a95580695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "#RUNNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd959cb-f76d-4fc5-b518-766036e8dad8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[start_slurm_job,end_slurm_job]=StartSlurmJobArray(num_jobs=num_jobs,num_slurm_jobs=num_slurm_jobs,ISRUN=True) #if ISRUN is False, then will not run using slurm_job_array\n",
    "\n",
    "print(f\"Running on Slurm_Jobs for Slurm_Job_Ids: {(start_slurm_job,end_slurm_job-1)}\")\n",
    "\n",
    "job_id_list=np.arange(start_slurm_job,end_slurm_job)\n",
    "for job_id in job_id_list:\n",
    "    if job_id % 1 == 0: print(f'current job_id = {job_id}')\n",
    "    [start_job,end_job,index_adjust]=StartJobArray(job_id,num_jobs)\n",
    "\n",
    "    #SLICING DATA\n",
    "    [data,parcel]=GetData(data1,parcel1,start_job,end_job)\n",
    "    [A_g,A_c]=GetAData(PROCESSING,start_job,end_job)\n",
    "    [A_g_Prior,A_c_Prior]=GetAPrior(PROCESSING,start_job) #FIX FOR FIRST TIMESTEP\n",
    "    [A_g_Post,A_c_Post]=GetAPost(PROCESSING,end_job) #FIX FOR LAST TIMESTEP #*#*#\n",
    "    [T,Z,Y,X]=GetSpatialData(data,start_job,end_job)\n",
    "\n",
    "    #RUNNING CALCULATION\n",
    "    VARs=RunCalculation(data, A_g,A_c, T,Z,Y,X, A_g_Prior,A_c_Prior,A_g_Post,A_c_Post) #*#*\n",
    "\n",
    "    #SAVING\n",
    "    SaveProfile(VARs,job_id,PROCESSING)\n",
    "    # check_memory(globals())\n",
    "    del VARs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d955ca78-4e4a-4a36-b28e-2af2debf508b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "#RECOMBINE SEPERATE JOB_ARRAYS AFTER\n",
    "recombine=False #KEEP FALSE IF JOB ARRAY IS RUNNING\n",
    "# recombine=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28761ce0-2d1d-4ee6-8477-39f425f26f05",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def Recombine(PROCESSING,num_jobs):\n",
    "#     dir2=dir+'Project_Algorithms/Entrainment/SBATCH/job_out_3/'\n",
    "#     dir3=dir+'Project_Algorithms/Entrainment/OUTPUT/'\n",
    "    \n",
    "#     if PROCESSING==False:\n",
    "#         out_file=dir3+f'3D_entrainmentdetrainment_profiles_{res}_{t_res}_{Np_str}.h5'\n",
    "#     elif PROCESSING==True:\n",
    "#         out_file=dir3+f'3D_entrainmentdetrainment_profiles_PREPROCESSING_{res}_{t_res}_{Np_str}.h5'\n",
    "    \n",
    "#     vars=[\"profile_array_e_g\",\"profile_array_e_c\",\n",
    "#           \"profile_array_d_g\",\"profile_array_d_c\",\n",
    "#           \"profile_array_g_to_c_E\",\"profile_array_g_to_c_D\",\n",
    "#           \"profile_array_c_to_g_E\", \"profile_array_c_to_g_D\"]\n",
    "#     InitiateArray(out_file,vars,t_chunk_size=50,z_chunk_size=17,y_chunk_size=25,x_chunk_size=64)\n",
    "    \n",
    "#     with h5py.File(out_file, 'r+') as f_out:\n",
    "#         for job_id in np.arange(1,num_jobs+1):\n",
    "#             if np.mod(job_id,10)==0: print(f\"job_id = {job_id}\")\n",
    "#             [a,b,_]=StartJobArray(job_id,num_jobs)\n",
    "    \n",
    "#             if PROCESSING==False:\n",
    "#                 in_file=dir2+f'3D_entrainmentdetrainment_profiles_{res}_{t_res}_{Np_str}_{job_id}.h5'\n",
    "#             elif PROCESSING==True:\n",
    "#                 in_file=dir2+f'3D_entrainmentdetrainment_profiles_PREPROCESSING_{res}_{t_res}_{Np_str}_{job_id}.h5'\n",
    "#             with h5py.File(in_file, 'r') as f_in: \n",
    "#                 for var in vars:\n",
    "#                     f_out[var][a:b]=f_in[var][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16f9c7f-464c-4674-8a05-292f0fc604c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recombine_Dask(PROCESSING): #*#*\n",
    "    #DASK-ENABLED\n",
    "    import glob\n",
    "    import re\n",
    "    from dask.diagnostics import ProgressBar\n",
    "\n",
    "    def recombine_func(in_files, out_file):\n",
    "        # Set Dask memory management config to avoid OOM without killing the job\n",
    "\n",
    "        matching_files = sorted(\n",
    "            glob.glob(in_files),\n",
    "            key=lambda f: int(re.search(r'_(\\d+)\\.h5$', f).group(1))\n",
    "        )\n",
    "\n",
    "        print(f'recombining {len(matching_files)} files')\n",
    "        print('Forming Dask Delayed Object')\n",
    "        if res=='1km' and Np_str=='1e6':\n",
    "            chunks = {  # 1*95*100*256 *4/1e9 = 3.5 mb per chunk * 8 variables = ~28 mb at a time\n",
    "                'phony_dim_0': 1,   # 132 chunks\n",
    "                'phony_dim_1': 34,  # 1 chunk\n",
    "                'phony_dim_2': 100, # 2 chunks\n",
    "                'phony_dim_3': 256, # 2 chunks\n",
    "            }  # Total chunks ~ 132*2*2 = 528 < 1e5 chunks\n",
    "        if res=='250m':\n",
    "            chunks = {  # 1*95*100*256 *4/1e9 = 9 mb per chunk * 8 variables = ~80 mb at a time\n",
    "                'phony_dim_0': 1,   # 661 chunks\n",
    "                'phony_dim_1': 95,  # 1 chunk\n",
    "                'phony_dim_2': 100, # 8 chunks\n",
    "                'phony_dim_3': 256, # 8 chunks\n",
    "            }  # Total chunks ~ 661*8*8 = 42000 < 1e5 chunks\n",
    "\n",
    "        out = xr.open_mfdataset(\n",
    "            matching_files,\n",
    "            engine='h5netcdf',\n",
    "            concat_dim='phony_dim_0',\n",
    "            combine='nested',\n",
    "            phony_dims='sort',\n",
    "            chunks=chunks,\n",
    "            parallel=True\n",
    "        )\n",
    "\n",
    "        print('RUNNING')\n",
    "        with ProgressBar():\n",
    "            delayed_write = out.to_netcdf(out_file, engine='h5netcdf', compute=False)\n",
    "            delayed_write.compute()\n",
    "\n",
    "    ####################################################################################\n",
    "    dir2 = dir + 'Project_Algorithms/Entrainment/SBATCH/job_out_3/'\n",
    "    dir3 = dir + 'Project_Algorithms/Entrainment/OUTPUT/'\n",
    "\n",
    "    if PROCESSING == False:\n",
    "        in_files = dir2 + f'3D_entrainmentdetrainment_profiles_{res}_{t_res}_{Np_str}_*.h5'\n",
    "    elif PROCESSING == True:\n",
    "        in_files = dir2 + f'3D_entrainmentdetrainment_profiles_PREPROCESSING_{res}_{t_res}_{Np_str}_*.h5'\n",
    "\n",
    "    if PROCESSING == False:\n",
    "        out_file = dir3 + f'3D_entrainmentdetrainment_profiles_{res}_{t_res}_{Np_str}.h5'\n",
    "    elif PROCESSING == True:\n",
    "        out_file = dir3 + f'3D_entrainmentdetrainment_profiles_PREPROCESSING_{res}_{t_res}_{Np_str}.h5'\n",
    "\n",
    "    recombine_func(in_files, out_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367d3d10-366f-4c5a-ab81-88ad5fd4a177",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if recombine==True:\n",
    "    # Recombine(PROCESSING,num_jobs=num_jobs)\n",
    "    Recombine_Dask(PROCESSING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa13f374-b4dc-4fc2-9a89-5f90b67bc3de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
