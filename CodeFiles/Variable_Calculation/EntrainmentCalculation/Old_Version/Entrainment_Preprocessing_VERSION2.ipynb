{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf81bf6-0a82-4533-b015-ee5cca588ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS FUNCTION IS FOR RUNNING WITH SLURM JOB ARRAY\n",
    "#(SPLITS UP JOB_ARRAY BELOW INTO EVEN MORE TASKS)\n",
    "def StartSlurmJobArray(num_jobs,num_slurm_jobs, ISRUN):\n",
    "    job_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0)) #this is the current SBATCH job id\n",
    "    if job_id==0: job_id=1\n",
    "    if ISRUN==False:\n",
    "        start_job=1;end_job=num_jobs+1\n",
    "        return start_job,end_job\n",
    "    total_elements=num_jobs #total num of variables\n",
    "\n",
    "    job_range = total_elements // num_slurm_jobs  # Base size for each chunk\n",
    "    remaining = total_elements % num_slurm_jobs   # Number of chunks with 1 extra \n",
    "    \n",
    "    # Function to compute the start and end for each job_id\n",
    "    def get_job_range(job_id, num_slurm_jobs):\n",
    "        job_id-=1\n",
    "        # Add one extra element to the first 'remaining' chunks\n",
    "        start_job = job_id * job_range + min(job_id, remaining)\n",
    "        end_job = start_job + job_range + (1 if job_id < remaining else 0)\n",
    "    \n",
    "        if job_id == num_slurm_jobs - 1: \n",
    "            end_job = total_elements \n",
    "        return start_job, end_job\n",
    "    # def job_testing():\n",
    "    #     #TESTING\n",
    "    #     start=[];end=[]\n",
    "    #     for job_id in range(1,num_slurm_jobs+1):\n",
    "    #         start_job, end_job = get_job_range(job_id)\n",
    "    #         print(start_job,end_job)\n",
    "    #         start.append(start_job)\n",
    "    #         end.append(end_job)\n",
    "    #     print(np.all(start!=end))\n",
    "    #     print(len(np.unique(start))==len(start))\n",
    "    #     print(len(np.unique(end))==len(end))\n",
    "    # job_testing()\n",
    "    # if sbatch==True:\n",
    "        \n",
    "    start_job, end_job = get_job_range(job_id, num_slurm_jobs)\n",
    "    index_adjust=start_job\n",
    "    # print(f'start_job = {start_job}, end_job = {end_job}')\n",
    "    if start_job==0: start_job=1\n",
    "    if end_job==total_elements: end_job+=1\n",
    "    return start_job,end_job\n",
    "\n",
    "# job_id=1\n",
    "# [start_slurm_job,end_slurm_job,slurm_index_adjust]=StartSlurmJobArray(num_jobs,num_slurm_jobs,ISRUN)\n",
    "# parcel=parcel1.isel(xh=slice(start_job,end_job))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaedc8a3-5fa2-4a86-94fd-f2bf961b3508",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import matplotlib.gridspec as gridspec\n",
    "import xarray as xr\n",
    "import os; import time\n",
    "import pickle\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f939068-fcd1-4e34-87d8-5b2816d67c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAIN DIRECTORIES\n",
    "mainDirectory='/mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/'\n",
    "scratchDirectory='/home/air673/koa_scratch/'\n",
    "codeDirectory='/mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/Project_Algorithms/Entrainment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7315b8-10e1-4a79-9077-e5eaa69203ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADING DATA\n",
    "def GetDataDirectories(simulationNumber):\n",
    "    if simulationNumber == 1:\n",
    "        Directory=os.path.join(mainDirectory,'Model/cm1r20.3/run')\n",
    "        res='1km'; t_res='5min'; Np_str='1e6'; Nz_str='34'\n",
    "    elif simulationNumber == 2:\n",
    "        Directory=scratchDirectory\n",
    "        res='1km'; t_res='1min'; Np_str='50e6'; Nz_str='95'\n",
    "    elif simulationNumber == 3:\n",
    "        Directory=scratchDirectory\n",
    "        res='250m'; t_res='1min'; Np_str='50e6'; Nz_str='95'\n",
    "        \n",
    "    dataDirectory = os.path.join(Directory, f\"cm1out_{res}_{t_res}_{Nz_str}nz.nc\")\n",
    "    parcelDirectory = os.path.join(Directory,f\"cm1out_pdata_{res}_{t_res}_{Np_str}np.nc\")\n",
    "    return dataDirectory, parcelDirectory, res,t_res,Np_str,Nz_str\n",
    "    \n",
    "def GetData(dataDirectory, parcelDirectory):\n",
    "    dataNC = xr.open_dataset(dataDirectory, decode_timedelta=True) \n",
    "    parcelNC = xr.open_dataset(parcelDirectory, decode_timedelta=True) \n",
    "    return dataNC,parcelNC\n",
    "\n",
    "def SubsetDataVars(dataNC):\n",
    "    varList = [\"thflux\", \"qvflux\", \"tsk\", \"cape\", \n",
    "               \"cin\", \"lcl\", \"lfc\", \"th\",\n",
    "               \"prs\", \"rho\", \"qv\", \"qc\",\n",
    "               \"qr\", \"qi\", \"qs\",\"qg\", \n",
    "               \"buoyancy\", \"uinterp\", \"vinterp\", \"winterp\",]\n",
    "    \n",
    "    varList += [\"ptb_hadv\", \"ptb_vadv\", \"ptb_hidiff\", \"ptb_vidiff\",\n",
    "                \"ptb_hturb\", \"ptb_vturb\", \"ptb_mp\", \"ptb_rdamp\", \n",
    "                \"ptb_rad\", \"ptb_div\", \"ptb_diss\",]\n",
    "    \n",
    "    varList += [\"qvb_hadv\", \"qvb_vadv\", \"qvb_hidiff\", \"qvb_vidiff\", \n",
    "                \"qvb_hturb\", \"qvb_vturb\", \"qvb_mp\",]\n",
    "    \n",
    "    varList += [\"wb_hadv\", \"wb_vadv\", \"wb_hidiff\", \"wb_vidiff\",\n",
    "                \"wb_hturb\", \"wb_vturb\", \"wb_pgrad\", \"wb_rdamp\", \"wb_buoy\",]\n",
    "\n",
    "    return dataNC[varList]\n",
    "\n",
    "[dataDirectory,parcelDirectory, res,t_res,Np_str,Nz_str] = GetDataDirectories(simulationNumber=1)\n",
    "[data1,parcel1] = GetData(dataDirectory, parcelDirectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd384c0f-a37e-4b9f-83c7-9e02f27c4887",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir='/mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455b64e3-907f-4a02-a163-3d72254d22ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70c666c-e571-4563-91fc-3ecf159f3190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "dir2='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "path=dir2+'../Functions/'\n",
    "sys.path.append(path)\n",
    "\n",
    "import NumericalFunctions\n",
    "from NumericalFunctions import * # import NumericalFunctions \n",
    "import PlottingFunctions\n",
    "from PlottingFunctions import * # import PlottingFunctions\n",
    "\n",
    "\n",
    "# # Get all functions in NumericalFunctions\n",
    "# import inspect\n",
    "# functions = [f[0] for f in inspect.getmembers(NumericalFunctions, inspect.isfunction)]\n",
    "# functions\n",
    "\n",
    "# # Get all functions in NumericalFunctions\n",
    "# import inspect\n",
    "# functions = [f[0] for f in inspect.getmembers(PlottingFunctions, inspect.isfunction)]\n",
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69609f71-c16b-4c20-acee-c4d5075c2add",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "#SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c67f2a-6543-49e4-a112-8c2d45fc4fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "#JOB ARRAY SETUP\n",
    "################################\n",
    "#*#*\n",
    "# how many total jobs are being run? i.e. array=1-100 ==> num_jobs=100\n",
    "if Np_str=='1e6':\n",
    "    num_jobs=60 #1M parcels\n",
    "    num_slurm_jobs=10\n",
    "elif Np_str=='50e6':\n",
    "    num_jobs=1200 #50M parcels\n",
    "    num_slurm_jobs=150\n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17544be7-9a5b-4431-ab34-c0f781f67ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "#DATA LOADING FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd4360b-5993-4c36-87bf-4e5d08d07c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INITIALIZE DATA FUNCTION\n",
    "###############################################################\n",
    "def InitiateArray(out_file, vars, t_chunk_size, p_chunk_size, t_size=None, p_size=None):\n",
    "    if t_size is None:\n",
    "        t_size = len(data1['time'])  # Number of timesteps\n",
    "    if p_size is None:\n",
    "        p_size = len(parcel1['xh'])  # Number of vertical levels\n",
    "\n",
    "    with h5py.File(out_file, 'w') as f:\n",
    "        for var_name in vars:\n",
    "            if var_name not in f:\n",
    "                # Set dtype conditionally\n",
    "                if var_name in ['Z', 'Y', 'X']:\n",
    "                    dtype = np.uint16\n",
    "                elif var_name in ['A_g','A_c','A_g_Processed','A_c_Processed']:\n",
    "                    dtype = np.bool_\n",
    "                else:\n",
    "                    dtype = np.float32  # or whatever your default is\n",
    "\n",
    "                f.create_dataset(\n",
    "                    var_name,\n",
    "                    shape=(t_size, p_size),\n",
    "                    chunks=(t_chunk_size, p_chunk_size),\n",
    "                    dtype=dtype\n",
    "                )\n",
    "def InitiateArray_Job(parcel,out_file, vars, t_chunk_size, p_chunk_size, t_size=None, p_size=None):\n",
    "    if t_size is None:\n",
    "        t_size = len(data1['time'])  # Number of timesteps\n",
    "    if p_size is None:\n",
    "        p_size = len(parcel['xh'])  # Number of vertical levels\n",
    "\n",
    "    with h5py.File(out_file, 'w') as f:\n",
    "        for var_name in vars:\n",
    "            if var_name not in f:\n",
    "                # Set dtype conditionally\n",
    "                if var_name in ['Z', 'Y', 'X']:\n",
    "                    dtype = np.uint16\n",
    "                elif var_name in ['A_g','A_c','A_g_Processed','A_c_Processed']:\n",
    "                    dtype = np.bool_\n",
    "                else:\n",
    "                    dtype = np.float32  # or whatever your default is\n",
    "\n",
    "                f.create_dataset(\n",
    "                    var_name,\n",
    "                    shape=(t_size, p_size),\n",
    "                    chunks=(t_chunk_size, p_chunk_size),\n",
    "                    dtype=dtype\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dff7c96-ed40-48b1-abb4-b017f6d84561",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JOB ARRAY SETUP\n",
    "def StartJobArray(num_jobs,job_id):\n",
    "    total_elements=len(parcel1['xh']) #total num of variables\n",
    "    \n",
    "    if num_jobs >= total_elements:\n",
    "        raise ValueError(\"Number of jobs cannot be greater than or equal to total elements.\")\n",
    "    \n",
    "    job_range = total_elements // num_jobs  # Base size for each chunk\n",
    "    remaining = total_elements % num_jobs   # Number of chunks with 1 extra \n",
    "    \n",
    "    # Function to compute the start and end for each job_id\n",
    "    def get_job_range(job_id):\n",
    "        job_id-=1\n",
    "        # Add one extra element to the first 'remaining' chunks\n",
    "        start_job = job_id * job_range + min(job_id, remaining)\n",
    "        end_job = start_job + job_range + (1 if job_id < remaining else 0)\n",
    "    \n",
    "        if job_id == num_jobs - 1: \n",
    "            end_job = total_elements #- 1\n",
    "        return start_job, end_job\n",
    "    # def job_testing():\n",
    "    #     #TESTING\n",
    "    #     start=[];end=[]\n",
    "    #     for job_id in range(1,num_jobs+1):\n",
    "    #         start_job, end_job = get_job_range(job_id)\n",
    "    #         print(start_job,end_job)\n",
    "    #         start.append(start_job)\n",
    "    #         end.append(end_job)\n",
    "    #     print(np.all(start!=end))\n",
    "    #     print(len(np.unique(start))==len(start))\n",
    "    #     print(len(np.unique(end))==len(end))\n",
    "    # job_testing()\n",
    "    \n",
    "    # job_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0)) #this is the current SBATCH job id\n",
    "    # if job_id==0: job_id=1\n",
    "    start_job, end_job = get_job_range(job_id)\n",
    "    # print(f'\\nstart_job = {start_job}, end_job = {end_job}')\n",
    "\n",
    "    index_adjust=start_job\n",
    "    return start_job,end_job,index_adjust\n",
    "# num_jobs=1000; job_id=1\n",
    "# [start_job,end_job,index_adjust] = StartJobArray(num_jobs,job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2376c6a0-87b9-4b10-a025-440f6f2daab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Back Data Later\n",
    "##############\n",
    "def make_data_dict(in_file,var_names,read_type,start_job,end_job):\n",
    "    if read_type=='h5py':\n",
    "        with h5py.File(in_file, 'r') as f:\n",
    "            data_dict = {var_name: f[var_name][:,start_job:end_job] for var_name in var_names}\n",
    "            \n",
    "    elif read_type=='xarray':\n",
    "        in_data = xr.open_dataset(\n",
    "            in_file,\n",
    "            engine='h5netcdf',\n",
    "            phony_dims='sort',\n",
    "            chunks={'phony_dim_0': 100, 'phony_dim_1': 1_000_000} \n",
    "        )\n",
    "        data_dict = {k: in_data[k][:,start_job:end_job].compute().data for k in var_names}\n",
    "    return data_dict\n",
    "\n",
    "# read_type='xarray'\n",
    "read_type='h5py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f6be76-90b3-4a05-94af-b163cedc638f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetArrays(start_job,end_job):\n",
    "    import h5py\n",
    "    dir2=dir+'Project_Algorithms/Lagrangian_Arrays/OUTPUT/'\n",
    "    in_file=dir2+f'lagrangian_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "    \n",
    "    var_names = ['A_g', 'A_c']\n",
    "    data_dict = make_data_dict(in_file,var_names,read_type,start_job,end_job)\n",
    "    A_g, A_c = (data_dict[k] for k in var_names)\n",
    "    \n",
    "    # # #Making Time Matrix\n",
    "    # Nt=len(data['time'])\n",
    "    # T = np.broadcast_to(np.arange(Nt)[:, None], A_c.shape)  # shape: (Nt, p)\n",
    "    \n",
    "    # check_memory(globals())\n",
    "    return A_g, A_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5fb09b-3e09-427c-8432-e76cba791479",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4247af-e710-4460-b363-11ea09477d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "#FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279c30c2-0351-4a31-a8a4-98cced1b8f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_idxs(f,case):\n",
    "    out=np.sort(np.add.outer(f, np.arange(case)).ravel())\n",
    "\n",
    "    # #OLD METHOD (SLOW)\n",
    "    # if np.any(f)==True:\n",
    "    #     out=np.sort(np.concatenate([np.arange(idx, idx + case-1+1) for idx in f]))\n",
    "    # else: \n",
    "    #     out=f\n",
    "    return out\n",
    "\n",
    "def find_sandwiched_patterns(changes, case):\n",
    "    arr=changes\n",
    "    \n",
    "    window_size = case + 1  # e.g., for case=2, window_size = 3\n",
    "    # The interior zeros count is (window_size - 2) which is case - 1\n",
    "    pattern1 = np.array([-1] + [0]*(case - 1) + [1])\n",
    "    pattern2 = np.array([1] + [0]*(case - 1) + [-1])\n",
    "    # print(pattern1,pattern2)\n",
    "    \n",
    "    # Manually construct sliding windows\n",
    "    windows = np.array([arr[i:i + window_size] for i in range(len(arr) - window_size + 1)])\n",
    "    # print(\"Sliding windows:\\n\", windows) #TESTING\n",
    "    \n",
    "    #THE ALGORITHM\n",
    "    turb_d=[]\n",
    "    turb_e=[]\n",
    "    count=0;max_iter=len(data1['time']);\n",
    "    while np.any(((windows == pattern1) | (windows == pattern2)).all(axis=1)):\n",
    "        count+=1; \n",
    "        if count>=max_iter: \n",
    "            print(count)\n",
    "            break\n",
    "        \n",
    "        next_ind = np.where(((windows == pattern1) | (windows == pattern2)).all(axis=1))[0][0]\n",
    "        \n",
    "        if (windows[next_ind] == pattern1).all():\n",
    "            turb_d.append(next_ind)\n",
    "        elif (windows[next_ind] == pattern2).all(): \n",
    "            turb_e.append(next_ind) #append to list\n",
    "    \n",
    "        windows[0:next_ind+(case)+1,:] = 0 #removes from windows\n",
    "    \n",
    "    turb_d=np.array(turb_d,dtype=int); turb_e=np.array(turb_e,dtype=int)\n",
    "\n",
    "    #EXTEND REST OF INDEXES TO PROCESS\n",
    "    turb_d=extend_idxs(turb_d,case=case)\n",
    "    turb_e=extend_idxs(turb_e,case=case)\n",
    "    return turb_d,turb_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106dbc65-0e40-4a4b-b0ff-01497659bea4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# TESTING\n",
    "# changes = np.array([0,0,0,-1,1,0,0,-1,0,0,0,1,-1,0,0])\n",
    "# [a,b] = find_sandwiched_patterns(changes, case=1) #<=1 in a row timesteps are removed\n",
    "# print(\"Case matches at indices:\", a,b)\n",
    "\n",
    "# changes = np.array([0,0,0,-1,0,1,0,0,-1,0,0,1,0,-1,0,0])\n",
    "# [a,b] = find_sandwiched_patterns(changes, case=2) #<=2 in a row timesteps are removed\n",
    "# print(\"Case matches at indices:\", a,b)\n",
    "\n",
    "# changes = np.array([0,0,0,-1,0,0,1,0,0,0,0,1,0,0,-1,0,0])\n",
    "# [a,b] = find_sandwiched_patterns(changes, case=3) #<=3 in a row timesteps are removed\n",
    "# print(\"Case matches at indices:\", a,b)\n",
    "\n",
    "# changes = np.array([0,0,0,-1,0,0,0,1,0,0,0,0,1,0,0,-1,0,0])\n",
    "# [a,b] = find_sandwiched_patterns(changes, case=4) #<=4 in a row timesteps are removed\n",
    "# print(\"Case matches at indices:\", a,b)\n",
    "\n",
    "# changes = np.array([0,0,0,-1,0,0,0,0,1,0,0,0,0,1,0,0,-1,0,0])\n",
    "# [a,b] = find_sandwiched_patterns(changes, case=5) #<=5 in a row timesteps are removed\n",
    "# print(\"Case matches at indices:\", a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f56dd61-519d-454f-a230-1a592b3f789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### (amount of time inside/outside of cloud to count as entrainment/detrainment)\n",
    "mins_thresh=5 #5 mins\n",
    "######\n",
    "\n",
    "t_per_mins=1/((data1['time'][1]-data1['time'][0])/1e9/60).item() #timesteps per minute (<=1)\n",
    "def get_changes(B):\n",
    "    changes = np.diff(np.concatenate(([B[0]], B)))  # Add 0s to detect edges\n",
    "    return changes\n",
    "def PreProcessing(A,p):\n",
    "    B = A[:,p] #UNCOMMENT WHEN NOT TESTING***\n",
    "\n",
    "    # Find the changes in the array\n",
    "    changes=get_changes(B)\n",
    "    # print(f'B = {B}'); print(f'changes = {changes}') \n",
    "\n",
    "    #Determining the Case Number\n",
    "    case=int(t_per_mins*mins_thresh) #UNCOMMENT WHEN NOT TESTING***\n",
    "    \n",
    "    if case>1:\n",
    "        for case_ind in np.arange(case,0,-1): \n",
    "        # for case_ind in [case]:\n",
    "            #Calling Algorithm and Correcting Parcel Data\n",
    "            [turb_d,turb_e]=find_sandwiched_patterns(changes, case=case_ind)\n",
    "            B[turb_d]=1\n",
    "            B[turb_e]=0     \n",
    "            changes=get_changes(B)\n",
    "            # print(B)\n",
    "    elif case==1:\n",
    "        #Calling Algorithm and Correcting Parcel Data\n",
    "        [turb_d,turb_e]=find_sandwiched_patterns(changes, case=case)\n",
    "        B[turb_d]=1\n",
    "        B[turb_e]=0\n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7d17ea-0807-4abb-bb7a-75014d2e6739",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUNNING\n",
    "def Apply(A_g,A_c):\n",
    "    A_g_Processed = A_g.copy()\n",
    "    A_c_Processed = A_c.copy()\n",
    "    for p in np.arange(Np):\n",
    "        if np.mod(p,1e3)==0: print(f\"{p}/{len(parcel['xh'])}\")\n",
    "        out1=PreProcessing(A_g_Processed,p); A_g_Processed[:,p]=out1\n",
    "        out2=PreProcessing(A_c_Processed,p); A_c_Processed[:,p]=out2\n",
    "    return A_g_Processed,A_c_Processed\n",
    "\n",
    "#SAVING\n",
    "def Save(parcel,A_g_Processed,A_c_Processed,job_id):\n",
    "    dir2=dir+'Project_Algorithms/Entrainment/Processing_Out/'\n",
    "    out_file=dir2+f'processed_binary_arrays_{res}_{t_res}_{Np_str}_{job_id}.h5'\n",
    "    \n",
    "    vars=['A_g_Processed','A_c_Processed']\n",
    "    InitiateArray_Job(parcel,out_file,vars,t_chunk_size=50,p_chunk_size=1000)\n",
    "    \n",
    "    with h5py.File(out_file, 'a') as f: \n",
    "        f['A_g_Processed'][:]=A_g_Processed\n",
    "        f['A_c_Processed'][:]=A_c_Processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c3de60-1c01-4245-a255-d0380e3dfc1d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #EXAMPLE TESTING #CASE COUNTDOWN\n",
    "# B=np.array([1,1,1,1,0,0,0,1,1,1,1,0,0,0,1,0,0,1,1,1,0,0,0,1])\n",
    "#                     #,#,#,        #,#,#       #,#,#\n",
    "# print(B)\n",
    "\n",
    "# #APPLYING\n",
    "# changes=get_changes(B)\n",
    "# [turb_d,turb_e]=find_sandwiched_patterns(changes,case=3)\n",
    "# print(turb_d,turb_e)\n",
    "\n",
    "# B[turb_d]=1\n",
    "# B[turb_e]=0\n",
    "# print(B)\n",
    "\n",
    "# #APPLYING\n",
    "# changes=get_changes(B)\n",
    "# [turb_d,turb_e]=find_sandwiched_patterns(changes,case=2)\n",
    "# print(turb_d,turb_e)\n",
    "\n",
    "# B[turb_d]=1\n",
    "# B[turb_e]=0\n",
    "# print(B)\n",
    "\n",
    "# #APPLYING\n",
    "# changes=get_changes(B)\n",
    "# [turb_d,turb_e]=find_sandwiched_patterns(changes,case=1)\n",
    "# print(turb_d,turb_e)\n",
    "\n",
    "# B[turb_d]=1\n",
    "# B[turb_e]=0\n",
    "# print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c5f8a2-0ec1-427c-9b42-edf2799f457a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # TESTING\n",
    "# # Case 1\n",
    "# case=1\n",
    "# B=np.array([1,0,1,1,0,0,1,0])\n",
    "# B=np.array([1,0,1,1,0,1,0]) \n",
    "# B=np.array([1,0,1,1,0,1,0,1])\n",
    "\n",
    "# B=np.array([1,0,1,0,1,1,1])\n",
    "# B=np.array([1,0,1,0,1,0,0])\n",
    "# B=np.array([0,1,0,1,0,0,0]) \n",
    "# B=np.array([0,1,0,1,0,1,1])\n",
    "\n",
    "# # Case 2\n",
    "# case=2\n",
    "# B=np.array([1,1,1,0,0,1,1,1,0,0,0,1,1,0,0,0])\n",
    "# B=np.array([1,0,0,1,0,0,1,0,0,0,1,0,1,0,0,0]) #101 should still get removed\n",
    "\n",
    "# # Case 3\n",
    "# case=3\n",
    "# B=np.array([1,1,1,1,0,0,0,1,1,1,1,0,0,0,1,0,0,0,0,1])\n",
    "# print(f'output =  {B}\\n')\n",
    "# p=1234; out=PreProcessing(p,updraft_type='cloudy') #TESTING\n",
    "# print(f'output =  {out}\\n')\n",
    "\n",
    "# # Case 5\n",
    "# case=5\n",
    "# B=np.array([1,1,1,1,0,0,0,0,0,1,1,1,1,0,0,0,1,0,0,0,0,1])\n",
    "# print(f'output =  {B}\\n')\n",
    "# p=1234; out=PreProcessing(p,updraft_type='cloudy') #TESTING\n",
    "# print(f'output =  {out}\\n')\n",
    "\n",
    "# # REAL CASE\n",
    "# count_per_row = (A_c >= 1).sum(axis=0)\n",
    "# where=np.where(count_per_row > 10)[0] # Find row indices where count is greater than 10\n",
    "# # print(where)\n",
    "# ind=12345; p=where[ind]; print(p) \n",
    "\n",
    "# print(A_c[:,p])\n",
    "# out=PreProcessing(p,updraft_type='cloudy') #TESTING\n",
    "# print(f'output =  {out}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fada64-99ef-4cb1-9de0-da3e990cbbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "#RUNNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7e4774-d1fa-40b1-878b-c1a9878375ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "########################################\n",
    "#RUNNING\n",
    "[start_slurm_job,end_slurm_job]=StartSlurmJobArray(num_jobs=num_jobs,num_slurm_jobs=num_slurm_jobs,ISRUN=True) #if ISRUN is False, then will not run using slurm_job_array\n",
    "\n",
    "print(f\"Running on Slurm_Jobs for Slurm_Job_Ids: {(start_slurm_job,end_slurm_job-1)}\")\n",
    "\n",
    "job_id_list=np.arange(start_slurm_job,end_slurm_job)\n",
    "for job_id in job_id_list:\n",
    "    if job_id % 1 ==0: print(f\"current job_id = {job_id}\\n\")\n",
    "    [start_job,end_job,index_adjust]=StartJobArray(num_jobs,job_id)\n",
    "    print(f'Running on Parcels {start_job}-{end_job}')\n",
    "    parcel=parcel1.isel(xh=slice(start_job,end_job)); Np=len(parcel['xh'])\n",
    "    [A_g,A_c]=GetArrays(start_job,end_job)\n",
    "    [A_g_Processed,A_c_Processed]=Apply(A_g,A_c)\n",
    "    Save(parcel,A_g_Processed,A_c_Processed,job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b382dfc-afff-4635-b869-32fd260db663",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7f87ec-d5b8-48b8-8a46-591df5982b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMBINING JOB_ARRAYS AFTER RUNNING\n",
    "########################################################################\n",
    "recombine=False #KEEP FALSE WHEN JOB ARRAY IS RUNNING\n",
    "recombine=True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08bfbca-e2a0-4ade-a64d-32a4887855cf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def Recombine(num_jobs):\n",
    "#     dir2=dir+'Project_Algorithms/Entrainment/Processing_Out/'\n",
    "#     dir3=dir+'Project_Algorithms/Entrainment/OUTPUT/'\n",
    "#     out_file=dir3+f'processed_binary_arrays_{res}_{t_res}_{Np_str}.h5'\n",
    "    \n",
    "#     vars=['A_g_Processed','A_c_Processed']\n",
    "#     InitiateArray(out_file,vars,t_chunk_size=50,p_chunk_size=100_000)\n",
    "    \n",
    "#     with h5py.File(out_file, 'r+') as f_out:\n",
    "        \n",
    "#         for job_id in np.arange(1,num_jobs+1):\n",
    "#             if np.mod(job_id,5)==0: print(f\"job_id = {job_id}\")\n",
    "#             [a,b,_] = StartJobArray(num_jobs,job_id)\n",
    "        \n",
    "#             in_file=dir2+f'processed_binary_arrays_{res}_{t_res}_{Np_str}_{job_id}.h5'\n",
    "    \n",
    "#             with h5py.File(in_file, 'r') as f_in: \n",
    "#                 for var in vars:\n",
    "#                     f_out[var][:,a:b]=f_in[var][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beda1d51-af31-41d6-934a-c15f4034746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recombine_Dask(): #*#*\n",
    "    #DASK-ENABLED\n",
    "    import glob\n",
    "    import re\n",
    "    import dask\n",
    "    from dask.diagnostics import ProgressBar\n",
    "    \n",
    "    def recombine_func(in_files,out_file):\n",
    "        # matching_files = sorted(glob.glob(in_files))\n",
    "        matching_files = sorted(\n",
    "        glob.glob(in_files),\n",
    "        key=lambda f: int(re.search(r'_(\\d+)\\.h5$', f).group(1))\n",
    "    )\n",
    "        \n",
    "        print(f'recombining {len(matching_files)} files')\n",
    "        print('Forming Dask Delayed Object')\n",
    "        # out=xr.open_mfdataset(matching_files,engine='h5netcdf',concat_dim='phony_dim_1',combine='nested',phony_dims='sort')\n",
    "        if Np_str == '1e6':\n",
    "            dim_1_length = 100_000\n",
    "        elif Np_str == '50e6':\n",
    "            dim_1_length = 500_000\n",
    "        chunks={'phony_dim_0': -1, 'phony_dim_1':dim_1_length}\n",
    "        out = xr.open_mfdataset(\n",
    "            matching_files,\n",
    "            engine='h5netcdf',\n",
    "            concat_dim='phony_dim_1',\n",
    "            combine='nested',\n",
    "            phony_dims='sort',\n",
    "            chunks=chunks,\n",
    "            parallel=True\n",
    "        ) \n",
    "        print('RUNNING')\n",
    "        with ProgressBar():\n",
    "            delayed_write=out.to_netcdf(out_file, engine='h5netcdf', compute=False)\n",
    "            delayed_write.compute()\n",
    "\n",
    "    ####################################################################################\n",
    "    dir2=dir+'Project_Algorithms/Entrainment/Processing_Out/'\n",
    "    dir3=dir+'Project_Algorithms/Entrainment/OUTPUT/'\n",
    "\n",
    "    in_files=dir2+f'processed_binary_arrays_{res}_{t_res}_{Np_str}_*.h5'\n",
    "    out_file=dir3+f'processed_binary_arrays_{res}_{t_res}_{Np_str}.h5'\n",
    "    recombine_func(in_files,out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e476e7e-f39f-46b5-bf38-a2d83fde7852",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if recombine==True:\n",
    "    # Recombine(num_jobs=num_jobs)\n",
    "    Recombine_Dask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cdde4f-77a3-49d7-9c20-1ab882f7405b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968ccc3f-35db-4088-8447-402df430e500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f830d8-1e10-4336-b26d-c871e2760d44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46476abf-3d03-42de-b0c7-238ba1cff1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING\n",
    "############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0474c6b2-7395-48df-926a-b99520df0019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TESTING RECOMBINE_ORIGINAL vs RECOMBINE_DASK\n",
    "# import xarray as xr\n",
    "\n",
    "# # Path to the output file you just saved\n",
    "# out_file1 = dir + f'Project_Algorithms/Entrainment/OUTPUT/processed_binary_arrays_{res}_{t_res}_{Np_str}.h5'\n",
    "# out_file2 = dir + f'Project_Algorithms/Entrainment/OUTPUT/processed_binary_arrays_{res}_{t_res}_{Np_str}_TEST.h5'\n",
    "\n",
    "# # Read it in using xarray\n",
    "# ds1=xr.open_dataset(out_file1, phony_dims='sort',engine='h5netcdf')\n",
    "# ds2 = xr.open_dataset(out_file2, engine='h5netcdf')\n",
    "# one=ds1['A_c_Processed'].isel(phony_dim_1=slice(1000, 2000)).data\n",
    "# two=ds2['A_c_Processed'].isel(phony_dim_1=slice(1000, 2000)).data\n",
    "# np.all(one==two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dc0608-5eb4-4f18-8492-00c7d04827aa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #TESTING FOR EQUALITY BETWEEN A_G and A_C\n",
    "# t=9\n",
    "# p=50650806\n",
    "# start_job=p;end_job=p+1\n",
    "# parcel=parcel1.isel(xh=slice(start_job,end_job)); Np=len(parcel['xh'])\n",
    "# [A_g,A_c]=GetArrays(start_job,end_job)\n",
    "# [A_g_Processed,A_c_Processed]=Apply(A_g,A_c)\n",
    "\n",
    "# print( np.where((A_g==A_c)&(A_c!=0)) )\n",
    "# print( A_g[508]==A_c[508] )\n",
    "\n",
    "# print( A_g[508] )\n",
    "# print( A_c[508] )\n",
    "# print( A_g_Processed[508] )\n",
    "# print( A_c_Processed[508] )\n",
    "\n",
    "# #WHY IS BOTH A_g and A_c changed from 0 to 1 #ISSUE CASE IS 508\n",
    "# print('A_g')\n",
    "# print( A_g[500:515].T ) \n",
    "# print('A_g_Processed')\n",
    "# print( A_g_Processed[500:515].T ) #==> LEAVES GENERAL UPDRAFT FOR LESS THAN 5 mins, so all 0s are turned to 1s\n",
    "# print('\\n')\n",
    "# print('A_c')\n",
    "# print( A_c[500:515].T )\n",
    "# print('A_c_Processed')\n",
    "# print( A_c_Processed[500:515].T ) #==> LEAVES CLOUDY UPDRAFT FOR LESS THAN 5 mins, so all 0s are turned to 1s\n",
    "\n",
    "# #PRINT SAVE\n",
    "# # A_g\n",
    "# # [[0 0 0 0 0 0 1 0 0 0 1 0 0 0 0]]\n",
    "# # A_g_Processed\n",
    "# # [[0 0 0 0 0 0 1 1 1 1 1 0 0 0 0]]\n",
    "\n",
    "\n",
    "# # A_c\n",
    "# # [[1 1 1 1 1 1 0 0 0 1 0 1 1 1 1]]\n",
    "# # A_c_Processed\n",
    "# # [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n",
    "\n",
    "# #CONCLUSION: \n",
    "# #Parcel moves between General and Cloudy Updrafts under 5 minutes, so for both Thresholds, it will count as Turbulent Entrainment and fill in as both in General and Cloudy Updraft, this will not effect final entrainment and detrainment values, but will affect the results of Updraft-Transfer Entrainment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36399544-c3ec-41b6-99ac-41ce92fa971a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #WALL TIME TESTING\n",
    "# TESTING\n",
    "# 27s/jobs = x/100_000 jobs\n",
    "# (100_000*27/10)/60**2 = 75 hours (QUITE LONG)\n",
    "\n",
    "# 30s/1job = x/10_000jobs\n",
    "# (10_000*30/1)/60**2 = 83 hours (QUITE LONG)\n",
    "\n",
    "# (5*60)s/1job = x/1000jobs\n",
    "# (10_000*(5*60)/1)/60**2 = 830 hours (QUITE LONG)\n",
    "# ==> if we then split into 100 slurm jobs ==> 8 hours "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f2824f-b8da-44f1-930c-04b4a5dbdcbd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # ###COMPARING SINGLE JOB RUN (COMPARING TO VERSION1)\n",
    "# # #SINGLE JOB\n",
    "# # dir2=dir+'Project_Algorithms/Entrainment/job_out_3/'\n",
    "# # in_file=dir2+f'processed_binary_arrays_{res}_{t_res}_{Np_str}_{job_id}.h5'\n",
    "\n",
    "# # with h5py.File(in_file, 'r') as f_in: \n",
    "# #     out=f_in['A_c_Processed'][:]\n",
    "# # np.where(out!=A_c_Processed)[0]\n",
    "\n",
    "# #ALL JOBS\n",
    "# a=0;b=10000\n",
    "\n",
    "# dir2=dir+'Project_Algorithms/Entrainment/'\n",
    "# in_file=dir2+f'processed_binary_arrays_{res}_{t_res}_{Np_str}.h5'\n",
    "# with h5py.File(in_file, 'r') as f_in: \n",
    "#     out1=f_in['A_c_Processed'][a:b]\n",
    "    \n",
    "# dir2=dir+'Project_Algorithms/Entrainment/'\n",
    "# in_file=dir2+f'processed_binary_arrays_{res}_{t_res}_{Np_str}_TEST.h5'\n",
    "# with h5py.File(in_file, 'r') as f_in: \n",
    "#     out2=f_in['A_c_Processed'][a:b]\n",
    "# np.where(out1!=out2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efc7746-ff8b-4979-9d73-2a2d6aabbbba",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# start_job=0;end_job=10000\n",
    "# # Reading Back Data Later\n",
    "# ##############\n",
    "# def make_data_dict(in_file,var_names,read_type):\n",
    "#     if read_type=='h5py':\n",
    "#         with h5py.File(in_file, 'r') as f:\n",
    "#             data_dict = {var_name: f[var_name][:,start_job:end_job] for var_name in var_names}\n",
    "            \n",
    "#     elif read_type=='xarray':\n",
    "#         in_data = xr.open_dataset(\n",
    "#             in_file,\n",
    "#             engine='h5netcdf',\n",
    "#             phony_dims='sort',\n",
    "#             chunks={'phony_dim_0': 100, 'phony_dim_1': 1_000_000} \n",
    "#         )\n",
    "#         data_dict = {k: in_data[k][:,start_job:end_job].compute().data for k in var_names}\n",
    "#     return data_dict\n",
    "\n",
    "# # read_type='xarray'\n",
    "# read_type='h5py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4ad837-5fd0-4ac7-ae93-ed9a51de1620",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import h5py\n",
    "# dir2=dir+'Project_Algorithms/Lagrangian_Arrays/'\n",
    "# in_file=dir2+f'lagrangian_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "\n",
    "# var_names = ['A_g', 'A_c']\n",
    "# data_dict = make_data_dict(in_file,var_names,read_type)\n",
    "# [A_g, A_c] = (data_dict[k] for k in var_names) #, W\n",
    "\n",
    "# check_memory(globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390b386e-47c9-4faa-b2ca-c70bdd292969",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #READING BACK IN\n",
    "# dir2=dir+'Project_Algorithms/Entrainment/'\n",
    "# in_file=dir2+f'processed_binary_arrays_{res}_{t_res}_{Np_str}.h5'\n",
    "\n",
    "# var_names = ['A_g_Processed', 'A_c_Processed']\n",
    "# data_dict = make_data_dict(in_file,var_names,read_type)\n",
    "# A_g_Processed, A_c_Processed = (data_dict[k] for k in var_names)\n",
    "# check_memory(globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565e1c04-32e4-4605-9493-31a7eef08e82",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ind=290 #time index\n",
    "# where=np.where(A_c[ind]!=A_c_Processed[ind])\n",
    "# where #which parcels have differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd71caf3-753e-420b-b903-c6bc1c343562",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ind2=0 #choose one of those parcels\n",
    "# # ind2+=1\n",
    "# where2=where[0][ind2]\n",
    "# print(A_c[:,where2])\n",
    "# print(A_c_Processed[:,where2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08a2bf0-9ebb-4277-a89d-4b70d2d27879",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #PLUGGING INTO ALGORITHM TO SEE WHERE THINGS WENT WRONG\n",
    "# B=A_c[:,where2]\n",
    "# print(B)\n",
    "# PreProcessing(p=where2,updraft_type='cloudy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2157d4e3-79a0-47d3-a1fe-82fd14fc123e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def extend_idxs(f,case):\n",
    "#     out=np.sort(np.add.outer(f, np.arange(case)).ravel())\n",
    "\n",
    "#     # #OLD METHOD (SLOW)\n",
    "#     # if np.any(f)==True:\n",
    "#     #     out=np.sort(np.concatenate([np.arange(idx, idx + case-1+1) for idx in f]))\n",
    "#     # else: \n",
    "#     #     out=f\n",
    "#     return out\n",
    "\n",
    "# def find_sandwiched_patterns(changes, case):\n",
    "#     arr=changes\n",
    "    \n",
    "#     window_size = case + 1  # e.g., for case=2, window_size = 3\n",
    "#     # The interior zeros count is (window_size - 2) which is case - 1\n",
    "#     pattern1 = np.array([-1] + [0]*(case - 1) + [1])\n",
    "#     pattern2 = np.array([1] + [0]*(case - 1) + [-1])\n",
    "#     # print(pattern1,pattern2)\n",
    "    \n",
    "#     # Manually construct sliding windows\n",
    "#     windows = np.array([arr[i:i + window_size] for i in range(len(arr) - window_size + 1)])\n",
    "#     # print(\"Sliding windows:\\n\", windows) #TESTING\n",
    "    \n",
    "#     #THE ALGORITHM\n",
    "#     turb_d=[]\n",
    "#     turb_e=[]\n",
    "#     count=0;max_iter=len(data['time']);\n",
    "#     while np.any(((windows == pattern1) | (windows == pattern2)).all(axis=1)):\n",
    "#         count+=1; \n",
    "#         if count>=max_iter: \n",
    "#             print(count)\n",
    "#             break\n",
    "        \n",
    "#         next_ind = np.where(((windows == pattern1) | (windows == pattern2)).all(axis=1))[0][0]\n",
    "        \n",
    "#         if (windows[next_ind] == pattern1).all():\n",
    "#             turb_d.append(next_ind)\n",
    "#         elif (windows[next_ind] == pattern2).all(): \n",
    "#             turb_e.append(next_ind) #append to list\n",
    "    \n",
    "#         windows[0:next_ind+(case)+1,:] = 0 #removes from windows\n",
    "    \n",
    "#     turb_d=np.array(turb_d,dtype=int); turb_e=np.array(turb_e,dtype=int)\n",
    "\n",
    "#     #EXTEND REST OF INDEXES TO PROCESS\n",
    "#     turb_d=extend_idxs(turb_d,case=case)\n",
    "#     turb_e=extend_idxs(turb_e,case=case)\n",
    "#     return turb_d,turb_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7222237-9b8f-4fd7-b02b-48474f252db7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ###### (amount of time inside/outside of cloud to count as entrainment/detrainment)\n",
    "# mins_thresh=5 #5 mins\n",
    "# ######\n",
    "\n",
    "# def get_changes(B):\n",
    "#     changes = np.diff(np.concatenate(([B[0]], B)))  # Add 0s to detect edges\n",
    "#     return changes\n",
    "# def PreProcessing(p,updraft_type):\n",
    "\n",
    "#     if updraft_type=='general':\n",
    "#         A=A_g.copy()\n",
    "#     elif updraft_type=='cloudy':\n",
    "#         A=A_c.copy()\n",
    "#     # B = A[:,p] #UNCOMMENT WHEN NOT TESTING***\n",
    "\n",
    "#     # Find the changes in the array\n",
    "#     changes=get_changes(B)\n",
    "#     # print(f'B = {B}'); print(f'changes = {changes}') \n",
    "\n",
    "#     #Determining the Case Number\n",
    "#     t_per_mins=1/((data['time'][1]-data['time'][0])/1e9/60).item() #timesteps per minute (<=1)\n",
    "#     case=int(t_per_mins*mins_thresh) #UNCOMMENT WHEN NOT TESTING***\n",
    "    \n",
    "#     if case>1:\n",
    "#         for case_ind in np.arange(case,0,-1): \n",
    "#         # for case_ind in [case]:\n",
    "#             #Calling Algorithm and Correcting Parcel Data\n",
    "#             [turb_d,turb_e]=find_sandwiched_patterns(changes, case=case_ind)\n",
    "#             B[turb_d]=1\n",
    "#             B[turb_e]=0     \n",
    "#             changes=get_changes(B)\n",
    "#             # print(B)\n",
    "#     elif case==1:\n",
    "#         #Calling Algorithm and Correcting Parcel Data\n",
    "#         [turb_d,turb_e]=find_sandwiched_patterns(changes, case=case)\n",
    "#         B[turb_d]=1\n",
    "#         B[turb_e]=0\n",
    "#     return B"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
