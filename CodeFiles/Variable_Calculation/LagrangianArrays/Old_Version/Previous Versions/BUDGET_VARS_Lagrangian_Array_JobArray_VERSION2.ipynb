{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fae0de0-de5b-44b8-b03b-3363e3f59fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in Packages and Data\n",
    "\n",
    "#Importing Packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import matplotlib.gridspec as gridspec\n",
    "import xarray as xr\n",
    "import os; import time\n",
    "import pickle\n",
    "import h5py\n",
    "###############################################################\n",
    "def coefs(coefficients,degree):\n",
    "    coef=coefficients\n",
    "    coefs=\"\"\n",
    "    for n in range(degree, -1, -1):\n",
    "        string=f\"({coefficients[len(coef)-(n+1)]:.1e})\"\n",
    "        coefs+=string + f\"x^{n}\"\n",
    "        if n != 0:\n",
    "            coefs+=\" + \"\n",
    "    return coefs\n",
    "###############################################################\n",
    "\n",
    "#Importing Model Data\n",
    "check=False\n",
    "dir='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "\n",
    "# # dx = 1 km; Np = 1M; Nt = 5 min\n",
    "# data=xr.open_dataset(dir+'../cm1r20.3/run/cm1out_1km_5min.nc') #***\n",
    "# parcel=xr.open_dataset(dir+'../cm1r20.3/run/cm1out_pdata_1km_5min_1e6.nc') #***\n",
    "# t_res='5min'; res='1km'\n",
    "# res='1km'\n",
    "# Np_str='1e6'\n",
    "\n",
    "# dx = 1km; Np = 50M\n",
    "#Importing Model Data\n",
    "check=False\n",
    "dir2='/home/air673/koa_scratch/'\n",
    "data=xr.open_dataset(dir2+'cm1out_1km_1min.nc') #***\n",
    "parcel=xr.open_dataset(dir2+'cm1out_pdata_1km_1min_50M.nc') #***\n",
    "res='1km'; t_res='1min'\n",
    "Np_str='50e6'\n",
    "\n",
    "\n",
    "# # dx = 250 m\n",
    "# #Importing Model Data\n",
    "# check=False\n",
    "# dir2='/home/air673/koa_scratch/'\n",
    "# data=xr.open_dataset(dir2+'cm1out_250m.nc') #***\n",
    "# parcel=xr.open_dataset(dir2+'cm1out_pdata_250m.nc') #***\n",
    "# res='250m'\n",
    "# Np_str='150e6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6160ed5f-703e-4a0c-8491-1845b80c2eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INITIALIZE DATA FUNCTION\n",
    "###############################################################\n",
    "def initiate_array(out_file, vars, t_chunk_size, p_chunk_size, t_size=None, p_size=None):\n",
    "    if t_size is None:\n",
    "        t_size = len(data['time'])  # Number of timesteps\n",
    "    if p_size is None:\n",
    "        p_size = len(parcel['xh'])  # Number of vertical levels\n",
    "\n",
    "    with h5py.File(out_file, 'w') as f:\n",
    "        for var_name in vars:\n",
    "            if var_name not in f:\n",
    "                # Set dtype conditionally\n",
    "                if var_name in ['Z', 'Y', 'X']:\n",
    "                    dtype = np.uint16\n",
    "                elif var_name in ['A_g', 'A_c']:\n",
    "                    dtype = np.bool_\n",
    "                else:\n",
    "                    dtype = np.float32  # or whatever your default is\n",
    "\n",
    "                f.create_dataset(\n",
    "                    var_name,\n",
    "                    shape=(t_size, p_size),\n",
    "                    chunks=(t_chunk_size, p_chunk_size),\n",
    "                    dtype=dtype\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de4d0e7-9b8b-4d3a-97e4-e5f6fb7084b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_memory():\n",
    "    import sys\n",
    "    ipython_vars = [\"In\", \"Out\", \"exit\", \"quit\", \"get_ipython\", \"ipython_vars\"]\n",
    "    print(\"Top 10 objects with highest memory usage\")\n",
    "    # Get a sorted list of the objects and their sizes\n",
    "    mem = {\n",
    "        key: round(value/1e6,2)\n",
    "        for key, value in sorted(\n",
    "            [\n",
    "                (x, sys.getsizeof(globals().get(x)))\n",
    "                for x in globals()\n",
    "                if not x.startswith(\"_\") and x not in sys.modules and x not in ipython_vars\n",
    "            ],\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True)[:10]\n",
    "    }\n",
    "    print({key:f\"{value} MB\" for key,value in mem.items()})\n",
    "    print(f\"\\n{round(sum(mem.values()),2)/1000} GB in use overall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f8c729-c6af-46c3-a0fe-ee6289ba9bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JOB ARRAY SETUP\n",
    "job_array=False\n",
    "job_array=True\n",
    "\n",
    "if job_array==True:\n",
    "\n",
    "    num_jobs=300 #how many total jobs are being run? i.e. array=1-100 ==> num_jobs=100 #***\n",
    "    total_elements=len(data['time']) #total num of variables\n",
    "\n",
    "    if num_jobs >= total_elements:\n",
    "        raise ValueError(\"Number of jobs cannot be greater than or equal to total elements.\")\n",
    "    \n",
    "    job_range = total_elements // num_jobs  # Base size for each chunk\n",
    "    remaining = total_elements % num_jobs   # Number of chunks with 1 extra \n",
    "    \n",
    "    # Function to compute the start and end for each job_id\n",
    "    def get_job_range(job_id, num_jobs):\n",
    "        job_id-=1\n",
    "        # Add one extra element to the first 'remaining' chunks\n",
    "        start_job = job_id * job_range + min(job_id, remaining)\n",
    "        end_job = start_job + job_range + (1 if job_id < remaining else 0)\n",
    "    \n",
    "        if job_id == num_jobs - 1: \n",
    "            end_job = total_elements #- 1\n",
    "        return start_job, end_job\n",
    "    # def job_testing():\n",
    "    #     #TESTING\n",
    "    #     start=[];end=[]\n",
    "    #     for job_id in range(1,num_jobs+1):\n",
    "    #         start_job, end_job = get_job_range(job_id)\n",
    "    #         print(start_job,end_job)\n",
    "    #         start.append(start_job)\n",
    "    #         end.append(end_job)\n",
    "    #     print(np.all(start!=end))\n",
    "    #     print(len(np.unique(start))==len(start))\n",
    "    #     print(len(np.unique(end))==len(end))\n",
    "    # job_testing()\n",
    "    \n",
    "    job_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0)) #this is the current SBATCH job id\n",
    "    if job_id==0: job_id=1\n",
    "    start_job, end_job = get_job_range(job_id, num_jobs)\n",
    "    index_adjust=start_job\n",
    "    print(f'start_job = {start_job}, end_job = {end_job}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50e63ad-e286-4607-a1cd-bbc2372f7c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "if job_array==True:\n",
    "    #Indexing Array with JobArray\n",
    "    data=data.isel(time=slice(start_job,end_job))\n",
    "    parcel=parcel.isel(time=slice(start_job,end_job))\n",
    "    #(for 150_000_000 parcels use 500-1000 jobs)\n",
    "\n",
    "if job_array==False:\n",
    "    start_job=0;end_job=len(data['time']);index_adjust=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d858fb01-270d-4440-8d96-69f4cdd96b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADING VARIABLES\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baa935c-d4ae-4df9-b524-e847be9854c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Back Data Later\n",
    "##############\n",
    "import h5py\n",
    "dir2=dir+'Project_Algorithms/Lagrangian_Arrays/'\n",
    "open_file=dir2+f'lagrangian_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "with h5py.File(open_file, 'r') as f:\n",
    "    Z = f['Z'][start_job:end_job]\n",
    "    Y = f['Y'][start_job:end_job]\n",
    "    X = f['X'][start_job:end_job]\n",
    "\n",
    "# #Making Time Matrix\n",
    "# rows, cols = A.shape[0], A.shape[1]\n",
    "# T = np.arange(rows).reshape(-1, 1) * np.ones((1, cols), dtype=int)\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bab5ef8-9068-491e-9b68-87014c61e7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_variable(varname):\n",
    "    if varname=='th_e':\n",
    "        with h5py.File(dir + 'Variable_Calculation/' + 'theta_e'+f'_{res}_{t_res}_{Np_str}'+'.h5', 'r') as f:\n",
    "            var_data = f['theta_e'][start_job:end_job]\n",
    "    else:\n",
    "        var_data=data[varname].data\n",
    "    return var_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864c2c87-092a-46c0-8e21-f08aee78ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lagrangian_array(varnames):\n",
    "    # Initialize dictionaries\n",
    "    var_data_dict = {varname: call_variable(varname) for varname in varnames}\n",
    "    VAR = {varname: np.zeros_like(Z, dtype='float32') for varname in varnames}\n",
    "\n",
    "    Nt = len(data['time'])\n",
    "    Np = len(parcel['xh'])\n",
    "    for p in np.arange(Np):\n",
    "        if np.mod(p, 1e6) == 0: \n",
    "            print(f\"{p}/{len(parcel['xh'])}\")\n",
    "\n",
    "        # Get Indices\n",
    "        zs = Z[:, p]\n",
    "        ys = Y[:, p]\n",
    "        xs = X[:, p]\n",
    "        ts = np.arange(Nt)\n",
    "\n",
    "        # Loop over all variables and fill the respective VAR array\n",
    "        for varname, var_data in var_data_dict.items():\n",
    "            VAR[varname][:, p] = var_data[ts, zs, ys, xs]\n",
    "\n",
    "    # Return all the arrays in a list\n",
    "    return [VAR[varname] for varname in varnames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69a41a0-786e-45ef-8597-289f94a3273c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#W BUDGET VARIABLES\n",
    "#########################################\n",
    "varnames=['wb_hadv','wb_vadv','wb_hidiff','wb_vidiff',\n",
    "          'wb_hturb','wb_vturb','wb_pgrad','wb_buoy']\n",
    "[WB_HADV,WB_VADV,WB_HIDIFF,WB_VIDIFF,WB_HTURB,WB_VTURB,WB_PGRAD,WB_BUOY]=make_lagrangian_array(varnames); check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fbb08c-c9c5-4416-a99f-d3c7cc165a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Data\n",
    "##############\n",
    "print('Saving Data\\n')\n",
    "import h5py\n",
    "dir2=dir+'Project_Algorithms/Lagrangian_Arrays/job_out/'\n",
    "out_file=dir2+f'W_BUDGET_VARS_binary_array_{res}_{t_res}_{Np_str}'\n",
    "if job_array==True:\n",
    "    out_file+=f'_{job_id}.h5'\n",
    "elif job_array==False:\n",
    "    out_file+=f'.h5'\n",
    "\n",
    "vars=['WB_HADV','WB_VADV','WB_HIDIFF','WB_VIDIFF','WB_HTURB','WB_VTURB','WB_PGRAD','WB_BUOY']\n",
    "initiate_array(out_file,vars,t_chunk_size=1,p_chunk_size=100_000)\n",
    "\n",
    "with h5py.File(out_file, 'a') as f: \n",
    "    f['WB_HADV'][:]=WB_HADV\n",
    "    f['WB_VADV'][:]=WB_VADV\n",
    "    f['WB_HIDIFF'][:]=WB_HIDIFF\n",
    "    f['WB_VIDIFF'][:]=WB_VIDIFF\n",
    "    f['WB_HTURB'][:]=WB_HTURB\n",
    "    f['WB_VTURB'][:]=WB_VTURB\n",
    "    f['WB_PGRAD'][:]=WB_PGRAD\n",
    "    f['WB_BUOY'][:]=WB_BUOY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fec4908-e1a9-47d2-8f34-9fcafde9367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#QV BUDGET VARIABLES\n",
    "#########################################\n",
    "varnames=['qvb_hadv','qvb_vadv','qvb_hidiff','qvb_vidiff',\n",
    "          'qvb_hturb','qvb_vturb','qvb_mp']\n",
    "[QVB_HADV,QVB_VADV,QVB_HIDIFF,QVB_VIDIFF,QVB_HTURB,QVB_VTURB,QVB_MP]=make_lagrangian_array(varnames); check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5255e0-7e19-4372-8558-58f21621ba2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Data\n",
    "##############\n",
    "print('Saving Data\\n')\n",
    "import h5py\n",
    "dir2=dir+'Project_Algorithms/Lagrangian_Arrays/job_out/'\n",
    "out_file=dir2+f'QV_BUDGET_VARS_binary_array_{res}_{t_res}_{Np_str}'\n",
    "if job_array==True:\n",
    "    out_file+=f'_{job_id}.h5'\n",
    "elif job_array==False:\n",
    "    out_file+=f'.h5'\n",
    "\n",
    "vars=['QVB_HADV','QVB_VADV','QVB_HIDIFF','QVB_VIDIFF','QVB_HTURB','QVB_VTURB','QVB_MP']\n",
    "initiate_array(out_file,vars,t_chunk_size=1,p_chunk_size=100_000)\n",
    "\n",
    "with h5py.File(out_file, 'a') as f: \n",
    "    f['QVB_HADV'][:]=QVB_HADV\n",
    "    f['QVB_VADV'][:]=QVB_VADV\n",
    "    f['QVB_HIDIFF'][:]=QVB_HIDIFF\n",
    "    f['QVB_VIDIFF'][:]=QVB_VIDIFF\n",
    "    f['QVB_HTURB'][:]=QVB_HTURB\n",
    "    f['QVB_VTURB'][:]=QVB_VTURB\n",
    "    f['QVB_MP'][:]=QVB_MP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36baaf58-1bf0-4f36-8830-253a29188e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TH BUDGET VARIABLES\n",
    "#########################################\n",
    "varnames=['ptb_hadv','ptb_vadv','ptb_hidiff','ptb_vidiff',\n",
    "          'ptb_hturb','ptb_vturb','ptb_mp','ptb_rad','ptb_div','ptb_diss']\n",
    "[PTB_HADV,PTB_VADV,PTB_HIDIFF,PTB_VIDIFF,PTB_HTURB,PTB_VTURB,PTB_MP,PTB_RAD,PTB_DIV,PTB_DISS]=make_lagrangian_array(varnames); check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca35c742-0d3b-4e95-a6af-9cb8b6cf1000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Data\n",
    "##############\n",
    "print('Saving Data\\n')\n",
    "import h5py\n",
    "dir2=dir+'Project_Algorithms/Lagrangian_Arrays/job_out/'\n",
    "out_file=dir2+f'TH_BUDGET_VARS_binary_array_{res}_{t_res}_{Np_str}'\n",
    "if job_array==True:\n",
    "    out_file+=f'_{job_id}.h5'\n",
    "elif job_array==False:\n",
    "    out_file+=f'.h5'\n",
    "\n",
    "vars=['PTB_HADV','PTB_VADV','PTB_HIDIFF','PTB_VIDIFF','PTB_HTURB','PTB_VTURB','PTB_MP',\n",
    "      'PTB_RAD','PTB_DIV','PTB_DISS']\n",
    "initiate_array(out_file,vars,t_chunk_size=1,p_chunk_size=100_000)\n",
    "\n",
    "with h5py.File(out_file, 'a') as f: \n",
    "    f['PTB_HADV'][:]=PTB_HADV\n",
    "    f['PTB_VADV'][:]=PTB_VADV\n",
    "    f['PTB_HIDIFF'][:]=PTB_HIDIFF\n",
    "    f['PTB_VIDIFF'][:]=PTB_VIDIFF\n",
    "    f['PTB_HTURB'][:]=PTB_HTURB\n",
    "    f['PTB_VTURB'][:]=PTB_VTURB\n",
    "    f['PTB_MP'][:]=PTB_MP\n",
    "    f['PTB_RAD'][:]=PTB_RAD\n",
    "    f['PTB_DIV'][:]=PTB_DIV\n",
    "    f['PTB_DISS'][:]=PTB_DISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca09f28-74a4-4369-8323-0196faf71db8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e0bcf9-ff7b-44a2-803d-336153aac975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "#RECOMBINE SEPERATE JOB_ARRAYS AFTER\n",
    "recombine=False #KEEP FALSE WHEN JOB ARRAY IS RUNNING\n",
    "# recombine=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8d98ba-d52a-48bc-ada9-12c782f7251c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# if recombine==True:\n",
    "#     dir2=dir+'Project_Algorithms/Lagrangian_Arrays/job_out/'\n",
    "#     dir3=dir+'Project_Algorithms/Lagrangian_Arrays/'\n",
    "#     out_file=dir3+f'W_BUDGET_VARS_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "    \n",
    "#     vars=['W_HADV','W_VADV','WB_HIDIFF','WB_VIDIFF','WB_HTURB','WB_VTURB','WB_PGRAD','WB_BUOY']\n",
    "#     initiate_array(out_file,vars,t_chunk_size=100,p_chunk_size=100_000)\n",
    "    \n",
    "#     with h5py.File(out_file, 'r+') as f_out:\n",
    "#         num_jobs=300\n",
    "#         for job_id in np.arange(1,num_jobs+1):\n",
    "#             if np.mod(job_id,5)==0: print(f\"job_id = {job_id}\")\n",
    "#             [a,b] = get_job_range(job_id,num_jobs)\n",
    "            \n",
    "#             in_file=dir2+f'W_BUDGET_VARS_binary_array_{res}_{t_res}_{Np_str}_{job_id}.h5'\n",
    "#             with h5py.File(in_file, 'r') as f_in: \n",
    "#                 for var in vars:\n",
    "#                     f_out[var][a:b]=f_in[var][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bf3733-f4bd-4c94-9076-850feca523f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DASK-ENABLED\n",
    "def recombine_func(in_files,out_file):\n",
    "    matching_files = sorted(glob.glob(in_files))\n",
    "    print('recombining')\n",
    "    # %%time\n",
    "    out=xr.open_mfdataset(matching_files,engine='h5netcdf',concat_dim='phony_dim_0',combine='nested',phony_dims='sort')\n",
    "    out.to_netcdf(out_file, engine='h5netcdf')\n",
    "    \n",
    "if recombine==True:\n",
    "    import glob\n",
    "    dir2=dir+'Project_Algorithms/Lagrangian_Arrays/job_out/'\n",
    "    dir3=dir+'Project_Algorithms/Lagrangian_Arrays/'\n",
    "    in_files = dir2 + f'W_BUDGET_VARS_binary_array_{res}_{t_res}_{Np_str}_*.h5'\n",
    "    out_file=dir3+f'W_BUDGET_VARS_binary_array_{res}_{t_res}_{Np_str}.h5' \n",
    "    \n",
    "    recombine_func(in_files,out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bc8181-0d8f-4981-8d34-d915a006a7b2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# if recombine==True:\n",
    "#     dir2=dir+'Project_Algorithms/Lagrangian_Arrays/job_out/'\n",
    "#     dir3=dir+'Project_Algorithms/Lagrangian_Arrays/'\n",
    "#     # out_file=dir3+f'QV_BUDGET_VARS_binary_array_{res}_{Np_str}_5min.h5'\n",
    "#     out_file=dir3+f'QV_BUDGET_VARS_binary_array_{res}_{Np_str}_1min.h5'\n",
    "    \n",
    "#     vars=['QVB_HADV','QVB_VADV','QVB_HIDIFF','QVB_VIDIFF','QVB_HTURB','QVB_VTURB','QVB_MP']\n",
    "#     initiate_array(out_file,vars,t_chunk_size=100,p_chunk_size=100_000)\n",
    "    \n",
    "#     with h5py.File(out_file, 'r+') as f_out:\n",
    "#         num_jobs=300\n",
    "#         for job_id in np.arange(1,num_jobs+1):\n",
    "#             if np.mod(job_id,5)==0: print(f\"job_id = {job_id}\")\n",
    "#             [a,b] = get_job_range(job_id,num_jobs)\n",
    "            \n",
    "#             # in_file=dir2+f'QV_BUDGET_VARS_binary_array_{res}_{Np_str}_5min_{job_id}.h5'\n",
    "#             in_file=dir2+f'QV_BUDGET_VARS_binary_array_{res}_{Np_str}_1min_{job_id}.h5'\n",
    "#             with h5py.File(in_file, 'r') as f_in: \n",
    "#                 for var in vars:\n",
    "#                     f_out[var][a:b]=f_in[var][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16d6c0e-6449-4572-a479-8dd3b615cc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DASK-ENABLED\n",
    "def recombine_func(in_files,out_file):\n",
    "    matching_files = sorted(glob.glob(in_files))\n",
    "    print('recombining')\n",
    "    # %%time\n",
    "    out=xr.open_mfdataset(matching_files,engine='h5netcdf',concat_dim='phony_dim_0',combine='nested',phony_dims='sort')\n",
    "    out.to_netcdf(out_file, engine='h5netcdf')\n",
    "    \n",
    "if recombine==True:\n",
    "    import glob\n",
    "    dir2=dir+'Project_Algorithms/Lagrangian_Arrays/job_out/'\n",
    "    dir3=dir+'Project_Algorithms/Lagrangian_Arrays/'\n",
    "    in_files = dir2 + f'QV_BUDGET_VARS_binary_array_{res}_{t_res}_{Np_str}_*.h5'\n",
    "    out_file=dir3+f'QV_BUDGET_VARS_binary_array_{res}_{t_res}_{Np_str}.h5' \n",
    "    \n",
    "    recombine_func(in_files,out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e31da4-3665-47fa-b1cb-d718ef65e56f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# if recombine==True:\n",
    "#     dir2=dir+'Project_Algorithms/Lagrangian_Arrays/job_out/'\n",
    "#     dir3=dir+'Project_Algorithms/Lagrangian_Arrays/'\n",
    "#     # out_file=dir3+f'TH_BUDGET_VARS_binary_array_{res}_{Np_str}_5min.h5'\n",
    "#     out_file=dir3+f'TH_BUDGET_VARS_binary_array_{res}_{Np_str}_1min.h5'\n",
    "    \n",
    "#     vars=['PTB_HADV','PTB_VADV','PTB_HIDIFF','PTB_VIDIFF','PTB_HTURB','PTB_VTURB','PTB_MP',\n",
    "#           'PTB_RAD','PTB_DIV','PTB_DISS']\n",
    "#     initiate_array(out_file,vars,t_chunk_size=100,p_chunk_size=100_000)\n",
    "    \n",
    "#     with h5py.File(out_file, 'r+') as f_out:\n",
    "#         num_jobs=300\n",
    "#         for job_id in np.arange(1,num_jobs+1):\n",
    "#             if np.mod(job_id,5)==0: print(f\"job_id = {job_id}\")\n",
    "#             [a,b] = get_job_range(job_id,num_jobs)\n",
    "            \n",
    "#             # in_file=dir2+f'TH_BUDGET_VARS_binary_array_{res}_{Np_str}_5min_{job_id}.h5'\n",
    "#             in_file=dir2+f'TH_BUDGET_VARS_binary_array_{res}_{Np_str}_1min_{job_id}.h5'\n",
    "#             with h5py.File(in_file, 'r') as f_in: \n",
    "#                 for var in vars:\n",
    "#                     f_out[var][a:b]=f_in[var][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1af3a68-478e-452d-9634-0fad64147ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DASK-ENABLED\n",
    "def recombine_func(in_files,out_file):\n",
    "    matching_files = sorted(glob.glob(in_files))\n",
    "    print('recombining')\n",
    "    # %%time\n",
    "    out=xr.open_mfdataset(matching_files,engine='h5netcdf',concat_dim='phony_dim_0',combine='nested',phony_dims='sort')\n",
    "    out.to_netcdf(out_file, engine='h5netcdf')\n",
    "    \n",
    "if recombine==True:\n",
    "    import glob\n",
    "    dir2=dir+'Project_Algorithms/Lagrangian_Arrays/job_out/'\n",
    "    dir3=dir+'Project_Algorithms/Lagrangian_Arrays/'\n",
    "    in_files = dir2 + f'TH_BUDGET_VARS_binary_array_{res}_{t_res}_{Np_str}_*.h5'\n",
    "    out_file=dir3+f'TH_BUDGET_VARS_binary_array_{res}_{t_res}_{Np_str}.h5' \n",
    "    \n",
    "    recombine_func(in_files,out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f9397c-4b54-48bf-9a6f-f17a64bbad57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "#READING BACK IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba170fe5-80c1-4cbf-8684-1789cab7edcb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import h5py\n",
    "# # File path to the saved data\n",
    "# dir2 = dir + 'Project_Algorithms/Lagrangian_Arrays/'\n",
    "# filename = f'W_BUDGET_VARS_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "\n",
    "# # Open the HDF5 file and read the datasets\n",
    "# with h5py.File(dir2 + filename, 'r') as f:\n",
    "#     WB_HADV = f['WB_HADV'][:]\n",
    "#     WB_VADV = f['WB_VADV'][:]\n",
    "#     WB_HIDIFF = f['WB_HIDIFF'][:]\n",
    "#     WB_VIDIFF = f['WB_VIDIFF'][:]\n",
    "#     WB_HTURB = f['WB_HTURB'][:]\n",
    "#     WB_VTURB = f['WB_VTURB'][:]\n",
    "#     WB_PGRAD = f['WB_PGRAD'][:]\n",
    "#     WB_BUOY = f['WB_BUOY'][:]\n",
    "\n",
    "# # Check memory after loading\n",
    "# check_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069a20a6-7d3f-4034-a835-ec37ba5bb116",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import h5py\n",
    "# # File path to the saved data\n",
    "# dir2 = dir + 'Project_Algorithms/Lagrangian_Arrays/'\n",
    "# filename = f'QV_BUDGET_VARS_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "\n",
    "# # Open the HDF5 file and read the datasets\n",
    "# with h5py.File(dir2 + filename, 'r') as f:\n",
    "#     QVB_HADV = f['QVB_HADV'][:]\n",
    "#     QVB_VADV = f['QVB_VADV'][:]\n",
    "#     QVB_HIDIFF = f['QVB_HIDIFF'][:]\n",
    "#     QVB_VIDIFF = f['QVB_VIDIFF'][:]\n",
    "#     QVB_HTURB = f['QVB_HTURB'][:]\n",
    "#     QVB_VTURB = f['QVB_VTURB'][:]\n",
    "#     QVB_MP = f['QVB_MP'][:]\n",
    "\n",
    "# # Check memory after loading\n",
    "# check_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062cf290-b932-4e7b-bdbb-c6bf99c49612",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import h5py\n",
    "# # File path to the saved data\n",
    "# dir2 = dir + 'Project_Algorithms/Lagrangian_Arrays/'\n",
    "# filename = f'TH_BUDGET_VARS_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "\n",
    "# # Open the HDF5 file and read the datasets\n",
    "# with h5py.File(dir2 + filename, 'r') as f:\n",
    "#     PTB_HADV = f['PTB_HADV'][:]\n",
    "#     PTB_VADV = f['PTB_VADV'][:]\n",
    "#     PTB_HIDIFF = f['PTB_HIDIFF'][:]\n",
    "#     PTB_VIDIFF = f['PTB_VIDIFF'][:]\n",
    "#     PTB_HTURB = f['PTB_HTURB'][:]\n",
    "#     PTB_VTURB = f['PTB_VTURB'][:]\n",
    "#     PTB_MP = f['PTB_MP'][:]\n",
    "#     PTB_RAD = f['PTB_RAD'][:]\n",
    "#     PTB_DIV = f['PTB_DIV'][:]\n",
    "#     PTB_DISS = f['PTB_DISS'][:]\n",
    "\n",
    "# # Check memory after loading\n",
    "# check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc7fd89-a7f4-4a28-a4aa-c55dc940d8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "#TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e425f2-427b-4b97-9748-1f28d7143f4f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# t=100\n",
    "# dir2 = dir + 'Project_Algorithms/Lagrangian_Arrays/'\n",
    "# filename = f'W_BUDGET_VARS_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "# with h5py.File(dir2 + filename, 'r') as f:\n",
    "#     WB_HADV = f['WB_HADV'][t]\n",
    "#     WB_VADV = f['WB_VADV'][t]\n",
    "#     WB_HIDIFF = f['WB_HIDIFF'][t]\n",
    "#     WB_VIDIFF = f['WB_VIDIFF'][t]\n",
    "#     WB_HTURB = f['WB_HTURB'][t]\n",
    "#     WB_VTURB = f['WB_VTURB'][t]\n",
    "#     WB_PGRAD = f['WB_PGRAD'][t]\n",
    "#     WB_BUOY = f['WB_BUOY'][t]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91abc7dc-6b2f-4645-af00-8644e196cfdd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# dir2 = dir + 'Project_Algorithms/Lagrangian_Arrays/'\n",
    "# filename = f'QV_BUDGET_VARS_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "# with h5py.File(dir2 + filename, 'r') as f:\n",
    "#     QVB_HADV = f['QVB_HADV'][t]\n",
    "#     QVB_VADV = f['QVB_VADV'][t]\n",
    "#     QVB_HIDIFF = f['QVB_HIDIFF'][t]\n",
    "#     QVB_VIDIFF = f['QVB_VIDIFF'][t]\n",
    "#     QVB_HTURB = f['QVB_HTURB'][t]\n",
    "#     QVB_VTURB = f['QVB_VTURB'][t]\n",
    "#     QVB_MP = f['QVB_MP'][t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a82295f-700b-437a-b282-60ba43cdcbe6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import h5py\n",
    "# # File path to the saved data\n",
    "# dir2 = dir + 'Project_Algorithms/Lagrangian_Arrays/'\n",
    "# filename = f'TH_BUDGET_VARS_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "\n",
    "# # Open the HDF5 file and read the datasets\n",
    "# with h5py.File(dir2 + filename, 'r') as f:\n",
    "#     PTB_HADV = f['PTB_HADV'][t]\n",
    "#     PTB_VADV = f['PTB_VADV'][t]\n",
    "#     PTB_HIDIFF = f['PTB_HIDIFF'][t]\n",
    "#     PTB_VIDIFF = f['PTB_VIDIFF'][t]\n",
    "#     PTB_HTURB = f['PTB_HTURB'][t]\n",
    "#     PTB_VTURB = f['PTB_VTURB'][t]\n",
    "#     PTB_MP = f['PTB_MP'][t]\n",
    "#     PTB_RAD = f['PTB_RAD'][t]\n",
    "#     PTB_DIV = f['PTB_DIV'][t]\n",
    "#     PTB_DISS = f['PTB_DISS'][t]\n",
    "\n",
    "# # Check memory after loading\n",
    "# check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a837edc5-1cc6-4210-92ba-2fcea9085a03",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# dir2=dir+'Project_Algorithms/Lagrangian_Arrays/'\n",
    "# with h5py.File(dir2+f'lagrangian_binary_array_{res}_{t_res}_{Np_str}.h5', 'r') as f:\n",
    "#     # Load the dataset by its name\n",
    "#     Z = f['Z'][t]\n",
    "#     Y = f['Y'][t]\n",
    "#     X = f['X'][t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f10cff-fe28-4f80-a50d-9fbd32804f03",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# t,p=100,1000\n",
    "# def test(t,p,VAR,var):\n",
    "#     z=Z[p];y=Y[p];x=X[p]\n",
    "#     out=data[var].isel(time=t,zh=z,yh=y,xh=x).data\n",
    "#     print(VAR[p],out)\n",
    "# def test2(t,p,VAR,var):\n",
    "#     z=Z[p];y=Y[p];x=X[p]\n",
    "#     out=data[var].isel(time=t,zf=z,yh=y,xh=x).data\n",
    "#     print(VAR[p],out)\n",
    "\n",
    "# test2(t,p,WB_HADV,'wb_hadv')\n",
    "# test(t,p,QVB_HADV,'qvb_hadv')\n",
    "# test(t,p,PTB_HADV,'ptb_hadv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
