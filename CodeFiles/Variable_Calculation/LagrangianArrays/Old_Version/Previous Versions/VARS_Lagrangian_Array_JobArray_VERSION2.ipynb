{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fae0de0-de5b-44b8-b03b-3363e3f59fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in Packages and Data\n",
    "\n",
    "#Importing Packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import matplotlib.gridspec as gridspec\n",
    "import xarray as xr\n",
    "import os; import time\n",
    "import pickle\n",
    "import h5py\n",
    "###############################################################\n",
    "def coefs(coefficients,degree):\n",
    "    coef=coefficients\n",
    "    coefs=\"\"\n",
    "    for n in range(degree, -1, -1):\n",
    "        string=f\"({coefficients[len(coef)-(n+1)]:.1e})\"\n",
    "        coefs+=string + f\"x^{n}\"\n",
    "        if n != 0:\n",
    "            coefs+=\" + \"\n",
    "    return coefs\n",
    "###############################################################\n",
    "\n",
    "#Importing Model Data\n",
    "check=False\n",
    "dir='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "\n",
    "# dx = 1 km; Np = 1M; Nt = 5 min\n",
    "data=xr.open_dataset(dir+'../cm1r20.3/run/cm1out_1km_5min.nc') #***\n",
    "parcel=xr.open_dataset(dir+'../cm1r20.3/run/cm1out_pdata_1km_5min_1e6.nc') #***\n",
    "t_res='5min'; res='1km'; Np_str='1e6'\n",
    "\n",
    "# # dx = 1km; Np = 50M\n",
    "# #Importing Model Data\n",
    "# check=False\n",
    "# dir2='/home/air673/koa_scratch/'\n",
    "# data=xr.open_dataset(dir2+'cm1out_1km_1min.nc') #***\n",
    "# parcel=xr.open_dataset(dir2+'cm1out_pdata_1km_1min_50M.nc') #***\n",
    "# res='1km'; t_res='1min'; Np_str='50e6'\n",
    "\n",
    "# # dx = 1km; Np = 100M\n",
    "# #Importing Model Data\n",
    "# check=False\n",
    "# dir2='/home/air673/koa_scratch/'\n",
    "# data=xr.open_dataset(dir2+'cm1out_1km_1min.nc') #***\n",
    "# parcel=xr.open_dataset(dir2+'cm1out_pdata_1km_1min_100M.nc') #***\n",
    "# res='1km'; t_res='1min'; Np_str='100e6'\n",
    "\n",
    "# # dx = 250 m\n",
    "# #Importing Model Data\n",
    "# check=False\n",
    "# dir2='/home/air673/koa_scratch/'\n",
    "# data=xr.open_dataset(dir2+'cm1out_250m.nc') #***\n",
    "# parcel=xr.open_dataset(dir2+'cm1out_pdata_250m.nc') #***\n",
    "# res='250m'\n",
    "# Np_str='150e6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364765b0-34e1-4e03-bfb5-10ed9a3a9c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INITIALIZE DATA FUNCTION\n",
    "###############################################################\n",
    "def initiate_array(out_file, vars, t_chunk_size, p_chunk_size, t_size=None, p_size=None):\n",
    "    if t_size is None:\n",
    "        t_size = len(data['time'])  # Number of timesteps\n",
    "    if p_size is None:\n",
    "        p_size = len(parcel['xh'])  # Number of vertical levels\n",
    "\n",
    "    with h5py.File(out_file, 'w') as f:\n",
    "        for var_name in vars:\n",
    "            if var_name not in f:\n",
    "                # Set dtype conditionally\n",
    "                if var_name in ['Z', 'Y', 'X']:\n",
    "                    dtype = np.uint16\n",
    "                elif var_name in ['A_g', 'A_c']:\n",
    "                    dtype = np.bool_\n",
    "                else:\n",
    "                    dtype = np.float32  # or whatever your default is\n",
    "\n",
    "                f.create_dataset(\n",
    "                    var_name,\n",
    "                    shape=(t_size, p_size),\n",
    "                    chunks=(t_chunk_size, p_chunk_size),\n",
    "                    dtype=dtype\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de4d0e7-9b8b-4d3a-97e4-e5f6fb7084b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_memory():\n",
    "    import sys\n",
    "    ipython_vars = [\"In\", \"Out\", \"exit\", \"quit\", \"get_ipython\", \"ipython_vars\"]\n",
    "    print(\"Top 10 objects with highest memory usage\")\n",
    "    # Get a sorted list of the objects and their sizes\n",
    "    mem = {\n",
    "        key: round(value/1e6,2)\n",
    "        for key, value in sorted(\n",
    "            [\n",
    "                (x, sys.getsizeof(globals().get(x)))\n",
    "                for x in globals()\n",
    "                if not x.startswith(\"_\") and x not in sys.modules and x not in ipython_vars\n",
    "            ],\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True)[:10]\n",
    "    }\n",
    "    print({key:f\"{value} MB\" for key,value in mem.items()})\n",
    "    print(f\"\\n{round(sum(mem.values()),2)/1000} GB in use overall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa113eb6-cc20-42d4-b17e-dc88fded285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JOB ARRAY SETUP\n",
    "job_array=False\n",
    "job_array=True\n",
    "\n",
    "if job_array==True:\n",
    "\n",
    "    num_jobs=3 #120 #how many total jobs are being run? i.e. array=1-100 ==> num_jobs=100 #***\n",
    "    total_elements=len(data['time']) #total num of variables\n",
    "\n",
    "    if num_jobs >= total_elements:\n",
    "        raise ValueError(\"Number of jobs cannot be greater than or equal to total elements.\")\n",
    "\n",
    "    \n",
    "    job_range = total_elements // num_jobs  # Base size for each chunk\n",
    "    remaining = total_elements % num_jobs   # Number of chunks with 1 extra \n",
    "    \n",
    "    # Function to compute the start and end for each job_id\n",
    "    def get_job_range(job_id, num_jobs):\n",
    "        job_id-=1\n",
    "        # Add one extra element to the first 'remaining' chunks\n",
    "        start_job = job_id * job_range + min(job_id, remaining)\n",
    "        end_job = start_job + job_range + (1 if job_id < remaining else 0)\n",
    "    \n",
    "        if job_id == num_jobs - 1: \n",
    "            end_job = total_elements #- 1\n",
    "        return start_job, end_job\n",
    "    # def job_testing():\n",
    "    #     #TESTING\n",
    "    #     start=[];end=[]\n",
    "    #     for job_id in range(1,num_jobs+1):\n",
    "    #         start_job, end_job = get_job_range(job_id, num_jobs)\n",
    "    #         print(start_job,end_job)\n",
    "    #         start.append(start_job)\n",
    "    #         end.append(end_job)\n",
    "    #     print(np.all(start!=end))\n",
    "    #     print(len(np.unique(start))==len(start))\n",
    "    #     print(len(np.unique(end))==len(end))\n",
    "    # job_testing()\n",
    "    \n",
    "    job_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0)) #this is the current SBATCH job id\n",
    "    if job_id==0: job_id=3\n",
    "    start_job, end_job = get_job_range(job_id, num_jobs)\n",
    "    index_adjust=start_job\n",
    "    print(f'start_job = {start_job}, end_job = {end_job}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6aebf7-aa9c-469d-9a45-712d5ff63ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if job_array==True:\n",
    "    #Indexing Array with JobArray\n",
    "    data=data.isel(time=slice(start_job,end_job))\n",
    "    parcel=parcel.isel(time=slice(start_job,end_job))\n",
    "    #(for 150_000_000 parcels use 500-1000 jobs)\n",
    "\n",
    "if job_array==False:\n",
    "    start_job=0;end_job=len(data['time']);index_adjust=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eec8fbe-59ca-42aa-b662-99de04a0d7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d858fb01-270d-4440-8d96-69f4cdd96b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADING VARIABLES\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baa935c-d4ae-4df9-b524-e847be9854c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Back Data Later\n",
    "##############\n",
    "import h5py\n",
    "dir2=dir+'Project_Algorithms/Lagrangian_Arrays/'\n",
    "open_file=dir2+f'lagrangian_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "with h5py.File(open_file, 'r') as f:\n",
    "    Z = f['Z'][start_job:end_job]\n",
    "    Y = f['Y'][start_job:end_job]\n",
    "    X = f['X'][start_job:end_job]\n",
    "\n",
    "# #Making Time Matrix\n",
    "# rows, cols = A.shape[0], A.shape[1]\n",
    "# T = np.arange(rows).reshape(-1, 1) * np.ones((1, cols), dtype=int)\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bab5ef8-9068-491e-9b68-87014c61e7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_variable(varname):\n",
    "    if varname=='th_e':\n",
    "        with h5py.File(dir + 'Variable_Calculation/' + 'theta_e'+f'_{res}_{t_res}'+'.h5', 'r') as f:\n",
    "            var_data = f['theta_e'][start_job:end_job]\n",
    "    elif varname=='HMC':\n",
    "        dir2='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "        file_path = dir2 + 'Variable_Calculation/' + '2D_Moisture_Convergence' + f'_{res}_{t_res}' + '.h5'\n",
    "        with h5py.File(file_path, 'r') as f:\n",
    "            var_data = f['conv'][start_job:end_job]\n",
    "    elif varname=='VMF_c':\n",
    "        dir2 = '/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "        dir3 = dir2 + 'Project_Algorithms/Entrainment/'\n",
    "        with h5py.File(dir3 + '3D_Eulerian_VMF'+f'_{res}_{t_res}'+'.h5', 'r') as f:\n",
    "            var_data = f['VMF_c'][start_job:end_job]\n",
    "    elif varname=='VMF_g':\n",
    "        dir2 = '/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "        dir3 = dir2 + 'Project_Algorithms/Entrainment/'\n",
    "        with h5py.File(dir3 + '3D_Eulerian_VMF'+f'_{res}_{t_res}'+'.h5', 'r') as f:\n",
    "            var_data = f['VMF_g'][start_job:end_job]\n",
    "    else:\n",
    "        var_data=data[varname].data\n",
    "    return var_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864c2c87-092a-46c0-8e21-f08aee78ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lagrangian_array(varnames):\n",
    "    # Initialize dictionaries\n",
    "    var_data_dict = {varname: call_variable(varname) for varname in varnames}\n",
    "    VAR = {varname: np.zeros_like(Z, dtype='float32') for varname in varnames}\n",
    "\n",
    "    Nt = len(data['time'])\n",
    "    Np = len(parcel['xh'])\n",
    "    for p in np.arange(Np):\n",
    "\n",
    "        if np.mod(p, 5e4) == 0: \n",
    "            print(f\"{p}/{len(parcel['xh'])}\")\n",
    "\n",
    "        # Get Indices\n",
    "        zs = Z[:, p]\n",
    "        ys = Y[:, p]\n",
    "        xs = X[:, p]\n",
    "        ts = np.arange(Nt)\n",
    "\n",
    "        # Loop over all variables and fill the respective VAR array\n",
    "        for varname, var_data in var_data_dict.items():\n",
    "            VAR[varname][:, p] = var_data[ts, zs, ys, xs]\n",
    "\n",
    "    # Return all the arrays in a list\n",
    "    return [VAR[varname] for varname in varnames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fec4908-e1a9-47d2-8f34-9fcafde9367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUNNING\n",
    "varnames=['qv','th','th_e','buoyancy','HMC','VMF_c','VMF_g']#,'qi','qr']\n",
    "[QV,TH,TH_E,BUOYANCY,HMC,VMF_c,VMF_g]=make_lagrangian_array(varnames); #,QI,QR]\n",
    "check_memory()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a756fc2a-7d8c-47d6-af97-d3c8616c9b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Data\n",
    "##############\n",
    "print('Saving Data\\n')\n",
    "import h5py\n",
    "dir2=dir+'Project_Algorithms/Lagrangian_Arrays/job_out/'\n",
    "out_file=dir2+f'VARS_binary_array_{res}_{t_res}_{Np_str}'\n",
    "if job_array==True:\n",
    "    out_file+=f'_{job_id}.h5'\n",
    "elif job_array==False:\n",
    "    out_file+=f'.h5'\n",
    "\n",
    "vars=['QV','TH','TH_E','BUOYANCY','HMC','VMF_c','VMF_g']#,'QI','QR']\n",
    "if Np_str=='1e6':\n",
    "    initiate_array(out_file,vars,t_chunk_size=1,p_chunk_size=100_000)\n",
    "elif Np_str=='50e6':\n",
    "    initiate_array(out_file,vars,t_chunk_size=1,p_chunk_size=500_000)\n",
    "\n",
    "with h5py.File(out_file, 'a') as f: \n",
    "    f['QV'][:]=QV\n",
    "    \n",
    "    f['TH'][:]=TH\n",
    "    f['TH_E'][:]=TH_E\n",
    "    \n",
    "    f['BUOYANCY'][:]=BUOYANCY\n",
    "    \n",
    "    f['HMC'][:]=HMC\n",
    "    \n",
    "    f['VMF_c'][:]=VMF_c\n",
    "\n",
    "    f['VMF_g'][:]=VMF_g\n",
    "\n",
    "    # f['QI'][:]=QI\n",
    "    # f['QR'][:]=QR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad95645-bce6-4e75-b1e6-5ecfb7ada0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "#RECOMBINE SEPERATE JOB_ARRAYS AFTER\n",
    "recombine=False #KEEP FALSE WHEN JOB ARRAY IS RUNNING\n",
    "# recombine=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b9c127-1a4f-4666-ada1-265ff780b389",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #OLD VERSION\n",
    "# if recombine==True:\n",
    "#     dir3=dir+'Project_Algorithms/Lagrangian_Arrays/' \n",
    "#     out_file=dir3+f'VARS_binary_array_{res}_{t_res}_{Np_str}.h5' \n",
    "    \n",
    "#     vars=['QV','TH','TH_E','BUOYANCY','HMC','VMF_c','VMF_g']\n",
    "\n",
    "#     if t_res=='5min':\n",
    "#         initiate_array(out_file,vars,t_chunk_size=20,p_chunk_size=500_000)\n",
    "#     elif t_res=='1min':\n",
    "#         initiate_array(out_file,vars,t_chunk_size=100,p_chunk_size=500_000)\n",
    "\n",
    "#     print('combining')\n",
    "#     with h5py.File(out_file, 'r+') as f_out:\n",
    "#         num_jobs=3\n",
    "#         for job_id in np.arange(1,num_jobs+1):\n",
    "#             if np.mod(job_id,5)==0: print(f\"job_id = {job_id}\")\n",
    "#             [a,b] = get_job_range(job_id,num_jobs)\n",
    "\n",
    "#             dir2=dir+'Project_Algorithms/Lagrangian_Arrays/job_out/'\n",
    "#             in_file=dir2+f'VARS_binary_array_{res}_{t_res}_{Np_str}_{job_id}.h5' \n",
    "#             with h5py.File(in_file, 'r') as f_in: \n",
    "#                 for var in vars:\n",
    "#                     f_out[var][a:b]=f_in[var][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502c2c6b-1be6-4f90-9cc7-cfafa0f1ba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "if recombine==True:\n",
    "    #DASK-ENABLED\n",
    "    import glob\n",
    "    import re\n",
    "    def recombine_func(in_files,out_file):\n",
    "        # matching_files = sorted(glob.glob(in_files))\n",
    "        matching_files = sorted(\n",
    "        glob.glob(in_files),\n",
    "        key=lambda f: int(re.search(r'_(\\d+)\\.h5$', f).group(1))\n",
    "    )\n",
    "        print('recombining')\n",
    "        from dask.diagnostics import ProgressBar\n",
    "        out=xr.open_mfdataset(matching_files,engine='h5netcdf',concat_dim='phony_dim_0',combine='nested',phony_dims='sort')\n",
    "        with ProgressBar():\n",
    "            out.to_netcdf(out_file, engine='h5netcdf')\n",
    "        \n",
    "    if recombine==True:\n",
    "        dir2=dir+'Project_Algorithms/Lagrangian_Arrays/job_out/'\n",
    "        dir3=dir+'Project_Algorithms/Lagrangian_Arrays/'\n",
    "        in_files = dir2 + f'VARS_binary_array_{res}_{t_res}_{Np_str}_*.h5'\n",
    "        out_file=dir3+f'VARS_binary_array_{res}_{t_res}_{Np_str}.h5' \n",
    "        \n",
    "        recombine_func(in_files,out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f9397c-4b54-48bf-9a6f-f17a64bbad57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062cf290-b932-4e7b-bdbb-c6bf99c49612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reading Back Data Later\n",
    "# ##############\n",
    "# import h5py\n",
    "# dir2=dir+'Project_Algorithms/Lagrangian_Arrays/'\n",
    "# with h5py.File(dir2+f'VARS_binary_array_{res}_{t_res}_{Np_str}.h5', 'r') as f:\n",
    "#     # Load the dataset by its name\n",
    "#     QV = f['QV'][:]\n",
    "#     TH = f['TH'][:]\n",
    "#     TH_E = f['TH_E'][:]\n",
    "#     BUOYANCY = f['BUOYANCY'][:]\n",
    "#     HMC = f['HMC'][:]\n",
    "#     VMF_c = f['VMF_c'][:]\n",
    "# check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aace102-3315-4a16-aa1a-9ac9e90e6d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "#TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af89a567-abc5-48b2-ae3b-0f22b753d9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1277af5a-99eb-4fbb-8c9c-9b7715016f93",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #INDIVIDUAL JOBS\n",
    "# num_jobs=120\n",
    "# job_id=2\n",
    "# [start_job,end_job]=get_job_range(job_id, num_jobs)\n",
    "# data2=data.isel(time=slice(start_job,end_job))\n",
    "\n",
    "# dir2=dir+'Project_Algorithms/Lagrangian_Arrays/job_out/'\n",
    "# open_file=dir2+f'VARS_binary_array_{res}_{t_res}_{Np_str}_{job_id}.h5'\n",
    "# with h5py.File(open_file, 'r') as f:\n",
    "#     # Load the dataset by its name\n",
    "#     QV = f['QV'][:]\n",
    "#     TH = f['TH'][:]\n",
    "#     TH_E = f['TH_E'][:]\n",
    "#     BUOYANCY = f['BUOYANCY'][:]\n",
    "#     HMC = f['HMC'][:]\n",
    "#     VMF_c = f['VMF_c'][:]\n",
    "# check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8e389e-682e-4c50-baff-3b828f8911cf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# dir2=dir+'Project_Algorithms/Lagrangian_Arrays/'\n",
    "# with h5py.File(dir2+f'lagrangian_binary_array_{res}_{t_res}_{Np_str}.h5', 'r') as f:\n",
    "#     # Load the dataset by its name\n",
    "#     Z = f['Z'][start_job:end_job]\n",
    "#     Y = f['Y'][start_job:end_job]\n",
    "#     X = f['X'][start_job:end_job]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5586cbf-a406-4f38-8def-fe17f42da00c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# t,p=0,1000\n",
    "# def test(t,p,VAR,var):\n",
    "#     z=Z[t,p];y=Y[t,p];x=X[t,p]\n",
    "#     out=data2[var].isel(time=t,zh=z,yh=y,xh=x).data\n",
    "#     print(VAR[t,p],out)\n",
    "\n",
    "# test(t,p,QV,'qv')\n",
    "# test(t,p,TH,'th')\n",
    "# test(t,p,BUOYANCY,'buoyancy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b0e77a-8b19-4220-9e16-3a72cdf17a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7212f27f-ffe9-4f47-bedb-a5691f382c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING\n",
    "# start_job=int(500/5);end_job=int(505/5)\n",
    "start_job=500;end_job=502\n",
    "data2=data.isel(time=slice(start_job,end_job))\n",
    "\n",
    "dir2=dir+'Project_Algorithms/Lagrangian_Arrays/'\n",
    "open_file=dir2+f'VARS_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "with h5py.File(open_file, 'r') as f:\n",
    "    # Load the dataset by its name\n",
    "    QV = f['QV'][start_job:end_job]\n",
    "    TH = f['TH'][start_job:end_job]\n",
    "    TH_E = f['TH_E'][start_job:end_job]\n",
    "    BUOYANCY = f['BUOYANCY'][start_job:end_job]\n",
    "    HMC = f['HMC'][start_job:end_job]\n",
    "    VMF_c = f['VMF_c'][start_job:end_job]\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae22e00-c7da-45e4-b315-4ef973f54e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir2=dir+'Project_Algorithms/Lagrangian_Arrays/'\n",
    "with h5py.File(dir2+f'lagrangian_binary_array_{res}_{t_res}_{Np_str}.h5', 'r') as f:\n",
    "    # Load the dataset by its name\n",
    "    Z = f['Z'][start_job:end_job]\n",
    "    Y = f['Y'][start_job:end_job]\n",
    "    X = f['X'][start_job:end_job]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e35855-a957-4044-aea3-37c692eb8193",
   "metadata": {},
   "outputs": [],
   "source": [
    "t,p=0,1000\n",
    "def test(t,p,VAR,var):\n",
    "    z=Z[t,p];y=Y[t,p];x=X[t,p]\n",
    "    out=data2[var].isel(time=t,zh=z,yh=y,xh=x).data\n",
    "    print(VAR[t,p],out)\n",
    "\n",
    "test(t,p,QV,'qv')\n",
    "test(t,p,TH,'th')\n",
    "test(t,p,BUOYANCY,'buoyancy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6515c719-b8b9-4719-8d49-8c245929c600",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
