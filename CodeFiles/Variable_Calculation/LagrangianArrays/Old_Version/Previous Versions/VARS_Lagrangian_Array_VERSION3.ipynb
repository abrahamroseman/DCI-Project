{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fae0de0-de5b-44b8-b03b-3363e3f59fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS FUNCTION IS FOR RUNNING WITH SLURM JOB ARRAY\n",
    "#(SPLITS UP JOB_ARRAY BELOW INTO EVEN MORE TASKS)\n",
    "def StartSlurmJobArray(num_jobs,num_slurm_jobs, ISRUN):\n",
    "    job_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0)) #this is the current SBATCH job id\n",
    "    if job_id==0: job_id=1\n",
    "    if ISRUN==False:\n",
    "        start_job=1;end_job=num_jobs+1\n",
    "        return start_job,end_job\n",
    "    total_elements=num_jobs #total num of variables\n",
    "\n",
    "    job_range = total_elements // num_slurm_jobs  # Base size for each chunk\n",
    "    remaining = total_elements % num_slurm_jobs   # Number of chunks with 1 extra \n",
    "    \n",
    "    # Function to compute the start and end for each job_id\n",
    "    def get_job_range(job_id, num_slurm_jobs):\n",
    "        job_id-=1\n",
    "        # Add one extra element to the first 'remaining' chunks\n",
    "        start_job = job_id * job_range + min(job_id, remaining)\n",
    "        end_job = start_job + job_range + (1 if job_id < remaining else 0)\n",
    "    \n",
    "        if job_id == num_slurm_jobs - 1: \n",
    "            end_job = total_elements \n",
    "        return start_job, end_job\n",
    "    # def job_testing():\n",
    "    #     #TESTING\n",
    "    #     start=[];end=[]\n",
    "    #     for job_id in range(1,num_slurm_jobs+1):\n",
    "    #         start_job, end_job = get_job_range(job_id)\n",
    "    #         print(start_job,end_job)\n",
    "    #         start.append(start_job)\n",
    "    #         end.append(end_job)\n",
    "    #     print(np.all(start!=end))\n",
    "    #     print(len(np.unique(start))==len(start))\n",
    "    #     print(len(np.unique(end))==len(end))\n",
    "    # job_testing()\n",
    "\n",
    "    # if sbatch==True:\n",
    "        \n",
    "    start_job, end_job = get_job_range(job_id, num_slurm_jobs)\n",
    "    index_adjust=start_job\n",
    "    # print(f'start_job = {start_job}, end_job = {end_job}')\n",
    "    if start_job==0: start_job=1\n",
    "    if end_job==total_elements: end_job+=1\n",
    "    return start_job,end_job\n",
    "\n",
    "# job_id=1\n",
    "# [start_slurm_job,end_slurm_job,slurm_index_adjust]=StartSlurmJobArray(num_jobs,num_slurm_jobs,ISRUN)\n",
    "# parcel=parcel1.isel(xh=slice(start_job,end_job))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b44bc7-9d0d-416d-9306-59f1014e803b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in Packages and Data\n",
    "\n",
    "#Importing Packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import matplotlib.gridspec as gridspec\n",
    "import xarray as xr\n",
    "import os; import time\n",
    "import pickle\n",
    "\n",
    "#IMPORTING H5PY\n",
    "###############################################################\n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\"\n",
    "import h5py #also in sbatch add: export HDF5_USE_FILE_LOCKING=FALSE\n",
    "###############################################################\n",
    "\n",
    "# Importing Model Data\n",
    "check=False\n",
    "dir='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "\n",
    "# # dx = 1 km; Np = 1M; Nt = 5 min\n",
    "# data1=xr.open_dataset(dir+'../cm1r20.3/run/cm1out_1km_5min.nc', decode_timedelta=True) #***\n",
    "# parcel1=xr.open_dataset(dir+'../cm1r20.3/run/cm1out_pdata_1km_5min_1e6.nc', decode_timedelta=True) #***\n",
    "# res='1km';t_res='5min'\n",
    "# Np_str='1e6'\n",
    "\n",
    "# # dx = 1km; Np = 50M\n",
    "# #Importing Model Data\n",
    "# dir2='/home/air673/koa_scratch/'\n",
    "# data1=xr.open_dataset(dir2+'cm1out_1km_1min.nc', decode_timedelta=True) #***\n",
    "# parcel1=xr.open_dataset(dir2+'cm1out_pdata_1km_1min_50M.nc', decode_timedelta=True) #***\n",
    "# res='1km'; t_res='1min'; Np_str='50e6'\n",
    "\n",
    "# dx = 1km; Np = 50M; Nz = 95\n",
    "#Importing Model Data\n",
    "dir2='/home/air673/koa_scratch/'\n",
    "data1=xr.open_dataset(dir2+'cm1out_1km_1min_95nz.nc', decode_timedelta=True) #***\n",
    "parcel1=xr.open_dataset(dir2+'cm1out_pdata_1km_1min_95nz.nc', decode_timedelta=True) #***\n",
    "res='1km'; t_res='1min_95nz'; Np_str='50e6'\n",
    "\n",
    "# # dx = 250m; Np = 50M\n",
    "# #Importing Model Data\n",
    "# dir2='/home/air673/koa_scratch/'\n",
    "# data1=xr.open_dataset(dir2+'cm1out_250m_1min_50M.nc', decode_timedelta=True) #***\n",
    "# parcel1=xr.open_dataset(dir2+'cm1out_pdata_250m_1min_50M.nc', decode_timedelta=True) #***\n",
    "# res='250m'; t_res='1min'; Np_str='50e6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2a15dd-27ba-44dd-b4b8-bf224085cb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "def print_memory_usage(note=\"\"):\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem = process.memory_info().rss / 1024 ** 2  # in MB\n",
    "    print(f\"[{note}] Memory usage: {mem:.2f} MB\")\n",
    "print_memory_usage(\"Current Memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fca28e-cac5-45d3-92ba-3c985465edd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "dir2='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "path=dir2+'../Functions/'\n",
    "sys.path.append(path)\n",
    "\n",
    "import NumericalFunctions\n",
    "from NumericalFunctions import * # import NumericalFunctions \n",
    "import PlottingFunctions\n",
    "from PlottingFunctions import * # import PlottingFunctions\n",
    "\n",
    "\n",
    "# # Get all functions in NumericalFunctions\n",
    "# import inspect\n",
    "# functions = [f[0] for f in inspect.getmembers(NumericalFunctions, inspect.isfunction)]\n",
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561a8ee0-e20f-4d60-aa5c-be52057cf11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "#DATA INITIALIZATION and LOADING FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27249259-c45c-422d-83e5-f3553fa8264f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#INITIALIZE DATA FUNCTION\n",
    "###############################################################\n",
    "def initiate_array(out_file, vars, t_chunk_size, p_chunk_size, t_size=None, p_size=None):\n",
    "    if t_size is None:\n",
    "        t_size = len(data['time'])  # Number of timesteps\n",
    "    if p_size is None:\n",
    "        p_size = len(parcel['xh'])  # Number of vertical levels\n",
    "\n",
    "    with h5py.File(out_file, 'w') as f:\n",
    "        for var_name in vars:\n",
    "            if var_name not in f:\n",
    "                # Set dtype conditionally\n",
    "                if var_name in ['Z', 'Y', 'X']:\n",
    "                    dtype = np.uint16\n",
    "                elif var_name in ['A_g', 'A_c']:\n",
    "                    dtype = np.bool_\n",
    "                else:\n",
    "                    dtype = np.float32  # or whatever your default is\n",
    "\n",
    "                f.create_dataset(\n",
    "                    var_name,\n",
    "                    shape=(t_size, p_size),\n",
    "                    chunks=(t_chunk_size, p_chunk_size),\n",
    "                    dtype=dtype\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af436f5-7b99-4119-bf5e-5295ca050b13",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#INITIALIZE DATA FUNCTION\n",
    "def InitiateArray_Job(data,out_file, vars, t_chunk_size, p_chunk_size, t_size=None, p_size=None):\n",
    "    if t_size is None:\n",
    "        t_size = len(data['time'])  # Number of timesteps\n",
    "    if p_size is None:\n",
    "        p_size = len(parcel1['xh'])  # Number of vertical levels\n",
    "\n",
    "    with h5py.File(out_file, 'w') as f:\n",
    "        for var_name in vars:\n",
    "            if var_name not in f:\n",
    "                # Set dtype conditionally\n",
    "                if var_name in ['Z', 'Y', 'X']:\n",
    "                    dtype = np.uint16\n",
    "                elif var_name in ['A_g', 'A_c']:\n",
    "                    dtype = np.bool_\n",
    "                else:\n",
    "                    dtype = np.float32  # or whatever your default is\n",
    "\n",
    "                f.create_dataset(\n",
    "                    var_name,\n",
    "                    shape=(t_size, p_size),\n",
    "                    chunks=(t_chunk_size, p_chunk_size),\n",
    "                    dtype=dtype\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab1d6c9-f41f-47f0-ad35-6ce0f06ec4ff",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#JOB ARRAY SETUP\n",
    "def StartJobArray(job_id,num_jobs):\n",
    "    total_elements=len(data1['time']) #total num of variables\n",
    "\n",
    "    if num_jobs >= total_elements:\n",
    "        raise ValueError(\"Number of jobs cannot be greater than or equal to total elements.\")\n",
    "    \n",
    "    job_range = total_elements // num_jobs  # Base size for each chunk\n",
    "    remaining = total_elements % num_jobs   # Number of chunks with 1 extra \n",
    "    \n",
    "    # Function to compute the start and end for each job_id\n",
    "    def get_job_range(job_id, num_jobs):\n",
    "        job_id-=1\n",
    "        # Add one extra element to the first 'remaining' chunks\n",
    "        start_job = job_id * job_range + min(job_id, remaining)\n",
    "        end_job = start_job + job_range + (1 if job_id < remaining else 0)\n",
    "    \n",
    "        if job_id == num_jobs - 1: \n",
    "            end_job = total_elements #- 1\n",
    "        return start_job, end_job\n",
    "    # def job_testing():\n",
    "    #     #TESTING\n",
    "    #     start=[];end=[]\n",
    "    #     for job_id in range(1,num_jobs+1):\n",
    "    #         start_job, end_job = get_job_range(job_id)\n",
    "    #         print(start_job,end_job)\n",
    "    #         start.append(start_job)\n",
    "    #         end.append(end_job)\n",
    "    #     print(np.all(start!=end))\n",
    "    #     print(len(np.unique(start))==len(start))\n",
    "    #     print(len(np.unique(end))==len(end))\n",
    "    # job_testing()\n",
    "\n",
    "    # if sbatch==True:\n",
    "    #     job_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0)) #this is the current SBATCH job id\n",
    "    #     if job_id==0: job_id=1\n",
    "        \n",
    "    start_job, end_job = get_job_range(job_id, num_jobs)\n",
    "    index_adjust=start_job\n",
    "    # print(f'start_job = {start_job}, end_job = {end_job}')\n",
    "    return start_job,end_job,index_adjust\n",
    "# job_id=1;num_jobs=60\n",
    "# [start_job,end_job,index_adjust]=StartJobArray(job_id,num_jobs)\n",
    "# data=data1.isel(time=slice(start_job,end_job))\n",
    "# parcel=parcel1.isel(time=slice(start_job,end_job))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e250e0f6-14ea-44bb-9c9a-b311107cdba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5fd460-f273-41e2-ac5f-b0cd4a504448",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "#FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d8eabc-f4e1-4bc1-b6e4-4f214aa223ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def GetZYX(parcel): #OLD #NO NEED TO RECALCULATE, ALREADY CALCULATED IN Lagrangian_Binary_Array\n",
    "#     #Lagrangian Position Arrays\n",
    "#     ##############\n",
    "#     def grid_location(z,y,x):\n",
    "#         zf=data1['zf'].values*1000; which_zh=np.clip(np.searchsorted(zf,z)-1,0,None).astype(np.uint16)\n",
    "#         #which_zh=np.where(which_zh == -1, 0, which_zh) \n",
    "        \n",
    "#         yf=data1['yf'].values*1000; which_yh=np.clip(np.searchsorted(yf,y)-1,0,None).astype(np.uint16) \n",
    "#         #which_yh=np.where(which_yh == -1, 0, which_yh) \n",
    "        \n",
    "#         xf=data1['xf'].values*1000; which_xh=np.clip(np.searchsorted(xf,x)-1,0,None).astype(np.uint16)\n",
    "#         #which_xh=np.where(which_xh == -1, 0, which_xh) \n",
    "        \n",
    "#         return which_zh,which_yh,which_xh\n",
    "    \n",
    "#     print('Creating Lagrangian X,Y,Z Binary Arrays')\n",
    "#     x=parcel['x'].data;y=parcel['y'].data;z=parcel['z'].data\n",
    "#     Z,Y,X=grid_location(z,y,x)\n",
    "    \n",
    "#     return Z,Y,X\n",
    "\n",
    "# Reading Back Data Later\n",
    "##############\n",
    "def make_data_dict(in_file,var_names,read_type,start_job,end_job):\n",
    "    if read_type=='h5py':\n",
    "        with h5py.File(in_file, 'r') as f:\n",
    "           data_dict = {var_name: f[var_name][start_job:end_job] for var_name in var_names}\n",
    "            \n",
    "    elif read_type=='xarray':\n",
    "        in_data = xr.open_dataset(\n",
    "            in_file,\n",
    "            engine='h5netcdf',\n",
    "            phony_dims='sort',\n",
    "            chunks={'phony_dim_0': 100, 'phony_dim_1': 1_000_000} \n",
    "        )\n",
    "        data_dict = {k: in_data[k][start_job:end_job].compute().data for k in var_names}\n",
    "    return data_dict\n",
    "\n",
    "# read_type='xarray'\n",
    "read_type='h5py'\n",
    "\n",
    "def GetZYX(start_job,end_job):\n",
    "    import h5py\n",
    "    dir2=dir+'Project_Algorithms/Lagrangian_Arrays/OUTPUT/'\n",
    "    in_file=dir2+f'lagrangian_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "    \n",
    "    var_names = ['Z', 'Y', 'X']\n",
    "    data_dict = make_data_dict(in_file,var_names,read_type,start_job,end_job)\n",
    "    Z, Y, X = (data_dict[k] for k in var_names)\n",
    "    return Z,Y,X\n",
    "#[Z,Y,X] GetZYX(parcel,start_job,end_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af32c6d-f363-4b55-ad51-2b1913065f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_variable(data,varname):\n",
    "    if res == \"1km\":\n",
    "        dir2='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "    if res == \"250m\":\n",
    "        dir2='/home/air673/koa_scratch/'\n",
    "    \n",
    "    if varname=='th_v':\n",
    "        with h5py.File(dir2 + 'Variable_Calculation/OUTPUT/' + 'theta_v'+f'_{res}_{t_res}'+'.h5', 'r') as f:\n",
    "            var_data = f['th_v'][start_job:end_job]\n",
    "    elif varname=='th_e':\n",
    "        with h5py.File(dir2 + 'Variable_Calculation/OUTPUT/' + 'theta_e'+f'_{res}_{t_res}'+'.h5', 'r') as f:\n",
    "            var_data = f['theta_e'][start_job:end_job]\n",
    "    elif varname in ['RH_vapor','RH_ice']:\n",
    "        with h5py.File(dir2 + 'Variable_Calculation/OUTPUT/' + 'theta_e'+f'_{res}_{t_res}'+'.h5', 'r') as f:\n",
    "            var_data = f[varname][start_job:end_job]\n",
    "    elif varname=='HMC':\n",
    "        file_path = dir2 + 'Variable_Calculation/OUTPUT/' + '2D_Moisture_Convergence' + f'_{res}_{t_res}' + '.h5'\n",
    "        with h5py.File(file_path, 'r') as f:\n",
    "            var_data = f['HMC'][start_job:end_job]\n",
    "\n",
    "    elif varname in ['VMF_g','VMF_c']:\n",
    "        file_path = dir2 + 'Variable_Calculation/OUTPUT/' + f'Eulerian_VMF_{res}_{t_res}.h5'\n",
    "        with h5py.File(file_path, 'r') as f:\n",
    "            var_data = f[varname][start_job:end_job]\n",
    "\n",
    "    elif varname in ['buoyancy_cm1','buoyancy_full','buoyancy_full_each_t']:\n",
    "        file_path = dir2 + 'Variable_Calculation/OUTPUT/' + f'Buoyancy_{res}_{t_res}.h5'\n",
    "        with h5py.File(file_path, 'r') as f:\n",
    "            var_data = f[varname][start_job:end_job]\n",
    "   \n",
    "    else:\n",
    "        var_data=data[varname].data\n",
    "    return var_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1465fadc-9309-417f-990b-d13eaf073fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def MakeLagrangianArray(data,varnames,Z,Y,X): #SLOW FOR LOOP VERSION\n",
    "#     print('Making Variables Lagrangian Array')\n",
    "#     # Initialize dictionaries\n",
    "#     var_data_dict = {varname: call_variable(data,varname) for varname in varnames}\n",
    "#     VAR = {varname: np.zeros_like(Z, dtype='float32') for varname in varnames}\n",
    "\n",
    "#     Nt = len(data['time'])\n",
    "#     Np = len(parcel1['xh'])\n",
    "#     for p in np.arange(Np):\n",
    "#         # if np.mod(p, 1e6) == 0: \n",
    "#         #     print(f\"{p}/{len(parcel1['xh'])}\")\n",
    "\n",
    "#         # Get Indices\n",
    "#         zs = Z[:, p]\n",
    "#         ys = Y[:, p]\n",
    "#         xs = X[:, p]\n",
    "#         ts = np.arange(Nt)\n",
    "\n",
    "#         # Loop over all variables and fill the respective VAR array\n",
    "#         for varname, var_data in var_data_dict.items():\n",
    "#             VAR[varname][:, p] = var_data[ts, zs, ys, xs]\n",
    "\n",
    "#     # Return all the arrays in a list\n",
    "#     # return [VAR[varname] for varname in varnames]\n",
    "#     return VAR\n",
    "\n",
    "def MakeLagrangianArray(data,varnames,Z,Y,X): #FAST ADVANCED INDEXING VERSION\n",
    "    print('Making Variables Lagrangian Array')\n",
    "    # Initialize dictionaries\n",
    "    var_data_dict = {varname: call_variable(data,varname) for varname in varnames}\n",
    "    VAR = {varname: np.zeros_like(Z, dtype='float32') for varname in varnames}\n",
    "\n",
    "    print('Running')\n",
    "    Nt = len(data['time'])\n",
    "    # Get Indices\n",
    "    z_idx = Z\n",
    "    y_idx = Y\n",
    "    x_idx = X\n",
    "    ts = np.arange(Nt)[:, None]\n",
    "    t_idx = np.broadcast_to(ts, Z.shape)\n",
    "\n",
    "    for varname, var_data in var_data_dict.items():\n",
    "        VAR[varname] = var_data[t_idx, z_idx, y_idx, x_idx]\n",
    "\n",
    "    # return [VAR[varname] for varname in varnames]\n",
    "    return VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf80a610-a7bf-4518-bf16-c40d244fb9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveData(data,varnames,VAR):\n",
    "    # Saving Data\n",
    "    ##############\n",
    "    print('Saving Data\\n')\n",
    "    import h5py\n",
    "    dir2=dir+'Project_Algorithms/Lagrangian_Arrays/SBATCH/job_out/'\n",
    "    out_file=dir2+f'lagrangian_VARS_array_{res}_{t_res}_{Np_str}'\n",
    "    out_file+=f'_{job_id}.h5'\n",
    "\n",
    "    out_varnames=[varname.upper() for varname in varnames]\n",
    "    InitiateArray_Job(data,out_file,out_varnames,t_chunk_size=1,p_chunk_size=1_000_000)\n",
    "    with h5py.File(out_file, 'a') as f: \n",
    "        for var in VAR:\n",
    "            f[var.upper()][:] = VAR[var]\n",
    "# SaveData(data,varnames,VAR)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043df228-3ec1-4a5e-901c-da43c1c999a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "#RUNNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c95a708-62a9-40f1-899f-32bc757fed08",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "#JOB ARRAY SETUP\n",
    "################################\n",
    "#*#*\n",
    "# how many total jobs are being run? i.e. array=1-100 ==> num_jobs=100\n",
    "if res == \"1km\" and Np_str=='1e6':\n",
    "    num_jobs=60 #1M parcels\n",
    "    num_slurm_jobs=10\n",
    "elif res == \"1km\" and Np_str=='50e6':\n",
    "    num_jobs=400 #50M parcels\n",
    "    num_slurm_jobs=60\n",
    "elif res == \"250m\" and Np_str=='50e6':\n",
    "    num_jobs=600 #50M parcels\n",
    "    num_slurm_jobs=150\n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbf5825-d283-47e8-9282-97e909de0beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "#RUN SETUP\n",
    "###############################\n",
    "varnames=['qv','th','th_v','th_e','RH_vapor','RH_ice','buoyancy_cm1','buoyancy_full','buoyancy_full_each_t','HMC','VMF_c','VMF_g']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c7bedd-dd0c-42e2-b57b-08fc86664716",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[start_slurm_job,end_slurm_job]=StartSlurmJobArray(num_jobs=num_jobs,num_slurm_jobs=num_slurm_jobs,ISRUN=True) #if ISRUN is False, then will not run using slurm_job_array\n",
    "print(f\"Running on Slurm_Jobs for Slurm_Job_Ids: {(start_slurm_job,end_slurm_job-1)}\")\n",
    "\n",
    "job_id_list=np.arange(start_slurm_job,end_slurm_job)\n",
    "for job_id in job_id_list:\n",
    "    if job_id % 1 == 0: print(f'current job_id = {job_id}/{num_jobs}')\n",
    "    [start_job,end_job,index_adjust]=StartJobArray(job_id,num_jobs)\n",
    "\n",
    "    #SLICING DATA\n",
    "    [start_job,end_job,index_adjust]=StartJobArray(job_id,num_jobs)\n",
    "    data=data1.isel(time=slice(start_job,end_job))\n",
    "    parcel=parcel1.isel(time=slice(start_job,end_job))\n",
    "\n",
    "    #GETTING POSITION VARIABLES\n",
    "    [Z,Y,X]=GetZYX(start_job,end_job)\n",
    "\n",
    "    #MAKING LAGRANGIAN ARRAYS (for threshold)\n",
    "    VAR=MakeLagrangianArray(data,varnames,Z,Y,X)\n",
    "\n",
    "    #SAVING\n",
    "    SaveData(data,varnames,VAR)\n",
    "\n",
    "    print_memory_usage(\"Current Memory\")\n",
    "    del VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9fcf73-cb49-406c-8468-00b7b2fc0106",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "#RECOMBINE SEPERATE JOB_ARRAYS AFTER\n",
    "recombine=False #KEEP FALSE WHEN JOB ARRAY IS RUNNING\n",
    "# recombine=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daecace-d163-4e6e-b191-13fec9f79b04",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #OLD\n",
    "# def Recombine(num_jobs):\n",
    "#     dir2=dir+'Project_Algorithms/Lagrangian_Arrays/SBATCH/job_out/'\n",
    "#     dir3=dir+'Project_Algorithms/Lagrangian_Arrays/OUTPUT/'\n",
    "#     out_file=dir3+f'lagrangian_VARS_array_{res}_{t_res}_{Np_str}.h5'\n",
    "\n",
    "#     print('initializing array')\n",
    "#     vars=['u','w','z','Z','Y','X','W','QCQI','A_g','A_c']\n",
    "#     InitiateArray_Job(out_file,vars,t_chunk_size=100,p_chunk_size=500_000)\n",
    "\n",
    "#     print('recombining')\n",
    "#     with h5py.File(out_file, 'r+') as f_out:\n",
    "#         for job_id in np.arange(1,num_jobs+1):\n",
    "#             if job_id % 5: print(f\"job_id = {job_id}\")\n",
    "#             [a,b] = get_job_range(job_id,num_jobs)\n",
    "    \n",
    "#             in_file=dir2+f'lagrangian_VARS_array_{res}_{t_res}_{Np_str}_{job_id}.h5' \n",
    "#             with h5py.File(in_file, 'r') as f_in: \n",
    "#                 for var in vars:\n",
    "#                     f_out[var][a:b]=f_in[var][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2987c2a0-b10c-466c-8776-4a64e6888ab2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def Recombine_Dask():\n",
    "    #DASK-ENABLED\n",
    "    import glob\n",
    "    import re\n",
    "    def recombine_func(in_files,out_file):\n",
    "        # matching_files = sorted(glob.glob(in_files))\n",
    "        matching_files = sorted(\n",
    "        glob.glob(in_files),\n",
    "        key=lambda f: int(re.search(r'_(\\d+)\\.h5$', f).group(1))\n",
    "    )\n",
    "     \n",
    "        print(f'recombining {len(matching_files)} files')\n",
    "        out=xr.open_mfdataset(matching_files,engine='h5netcdf',concat_dim='phony_dim_0',combine='nested',phony_dims='sort')\n",
    "        from dask.diagnostics import ProgressBar\n",
    "        with ProgressBar():\n",
    "            out.to_netcdf(out_file, engine='h5netcdf')\n",
    "        \n",
    "    if recombine==True:\n",
    "        dir2=dir+'Project_Algorithms/Lagrangian_Arrays/SBATCH/job_out/'\n",
    "        dir3=dir+'Project_Algorithms/Lagrangian_Arrays/OUTPUT/'\n",
    "        in_files = dir2 + f'lagrangian_VARS_array_{res}_{t_res}_{Np_str}_*.h5'\n",
    "        out_file=dir3+f'lagrangian_VARS_array_{res}_{t_res}_{Np_str}.h5' \n",
    "        \n",
    "        recombine_func(in_files,out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff2d447-8f5d-4764-b6c1-21e87339d524",
   "metadata": {},
   "outputs": [],
   "source": [
    "if recombine==True:\n",
    "    # Recombine(num_jobs=num_jobs)\n",
    "    Recombine_Dask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f673b9-f65a-4933-b552-cac652d7fa6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bacea7-672a-4704-ab2b-dd6b57588ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad14a22a-78b4-46fc-9565-6d55e186457f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023e1a35-1c0e-4ef4-b483-3c791d1a142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d64d948-dec3-457e-8d98-da63afab9e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #LOADING BACK IN\n",
    "# #TESTING\n",
    "# # start_job=int(500/5);end_job=int(505/5)\n",
    "# start_job=500;end_job=502\n",
    "# data2=data.isel(time=slice(start_job,end_job))\n",
    "\n",
    "# dir2=dir+'Project_Algorithms/Lagrangian_Arrays/'\n",
    "# open_file=dir2+f'lagrangian_VARS_array_{res}_{t_res}_{Np_str}.h5'\n",
    "# with h5py.File(open_file, 'r') as f:\n",
    "#     # Load the dataset by its name\n",
    "#     QV = f['QV'][start_job:end_job]\n",
    "#     RH = f['RH'][start_job:end_job]\n",
    "#     # TH = f['TH'][start_job:end_job]\n",
    "#     # TH_V = f['TH_V'][start_job:end_job]\n",
    "#     # TH_E = f['TH_E'][start_job:end_job]\n",
    "#     # BUOYANCY = f['BUOYANCY'][start_job:end_job]\n",
    "#     # HMC = f['HMC'][start_job:end_job]\n",
    "#     # VMF_c = f['VMF_c'][start_job:end_job]\n",
    "#     # for key in f:\n",
    "#         # globals()[key]=f[key][start_job:end_job]\n",
    "# # check_memory(globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f814508-6810-42c0-8bb6-d3391c8f8e0d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefb3e1a-651e-4582-87e5-36d740a078a0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #TESTING (IS VERSION3==VERSION2)\n",
    "# job_id=1\n",
    "# [start_job,end_job,index_adjust]=StartJobArray(job_id,num_jobs)\n",
    "# # print(start_job,end_job)\n",
    "# # start_job,end_job=0,3\n",
    "\n",
    "# #TESTING\n",
    "# dir2=dir+'Project_Algorithms/Lagrangian_Arrays/'\n",
    "# open_file=dir2+f'VARS_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "# with h5py.File(open_file, 'r') as f:\n",
    "#     # Load the dataset by its name\n",
    "#     for key in f:\n",
    "#         globals()[key+\"1\"]=f[key][start_job:end_job]\n",
    "#     # QV = f['QV'][start_job:end_job]\n",
    "#     # TH = f['TH'][start_job:end_job]\n",
    "#     # TH_V = f['TH_V'][start_job:end_job]\n",
    "#     # TH_E = f['TH_E'][start_job:end_job]\n",
    "#     # BUOYANCY = f['BUOYANCY'][start_job:end_job]\n",
    "#     # HMC = f['HMC'][start_job:end_job]\n",
    "#     # VMF_c = f['VMF_c'][start_job:end_job]\n",
    "# check_memory(globals())\n",
    "\n",
    "# job_id=1\n",
    "# [start_job,end_job,index_adjust]=StartJobArray(job_id,num_jobs)\n",
    "# # print(start_job,end_job)\n",
    "# dir2=dir+'Project_Algorithms/Lagrangian_Arrays/job_out/'\n",
    "# out_file=dir2+f'lagrangian_VARS_array_{res}_{t_res}_{Np_str}'\n",
    "# out_file+=f'_{job_id}.h5'\n",
    "# with h5py.File(out_file, 'r') as f:\n",
    "#     # Load the dataset by its name\n",
    "#     for key in f:\n",
    "#         globals()[key+\"2\"]=f[key][:]\n",
    "# check_memory(globals())\n",
    "\n",
    "# print(np.all(QV1==QV2))\n",
    "# print(np.all(TH1==TH2))\n",
    "# print(np.all(TH_V1==TH_V2))\n",
    "# print(np.all(TH_E1==TH_E2))\n",
    "# print(np.all(BUOYANCY1==BUOYANCY2))\n",
    "# print(np.all(HMC1==HMC2))\n",
    "# print(np.all(VMF_c1==VMF_C2))\n",
    "# print(np.all(VMF_g1==VMF_G2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d34db0f-1c99-4e35-82ce-dc10e8d9221f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #TESTING ARE RESULTS CORRECT\n",
    "# job_id=1;num_jobs=400\n",
    "# [start_job,end_job,index_adjust]=StartJobArray(job_id,num_jobs)\n",
    "# # print(start_job,end_job)\n",
    "# # start_job,end_job=0,3\n",
    "\n",
    "# dir2=dir+'Project_Algorithms/Lagrangian_Arrays/'\n",
    "# with h5py.File(dir2+f'lagrangian_binary_array_{res}_{t_res}_{Np_str}.h5', 'r') as f:\n",
    "#     # Load the dataset by its name\n",
    "#     Z = f['Z'][start_job:end_job]\n",
    "#     Y = f['Y'][start_job:end_job]\n",
    "#     X = f['X'][start_job:end_job]\n",
    "\n",
    "# #TESTING\n",
    "\n",
    "# dir2=dir+'Project_Algorithms/Lagrangian_Arrays/job_out/'\n",
    "# open_file=dir2+f'lagrangian_VARS_array_{res}_{t_res}_{Np_str}_{job_id}.h5' \n",
    "# with h5py.File(open_file, 'r') as f:\n",
    "#     key_list=list(f.keys())\n",
    "#     for key in f:\n",
    "#         print(key)\n",
    "#         globals()[key]=f[key][:]\n",
    "# data2=data1.isel(time=slice(start_job,end_job))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e35855-a957-4044-aea3-37c692eb8193",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #TESTING ARE RESULTS CORRECT\n",
    "# t,p=0,10000\n",
    "# def test(t,p,VAR,var):\n",
    "#     z=Z[t,p];y=Y[t,p];x=X[t,p]\n",
    "#     out=data2[var].isel(time=t,zh=z,yh=y,xh=x).data\n",
    "#     print(VAR[t,p],out)\n",
    "# def test2(t,p,VAR,var):\n",
    "#     if var in ['hmc']:\n",
    "#         var=var.upper()\n",
    "#     if var in ['vmf_c','vmf_g']:\n",
    "#         var='VMF'+f'_{var[-1]}'\n",
    "#     z=Z[t,p];y=Y[t,p];x=X[t,p]\n",
    "#     out=call_variable(data,var)[t,z,y,x]\n",
    "#     # out=data2[var].isel(time=t,zh=z,yh=y,xh=x).data\n",
    "#     print(VAR[t,p],out)\n",
    "# for key in [key_list[i] for i in [0, 2, 3]]:\n",
    "#     test2(t,p,globals()[key],key.lower())\n",
    "# for key in [key_list[i] for i in [1, 4, 5,6]]:\n",
    "#     test2(t,p,globals()[key],key.lower())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "work"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
