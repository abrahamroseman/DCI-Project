{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe11247d-70be-4b25-8409-3d6cce0c0f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in Packages and Data\n",
    "\n",
    "#Importing Packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import matplotlib.gridspec as gridspec\n",
    "import xarray as xr\n",
    "import os; import time\n",
    "import pickle\n",
    "import h5py\n",
    "###############################################################\n",
    "def coefs(coefficients,degree):\n",
    "    coef=coefficients\n",
    "    coefs=\"\"\n",
    "    for n in range(degree, -1, -1):\n",
    "        string=f\"({coefficients[len(coef)-(n+1)]:.1e})\"\n",
    "        coefs+=string + f\"x^{n}\"\n",
    "        if n != 0:\n",
    "            coefs+=\" + \"\n",
    "    return coefs\n",
    "###############################################################\n",
    "\n",
    "# Importing Model Data\n",
    "check=False\n",
    "dir='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "###############################################################\n",
    "\n",
    "# # dx = 1 km; Np = 1M; Nt = 5 min\n",
    "# data=xr.open_dataset(dir+'../cm1r20.3/run/cm1out_1km_5min.nc') #***\n",
    "# parcel=xr.open_dataset(dir+'../cm1r20.3/run/cm1out_pdata_1km_5min_1e6.nc') #***\n",
    "# res='1km';t_res='5min'\n",
    "# Np_str='1e6'\n",
    "\n",
    "# dx = 1km; Np = 50M\n",
    "#Importing Model Data\n",
    "check=False\n",
    "dir2='/home/air673/koa_scratch/'\n",
    "data=xr.open_dataset(dir2+'cm1out_1km_1min.nc') #***\n",
    "parcel=xr.open_dataset(dir2+'cm1out_pdata_1km_1min_50M.nc') #***\n",
    "res='1km'; t_res='1min'; Np_str='50e6'\n",
    "\n",
    "# # dx = 1km; Np = 100M\n",
    "# #Importing Model Data\n",
    "# check=False\n",
    "# dir2='/home/air673/koa_scratch/'\n",
    "# data=xr.open_dataset(dir2+'cm1out_1km_1min.nc') #***\n",
    "# parcel=xr.open_dataset(dir2+'cm1out_pdata_1km_1min_100M.nc') #***\n",
    "# res='1km'; t_res='1min'; Np_str='100e6'\n",
    "\n",
    "# #uncomment if using 250m data\n",
    "# #Importing Model Data\n",
    "# check=False\n",
    "# dir2='/home/air673/koa_scratch/'\n",
    "# data=xr.open_dataset(dir2+'cm1out_250m.nc') #***\n",
    "# # # parcel=xr.open_dataset(dir2+'cm1out_pdata_250m.nc') #***\n",
    "\n",
    "# # Restricts the timesteps of the data from timesteps0 to 140\n",
    "# data=data.isel(time=np.arange(0,400+1))\n",
    "# # # parcel=parcel.isel(time=np.arange(0,400+1))\n",
    "# res='250m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309327a9-a931-4dee-b35f-7b51bae815c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "dir2='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "path=dir2+'../Functions/'\n",
    "sys.path.append(path)\n",
    "\n",
    "import NumericalFunctions\n",
    "from NumericalFunctions import * # import NumericalFunctions \n",
    "import PlottingFunctions\n",
    "from PlottingFunctions import * # import PlottingFunctions\n",
    "\n",
    "\n",
    "# # Get all functions in NumericalFunctions\n",
    "# import inspect\n",
    "# functions = [f[0] for f in inspect.getmembers(NumericalFunctions, inspect.isfunction)]\n",
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2288d55-4274-4a68-84e3-963a6426c963",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JOB ARRAY SETUP\n",
    "job_array=False;index_adjust=0\n",
    "job_array=True\n",
    "\n",
    "if job_array==True:\n",
    "\n",
    "    num_jobs=300 #how many total jobs are being run? i.e. array=1-100 ==> num_jobs=100 #***\n",
    "    total_elements=len(data['time']) #total num of variables\n",
    "\n",
    "    if num_jobs >= total_elements:\n",
    "        raise ValueError(\"Number of jobs cannot be greater than or equal to total elements.\")\n",
    "    \n",
    "    job_range = total_elements // num_jobs  # Base size for each chunk\n",
    "    remaining = total_elements % num_jobs   # Number of chunks with 1 extra \n",
    "    \n",
    "    # Function to compute the start and end for each job_id\n",
    "    def get_job_range(job_id, num_jobs):\n",
    "        job_id-=1\n",
    "        # Add one extra element to the first 'remaining' chunks\n",
    "        start_job = job_id * job_range + min(job_id, remaining)\n",
    "        end_job = start_job + job_range + (1 if job_id < remaining else 0)\n",
    "    \n",
    "        if job_id == num_jobs - 1: \n",
    "            end_job = total_elements #- 1\n",
    "        return start_job, end_job\n",
    "    # def job_testing(num_jobs):\n",
    "    #     #TESTING\n",
    "    #     start=[];end=[]\n",
    "    #     for i,job_id in enumerate(range(1,num_jobs+1)):\n",
    "    #         start_job, end_job = get_job_range(job_id,num_jobs)\n",
    "    #         print(start_job,end_job)\n",
    "    #         start.append(start_job)\n",
    "    #         end.append(end_job)\n",
    "    #         # plt.scatter(i,end_job-start_job)\n",
    "    #     print(np.all(start!=end))\n",
    "    #     print(len(np.unique(start))==len(start))\n",
    "    #     print(len(np.unique(end))==len(end))\n",
    "    # job_testing(num_jobs)\n",
    "    \n",
    "    job_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0)) #this is the current SBATCH job id\n",
    "    if job_id==0: job_id=1\n",
    "    start_job, end_job = get_job_range(job_id, num_jobs)\n",
    "    index_adjust=start_job\n",
    "    print(f'start_job = {start_job}, end_job = {end_job}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d9a520-3229-4acd-89f9-f42c84862892",
   "metadata": {},
   "outputs": [],
   "source": [
    "if job_array==True:\n",
    "    #Indexing Array with JobArray\n",
    "    data=data.isel(time=slice(start_job,end_job))\n",
    "    parcel=parcel.isel(time=slice(start_job,end_job))\n",
    "    #(for 150_000_000 parcels use 500-1000 jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d732e0-763c-4ce0-8856-b7a325fa3f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Back Data Later\n",
    "##############\n",
    "\n",
    "def make_data_dict(in_file,var_names,read_type):\n",
    "    if read_type=='h5py':\n",
    "        with h5py.File(in_file, 'r') as f:\n",
    "            if job_array==True:\n",
    "                data_dict = {var_name: f[var_name][start_job:end_job] for var_name in var_names}\n",
    "            elif job_array==False:\n",
    "                data_dict = {var_name: f[var_name][:] for var_name in var_names}\n",
    "            \n",
    "    elif read_type=='xarray':\n",
    "        in_data = xr.open_dataset(\n",
    "            in_file,\n",
    "            engine='h5netcdf',\n",
    "            phony_dims='sort',\n",
    "            chunks={'phony_dim_0': 100, 'phony_dim_1': 100_000} \n",
    "        )\n",
    "        if job_array==True:\n",
    "            data_dict = {k: in_data[k][start_job:end_job].compute().data for k in var_names}\n",
    "        elif job_array==False:\n",
    "            data_dict = {k: in_data[k][:].compute().data for k in var_names}\n",
    "    return data_dict\n",
    "\n",
    "# read_type='xarray'\n",
    "read_type='h5py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501f0f89-6925-4b90-822d-af8d2b5d5d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "dir2=dir+'Project_Algorithms/Lagrangian_Arrays/'\n",
    "in_file=dir2+f'lagrangian_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "\n",
    "var_names = ['A_g', 'A_c', 'W', 'QCQI', 'Z', 'Y', 'X']\n",
    "data_dict = make_data_dict(in_file,var_names,read_type)\n",
    "A_g, A_c, W, QCQI, Z, Y, X = (data_dict[k] for k in var_names)\n",
    "\n",
    "# #Making Time Matrix\n",
    "# rows, cols = A.shape[0], A.shape[1]\n",
    "# T = np.arange(rows).reshape(-1, 1) * np.ones((1, cols), dtype=int)\n",
    "check_memory(globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fefbfa-46f5-466a-9372-acc7afc2f0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Back Data Later\n",
    "##############\n",
    "import h5py\n",
    "dir2=dir+'Project_Algorithms/Lagrangian_Arrays/'\n",
    "in_file=dir2+f'VARS_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "\n",
    "var_names = ['QV','TH','TH_E','BUOYANCY','HMC']\n",
    "data_dict = make_data_dict(in_file,var_names,read_type)\n",
    "QV, TH, TH_E, BUOYANCY, HMC = (data_dict[k] for k in var_names)\n",
    "check_memory(globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcafadf7-a5da-4898-8767-8314d2fbe4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DOMAIN SUBSETTING\n",
    "############################################################\n",
    "ocean_percent=2/8\n",
    "\n",
    "left_to_coast=data['xh'][0]+(data['xh'][-1]-data['xh'][0])*ocean_percent\n",
    "where_coast_xh=np.where(data['xh']>=left_to_coast)[0][0]#-25\n",
    "where_coast_xf=np.where(data['xf']>=left_to_coast)[0][0]#-25\n",
    "end_xh=len(data['xh'])-1-50\n",
    "end_xf=len(data['xf'])-1-50\n",
    "\n",
    "print(f'x in {0}:{where_coast_xh-1} FOR SEA')\n",
    "print(f'x in {where_coast_xh}:{end_xh} FOR LAND')\n",
    "# t_end=78 \n",
    "# if res=='250m':t_end=410\n",
    "# print(f't in {0}:{t_end} (6.5 hours)')\n",
    "if t_res==\"5min\":\n",
    "    t_start=36\n",
    "elif t_res==\"1min\":\n",
    "    t_start=36*5\n",
    "print(f't in {t_start}:end (8 hours)')\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "#SUBSETTING CODE\n",
    "A_g[(X<where_coast_xh)|(X>end_xh)]=0\n",
    "A_c[(X<where_coast_xh)|(X>end_xh)]=0\n",
    "\n",
    "#SUBSETTING TIME FOR JOB ARRAY\n",
    "if job_array==True:\n",
    "    if end_job<=t_start:\n",
    "        A_g[0:t_start]=0\n",
    "        A_c[0:t_start]=0\n",
    "elif job_array==False:\n",
    "    A_g[0:t_start]=0\n",
    "    A_c[0:t_start]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1164b8e9-0d2b-463a-9aec-357604c8af2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "#Functions\n",
    "# Full Profile function makes profile together for all timesteps. AveragedProfiles funciton takes the final mean of the combined profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42f9986-3f13-4914-b600-3c466046468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir2=dir+'Project_Algorithms/Domain_Profiles/'\n",
    "\n",
    "def LagrangianProfiles(type):\n",
    "    print(f'currently working on type {type}')\n",
    "    \n",
    "    #Create Storage Arrays for All Variables\n",
    "    vars=['w','qv','qc_plus_qi','th','th_e','buoyancy','HMC']\n",
    "    for var in vars:\n",
    "        zhs=data['zh'].values\n",
    "        globals()[f\"profile_{var}\"]=np.zeros((len(zhs), 3)) #column 1: var, column 2: counter, column 3: list of zhs\n",
    "        globals()[f\"profile_{var}\"][:,2]=zhs\n",
    "    \n",
    "    Nt=len(data['time'])\n",
    "    for t in np.arange(Nt):\n",
    "        # if np.mod(t,20)==0: print(f\"time {t}\")\n",
    "        \n",
    "        #Call Variables and Store in a Dictionary\n",
    "        var_data_map = {\n",
    "            'w': W,\n",
    "            'qv': QV,\n",
    "            'qc_plus_qi': QCQI,\n",
    "            'th': TH,\n",
    "            'th_e': TH_E,\n",
    "            'buoyancy': BUOYANCY,\n",
    "            'HMC': HMC\n",
    "            }\n",
    "    \n",
    "        #Get the Lagrangian Parcel Grid Locations\n",
    "        if type=='general':\n",
    "            where_g=np.where(A_g[t,:]==1)\n",
    "            zs=Z[t,where_g[0]]\n",
    "        elif type=='cloudy':\n",
    "            where_c=np.where(A_c[t,:]==1)\n",
    "            zs=Z[t,where_c[0]]\n",
    "    \n",
    "        #For Each Variable, Link profile_array to profile_{var} and Add Profile Elements to Storage Array\n",
    "        for var in vars:\n",
    "            profile_array = globals()[f\"profile_{var}\"]\n",
    "            if type=='general':\n",
    "                var_data = var_data_map[var][t,where_g[0]]\n",
    "            elif type=='cloudy':\n",
    "                var_data = var_data_map[var][t,where_c[0]]\n",
    "            \n",
    "            np.add.at(profile_array[:, 0], zs, var_data)\n",
    "            np.add.at(profile_array[:, 1], zs, 1)\n",
    "    \n",
    "    #OUTPUTTING RESULTS       \n",
    "    if type=='general':\n",
    "        output_file=dir2+f'job_out/general_lagrangian_profiles_{res}_{t_res}_{Np_str}'\n",
    "    elif type=='cloudy':\n",
    "        output_file=dir2+f'job_out/cloudy_lagrangian_profiles_{res}_{t_res}_{Np_str}'\n",
    "    if job_array==True:\n",
    "        output_file+=f'_{job_id}.h5'\n",
    "    elif job_array==False:\n",
    "        output_file+=f'.h5'\n",
    "\n",
    "    print(f\"saving as {output_file}\")\n",
    "    import h5py\n",
    "    with h5py.File(output_file, 'w') as f:\n",
    "        f.create_dataset('profile_w', data=profile_w, compression=\"gzip\")\n",
    "        f.create_dataset('profile_qv', data=profile_qv, compression=\"gzip\")\n",
    "        f.create_dataset('profile_qc_plus_qi', data=profile_qc_plus_qi, compression=\"gzip\")\n",
    "        f.create_dataset('profile_th', data=profile_th, compression=\"gzip\")\n",
    "        f.create_dataset('profile_th_e', data=profile_th_e, compression=\"gzip\")\n",
    "        f.create_dataset('profile_buoyancy', data=profile_buoyancy, compression=\"gzip\")\n",
    "        f.create_dataset('profile_HMC', data=profile_HMC, compression=\"gzip\")\n",
    "    print('done\\n')\n",
    "    return profile_w,profile_qv,profile_qc_plus_qi,profile_th,profile_th_e,profile_buoyancy,profile_HMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e71f6d-141b-4b51-bede-d2745dcfe77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUNNING\n",
    "[profile_w,profile_qv,profile_qc_plus_qi,profile_th,profile_th_e,profile_buoyancy,profile_HMC]=LagrangianProfiles(type='general')\n",
    "\n",
    "[profile_w,profile_qv,profile_qc_plus_qi,profile_th,profile_th_e,profile_buoyancy,profile_HMC]=LagrangianProfiles(type='cloudy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deef9c9-4c7a-4fda-9dfa-7396b919e919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1467f9b-2f5c-40e1-b59e-df3a108ae178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3048d08a-7eff-4515-8b17-b3d97fd3ce0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eace57-c1f5-427e-ab99-86c39b821afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "#RECOMBINE SEPERATE JOB_ARRAYS AFTER\n",
    "recombine=False #KEEP FALSE WHEN JOB ARRAY IS RUNNING\n",
    "# recombine=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58196976-18fc-4259-90c0-4a173c66a2a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if recombine==True:\n",
    "    dir2=dir+'Project_Algorithms/Domain_Profiles/'\n",
    "    \n",
    "    types=['general','cloudy']\n",
    "    for type in types:\n",
    "        #MAKING OUTPUT FILE PATH\n",
    "        if type == \"general\":\n",
    "            output_file = dir2+f'job_out/general_lagrangian_profiles_{res}_{t_res}_{Np_str}.h5' \n",
    "        elif type == \"cloudy\":\n",
    "            output_file = dir2+f'job_out/cloudy_lagrangian_profiles_{res}_{t_res}_{Np_str}.h5'\n",
    "        \n",
    "        #MAKING PROFILES DICTIONARY\n",
    "        zhs = data['zh'].values\n",
    "        profiles = {}  # Store profiles for all variables\n",
    "        vars_list = ['w', 'qv', 'qc_plus_qi', 'th', 'th_e', 'buoyancy', 'HMC']\n",
    "        for var in vars_list:\n",
    "            profiles[var] = np.zeros((len(zhs), 3))  # column 1: var, column 2: counter, column 3: list of zhs\n",
    "            profiles[var][:, 2] = zhs \n",
    "        \n",
    "        num_jobs=300\n",
    "        for job_id in np.arange(1,num_jobs+1):\n",
    "            if np.mod(job_id,10)==0: print(f\"job_id = {job_id}\")\n",
    "    \n",
    "            #CALLING IN DATA\n",
    "            if type == \"general\":\n",
    "                input_file = dir2+f'job_out/general_lagrangian_profiles_{res}_{t_res}_{Np_str}_{job_id}.h5' \n",
    "            elif type == \"cloudy\":\n",
    "                input_file = dir2+f'job_out/cloudy_lagrangian_profiles_{res}_{t_res}_{Np_str}_{job_id}.h5'\n",
    "    \n",
    "            #COMPILING PROFILES\n",
    "            with h5py.File(input_file, 'r') as f:\n",
    "                for var in vars_list:  \n",
    "                    profiles[var][:,0:1+1]+=f[f'profile_{var}'][:,0:1+1]\n",
    "        \n",
    "        #SAVING INTO FINAL FORM\n",
    "        with h5py.File(output_file, 'w') as f:\n",
    "            for var in profiles:\n",
    "                profile_var = profiles[var]\n",
    "                f.create_dataset(f'profile_{var}', data=profile_var, compression=\"gzip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
