{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea60562-c790-4305-9459-d4d1e4fda244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS FUNCTION IS FOR RUNNING WITH SLURM JOB ARRAY\n",
    "#(SPLITS UP JOB_ARRAY BELOW INTO EVEN MORE TASKS)\n",
    "def StartSlurmJobArray(num_jobs,num_slurm_jobs, ISRUN):\n",
    "    job_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0)) #this is the current SBATCH job id\n",
    "    if job_id==0: job_id=50\n",
    "    if ISRUN==False:\n",
    "        start_job=1;end_job=num_jobs+1\n",
    "        return start_job,end_job\n",
    "    total_elements=num_jobs #total num of variables\n",
    "\n",
    "    job_range = total_elements // num_slurm_jobs  # Base size for each chunk\n",
    "    remaining = total_elements % num_slurm_jobs   # Number of chunks with 1 extra \n",
    "    \n",
    "    # Function to compute the start and end for each job_id\n",
    "    def get_job_range(job_id, num_slurm_jobs):\n",
    "        job_id-=1\n",
    "        # Add one extra element to the first 'remaining' chunks\n",
    "        start_job = job_id * job_range + min(job_id, remaining)\n",
    "        end_job = start_job + job_range + (1 if job_id < remaining else 0)\n",
    "    \n",
    "        if job_id == num_slurm_jobs - 1: \n",
    "            end_job = total_elements \n",
    "        return start_job, end_job\n",
    "    # def job_testing():\n",
    "    #     #TESTING\n",
    "    #     start=[];end=[]\n",
    "    #     for job_id in range(1,num_slurm_jobs+1):\n",
    "    #         start_job, end_job = get_job_range(job_id)\n",
    "    #         print(start_job,end_job)\n",
    "    #         start.append(start_job)\n",
    "    #         end.append(end_job)\n",
    "    #     print(np.all(start!=end))\n",
    "    #     print(len(np.unique(start))==len(start))\n",
    "    #     print(len(np.unique(end))==len(end))\n",
    "    # job_testing()\n",
    "    # if sbatch==True:\n",
    "        \n",
    "    start_job, end_job = get_job_range(job_id, num_slurm_jobs)\n",
    "    index_adjust=start_job\n",
    "    # print(f'start_job = {start_job}, end_job = {end_job}')\n",
    "    if start_job==0: start_job=1\n",
    "    if end_job==total_elements: end_job+=1\n",
    "    return start_job,end_job\n",
    "\n",
    "# job_id=1\n",
    "# [start_slurm_job,end_slurm_job,slurm_index_adjust]=StartSlurmJobArray(num_jobs,num_slurm_jobs,ISRUN)\n",
    "# parcel=parcel1.isel(xh=slice(start_job,end_job))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b39499-34af-4937-afee-d07e9183d6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in Packages and Data\n",
    "\n",
    "#Importing Packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import matplotlib.gridspec as gridspec\n",
    "import xarray as xr\n",
    "import os; import time\n",
    "import pickle\n",
    "import h5py\n",
    "###############################################################\n",
    "def coefs(coefficients,degree):\n",
    "    coef=coefficients\n",
    "    coefs=\"\"\n",
    "    for n in range(degree, -1, -1):\n",
    "        string=f\"({coefficients[len(coef)-(n+1)]:.1e})\"\n",
    "        coefs+=string + f\"x^{n}\"\n",
    "        if n != 0:\n",
    "            coefs+=\" + \"\n",
    "    return coefs\n",
    "###############################################################\n",
    "\n",
    "# Importing Model Data\n",
    "check=False\n",
    "dir='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "\n",
    "# # dx = 1 km; Np = 1M; Nt = 5 min\n",
    "# data1=xr.open_dataset(dir+'../cm1r20.3/run/cm1out_1km_5min.nc') #***\n",
    "# parcel1=xr.open_dataset(dir+'../cm1r20.3/run/cm1out_pdata_1km_5min_1e6.nc') #***\n",
    "# res='1km';t_res='5min'\n",
    "# Np_str='1e6'\n",
    "\n",
    "# # dx = 1km; Np = 50M\n",
    "# #Importing Model Data\n",
    "# dir2='/home/air673/koa_scratch/'\n",
    "# data1=xr.open_dataset(dir2+'cm1out_1km_1min.nc') #***\n",
    "# parcel1=xr.open_dataset(dir2+'cm1out_pdata_1km_1min_50M.nc') #***\n",
    "# res='1km'; t_res='1min'; Np_str='50e6'\n",
    "\n",
    "# dx = 1km; Np = 50M; Nz = 95\n",
    "#Importing Model Data\n",
    "dir2='/home/air673/koa_scratch/'\n",
    "data1=xr.open_dataset(dir2+'cm1out_1km_1min_95nz.nc') #***\n",
    "parcel1=xr.open_dataset(dir2+'cm1out_pdata_1km_1min_95nz.nc') #***\n",
    "res='1km'; t_res='1min_95nz'; Np_str='50e6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58a6dec-11f3-4a90-99b0-9f553357300a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADING IN H5 VARIABLES\n",
    "PROCESSING=False\n",
    "PROCESSING=True\n",
    "\n",
    "print('loading vars')\n",
    "dir2=dir+'Project_Algorithms/Entrainment/job_out_3/'\n",
    "dir3=dir+'Project_Algorithms/Entrainment/'\n",
    "\n",
    "if PROCESSING==False:\n",
    "    file_path=dir3+f'3D_entrainmentdetrainment_profiles_{res}_{t_res}_{Np_str}.h5'\n",
    "elif PROCESSING==True:\n",
    "    file_path=dir3+f'3D_entrainmentdetrainment_profiles_PREPROCESSING_{res}_{t_res}_{Np_str}.h5'\n",
    "        \n",
    "ED_ds1 = xr.open_dataset(file_path, phony_dims='sort',engine='h5netcdf')  # or engine='netcdf4'\n",
    "ED_ds1 = ED_ds1.rename({\n",
    "    'phony_dim_0': 'time',\n",
    "    'phony_dim_1': 'zh',\n",
    "    'phony_dim_2': 'yh',\n",
    "    'phony_dim_3': 'xh'\n",
    "})\n",
    "\n",
    "file_path = dir + 'Variable_Calculation/' + f'Eulerian_Binary_Array_{res}_{t_res}.h5'\n",
    "A_ds1 = xr.open_dataset(file_path, phony_dims='sort',engine='h5netcdf')  # or engine='netcdf4'\n",
    "A_ds1 = A_ds1.rename({\n",
    "    'phony_dim_0': 'time',\n",
    "    'phony_dim_1': 'zh',\n",
    "    'phony_dim_2': 'yh',\n",
    "    'phony_dim_3': 'xh'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506b952a-76c0-4625-a9f0-8457335b0fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "dir2='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "path=dir2+'../Functions/'\n",
    "sys.path.append(path)\n",
    "\n",
    "import NumericalFunctions\n",
    "from NumericalFunctions import * # import NumericalFunctions \n",
    "import PlottingFunctions\n",
    "from PlottingFunctions import * # import PlottingFunctions\n",
    "\n",
    "\n",
    "# # Get all functions in NumericalFunctions\n",
    "# import inspect\n",
    "# functions = [f[0] for f in inspect.getmembers(NumericalFunctions, inspect.isfunction)]\n",
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e47aae-5094-4285-8385-fb8892f786cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DOMAIN SUBSETTING\n",
    "############################################################\n",
    "\n",
    "#FINDING NEW LEFT T-BOUNDARY\n",
    "dt=data1['time'][1].item()/1e9 #seconds per timestep\n",
    "dhours=(dt/60**2) #hours per timestep\n",
    "start_hour=4 #10:00 am\n",
    "t_start=int(start_hour/dhours)\n",
    "#FINDING NEW RIGHT T-BOUNDARY\n",
    "end_hour=11 #5pm\n",
    "t_end=int(end_hour/dhours)\n",
    "#PRINTING\n",
    "print(f't in {t_start}:{t_end}')\n",
    "\n",
    "\n",
    "#FINDING NEW TOP Z-BOUNDARY\n",
    "dzh=data1['zh']\n",
    "zh_bottom=0; zh_top=np.where(dzh>=19)[0][0]\n",
    "dzf=data1['zf']\n",
    "zf_bottom=0; zf_top=np.where(dzf>=20)[0][0]\n",
    "#PRINTING\n",
    "print(f'zh_top at {zh_bottom}:{zh_top}')\n",
    "\n",
    "#FINDING THE NEW LEFT X-BOUNDARY\n",
    "ocean_percent=2/8\n",
    "left_to_coast=data1['xh'][0]+(data1['xh'][-1]-data1['xh'][0])*ocean_percent\n",
    "where_coast_xh=np.where(data1['xh']>=left_to_coast)[0][0]#-25\n",
    "where_coast_xf=np.where(data1['xf']>=left_to_coast)[0][0]#-25\n",
    "#FINDING THE NEW RIGHT X-BOUNDARY\n",
    "right_fraction=80/100\n",
    "xf=data1['xf']-data1['xf'][0]\n",
    "Nxf=len(xf)\n",
    "end_xf=np.where(xf>Nxf*right_fraction)[0][0]\n",
    "xh=data1['xh']-data1['xh'][0]\n",
    "Nxh=len(xh)\n",
    "end_xh=np.where(xh>Nxh*right_fraction)[0][0]\n",
    "#PRINTING\n",
    "# print(f'x in {0}:{where_coast_xh-1} FOR SEA')\n",
    "print(f'x in {where_coast_xh}:{end_xh} FOR LAND')\n",
    "\n",
    "\n",
    "#SUBSETTING CODE\n",
    "data1=data1.isel(time=slice(t_start,None),zh=slice(zh_bottom,zh_top),zf=slice(zf_bottom,zf_top),xh=slice(where_coast_xh,end_xh+1),xf=slice(where_coast_xf,end_xf+1))\n",
    "A_ds1=A_ds1.isel(time=slice(t_start,None),zh=slice(zh_bottom,zh_top),xh=slice(where_coast_xh,end_xh+1))\n",
    "ED_ds1=ED_ds1.isel(time=slice(t_start,None),zh=slice(zh_bottom,zh_top),xh=slice(where_coast_xh,end_xh+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2672896-4e13-42f3-8593-6abf84e9ca13",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c47b32a-d08c-4862-9836-978f472e2013",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "#JOB ARRAY SETUP\n",
    "################################\n",
    "#*#*\n",
    "# how many total jobs are being run? i.e. array=1-100 ==> num_jobs=100\n",
    "if '1e6' in Np_str:\n",
    "    num_jobs=60 #1M parcels\n",
    "if '50e6' in Np_str:\n",
    "    num_jobs=200 #50M parcels\n",
    "num_slurm_jobs=60\n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b43ffc-b2d9-4b96-8fa8-a7533bbfdc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "#DATA LOADING FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38291f1-169f-4fe1-be13-2af7abd5b0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JOB ARRAY SETUP\n",
    "def StartJobArray(job_id,num_jobs):\n",
    "    total_elements=len(data1['time']) #total num of variables\n",
    "\n",
    "    if num_jobs >= total_elements:\n",
    "        raise ValueError(\"Number of jobs cannot be greater than or equal to total elements.\")\n",
    "    \n",
    "    job_range = total_elements // num_jobs  # Base size for each chunk\n",
    "    remaining = total_elements % num_jobs   # Number of chunks with 1 extra \n",
    "    \n",
    "    # Function to compute the start and end for each job_id\n",
    "    def get_job_range(job_id, num_jobs):\n",
    "        job_id-=1\n",
    "        # Add one extra element to the first 'remaining' chunks\n",
    "        start_job = job_id * job_range + min(job_id, remaining)\n",
    "        end_job = start_job + job_range + (1 if job_id < remaining else 0)\n",
    "    \n",
    "        if job_id == num_jobs - 1: \n",
    "            end_job = total_elements #- 1\n",
    "        return start_job, end_job\n",
    "    # def job_testing():\n",
    "    #     #TESTING\n",
    "    #     start=[];end=[]\n",
    "    #     for job_id in range(1,num_jobs+1):\n",
    "    #         start_job, end_job = get_job_range(job_id)\n",
    "    #         print(start_job,end_job)\n",
    "    #         start.append(start_job)\n",
    "    #         end.append(end_job)\n",
    "    #     print(np.all(start!=end))\n",
    "    #     print(len(np.unique(start))==len(start))\n",
    "    #     print(len(np.unique(end))==len(end))\n",
    "    # job_testing()\n",
    "\n",
    "    # if sbatch==True:\n",
    "    #     job_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0)) #this is the current SBATCH job id\n",
    "    #     if job_id==0: job_id=1\n",
    "        \n",
    "    start_job, end_job = get_job_range(job_id, num_jobs)\n",
    "    index_adjust=start_job\n",
    "    # print(f'start_job = {start_job}, end_job = {end_job}')\n",
    "    return start_job,end_job,index_adjust\n",
    "# job_id=1\n",
    "# [start_job,end_job,index_adjust]=StartJobArray(job_id,num_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d77403e-a491-4a71-9aa4-5e0f2166abc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetData(ED_ds1,start_job,end_job):\n",
    "    #Indexing Array with JobArray\n",
    "    ED_ds=ED_ds1.isel(time=slice(start_job,end_job))\n",
    "    return ED_ds\n",
    "# ED_ds=GetData(ED_ds1,start_job,end_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c71d95f-dc65-438d-8a64-c9b2fef4c8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetA(A_ds1,start_job,end_job):\n",
    "    A_ds=A_ds1.isel(time=slice(start_job,end_job))\n",
    "    A_g=A_ds['A_g'].data; A_c=A_ds['A_c'].data\n",
    "    return A_g,A_c\n",
    "# [A_g,A_c]=GetA(A_ds1,start_job,end_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c5c310-130a-4776-beda-2412e8b3f54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants\n",
    "Cp=1004 #Jkg-1K-1\n",
    "Cv=717 #Jkg-1K-1\n",
    "Rd=Cp-Cv #Jkg-1K-1\n",
    "eps=0.608\n",
    "\n",
    "Lx=(data1['xf'][-1].item()-data1['xf'][0].item())*1000 #x length (m)\n",
    "Ly=(data1['yf'][-1].item()-data1['yf'][0].item())*1000 #y length (m)\n",
    "Np=len(parcel1['xh']) #number of lagrangian parcles\n",
    "dt=(data1['time'][1]-data1['time'][0]).item()/1e9 #sec\n",
    "dx=(data1['xf'][1].item()-data1['xf'][0].item())*1e3 #meters\n",
    "dy=(data1['yf'][1].item()-data1['yf'][0].item())*1e3 #meters\n",
    "xs=data1['xf'].values*1000\n",
    "ys=data1['yf'].values*1000\n",
    "zs=data1['zf'].values*1000\n",
    "\n",
    "def zf(z):\n",
    "    k=z #z is the # level of z\n",
    "    out=data1['zf'].values[k]*1000\n",
    "    \n",
    "    return out\n",
    "# def rho(x,y,z,t):\n",
    "#     p=data1['prs'].isel(xh=x,yh=y,zh=z,time=t).item()\n",
    "#     p0=101325 #Pa\n",
    "#     theta=data1['th'].isel(xh=x,yh=y,zh=z,time=t).item()\n",
    "#     T=theta*(p/p0)**(Rd/Cp)\n",
    "#     qv=data1['qv'].isel(xh=x,yh=y,zh=z,time=t).item()\n",
    "#     # Tv=T*(1+eps*qv)\n",
    "#     Tv=T*(eps+qv)/(eps*(1+qv))\n",
    "#     rho = p/(Rd*Tv)\n",
    "#     out=rho\n",
    "#     return out\n",
    "\n",
    "def rho(x,y,z,rho_data_t):\n",
    "    out=rho_data_t[z,y,x]\n",
    "    return out\n",
    "def m(t):\n",
    "    rho_data_t=data1['rho'].isel(time=t).data\n",
    "    \n",
    "    m=0\n",
    "    #triple sum\n",
    "    for k in range(len(data1['zh'])):\n",
    "        dz=(zf(k+1)-zf(k))\n",
    "        for j in range(len(data1['yh'])):\n",
    "            for i in range(len(data1['xh'])):\n",
    "                rho_out=rho(i,j,k,rho_data_t)\n",
    "                m+=rho_out*dz\n",
    "\n",
    "    #triple sum\n",
    "    out=m*dx*dy/Np\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73736a5f-5485-4b40-9d19-df359d6e41a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Calculate Mass Constant\n",
    "# calculate='single_time'\n",
    "# calculate=True\n",
    "calculate=False\n",
    "\n",
    "if calculate==True:\n",
    "    Nt=len(data1['time'])\n",
    "    m_arr=np.zeros((Nt))\n",
    "    for t in np.arange(Nt):\n",
    "        if np.mod(t,25)==0: print(t)\n",
    "        m_arr[t]=m(t)\n",
    "    dir3=dir+f'Project_Algorithms/Entrainment/'\n",
    "    np.save(dir3+f'Mass_Array_{res}_{t_res}_{Np_str}.npy', m_arr)\n",
    "elif calculate=='single_time':\n",
    "    Nt=len(data1['time'])\n",
    "    m_arr=np.zeros((Nt))\n",
    "\n",
    "    t=0 #len(data1['time'])//2 #Pick some middle time\n",
    "    m_300=m(t)\n",
    "    for t in np.arange(Nt):\n",
    "        m_arr[t]=m_300 #UNCOMMENT FOR FULL CALCULATION\n",
    "    dir3=dir+f'Project_Algorithms/Entrainment/'\n",
    "    np.save(dir3+f'Mass_Array_{res}_{t_res}_{Np_str}.npy', m_arr)\n",
    "else:\n",
    "    dir3=dir+f'Project_Algorithms/Entrainment/'\n",
    "    m_arr = np.load(dir3+f'Mass_Array_{res}_{t_res}_{Np_str}.npy')\n",
    "\n",
    "# # TESTING\n",
    "# lst=[]\n",
    "# for t in np.arange(133):\n",
    "#     lst.append(m_arr[t])\n",
    "\n",
    "# plt.plot(lst)\n",
    "# (np.max(lst)-np.min(lst))*100/np.mean(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3602bf2d-c363-4191-b031-29e7a3a55e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_constant(profile_array,apply):\n",
    "    if apply==True:\n",
    "        Nt=profile_array.shape[0]\n",
    "        Nz=profile_array.shape[1]\n",
    "    \n",
    "        profile_array/=(dx*dy*dt)\n",
    "        for t in np.arange(Nt):\n",
    "            profile_array[t]*=m_arr[t+index_adjust]\n",
    "        for z in np.arange(Nz):\n",
    "            dz=zf(z+1)-zf(z)\n",
    "            profile_array[:,z]/=dz\n",
    "    # print(dx,dy,dz,dt,m_arr[t+index_adjust],Nt,Nz) #TESTING\n",
    "    return profile_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbe2b0f-b284-4954-ab2d-521824e0b007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CallVariables(ED_ds):\n",
    "    \n",
    "    profile_array_e_g = ED_ds[\"profile_array_e_g\"].data\n",
    "    profile_array_e_c = ED_ds[\"profile_array_e_c\"].data\n",
    "    profile_array_d_g = ED_ds[\"profile_array_d_g\"].data\n",
    "    profile_array_d_c = ED_ds[\"profile_array_d_c\"].data\n",
    "    \n",
    "    # APPLY CONSTANTS TO ENTRAINMENT/DETRAINMENT VALUES\n",
    "    profile_array_e_g = apply_constant(profile_array_e_g, apply=True)\n",
    "    profile_array_e_c = apply_constant(profile_array_e_c, apply=True)\n",
    "    profile_array_d_g = -apply_constant(profile_array_d_g, apply=True)\n",
    "    profile_array_d_c = -apply_constant(profile_array_d_c, apply=True)\n",
    "    profile_array_net_g = profile_array_e_g - profile_array_d_g\n",
    "    profile_array_net_c = profile_array_e_c - profile_array_d_c\n",
    "\n",
    "    VARs = {\n",
    "        \"profile_array_e_g\": profile_array_e_g,\n",
    "        \"profile_array_e_c\": profile_array_e_c,\n",
    "        \"profile_array_d_g\": profile_array_d_g,\n",
    "        \"profile_array_d_c\": profile_array_d_c,\n",
    "        \"profile_array_net_g\": profile_array_net_g,\n",
    "        \"profile_array_net_c\": profile_array_net_c\n",
    "    }\n",
    "    return VARs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463223d6-e7fb-4a44-b4fc-f7eea88aa473",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "#Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7851e769-c67c-493f-a84c-7d7ed1cd37a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thresholds\n",
    "w_thresh1 = 0.1\n",
    "w_thresh2 = 0.5\n",
    "qcqi_thresh = 1e-6\n",
    "\n",
    "def DomainProfile(VARs,data_type, A_g,A_c):\n",
    "    zhs = data1['zh'].values\n",
    "    profiles = {}  # Store profiles for all variables\n",
    "\n",
    "    # Initialize profiles for each variable\n",
    "    for var in VARs:\n",
    "        profiles[var] = np.zeros((len(zhs), 3))  # column 1: var, column 2: counter, column 3: list of zhs\n",
    "        profiles[var][:, 2] = zhs\n",
    "\n",
    "    # Threshold mask\n",
    "    if data_type == \"general\":\n",
    "        where_updraft = (A_g==True)\n",
    "    elif data_type == \"cloudy\":\n",
    "        where_updraft = (A_c==True)\n",
    "    t_ind, z_ind, y_ind, x_ind = np.where(where_updraft)\n",
    "\n",
    "    # Variable selection dictionary\n",
    "\n",
    "    # Iterate over each variable in var_names and bin the data\n",
    "    for var in VARs:\n",
    "        masked_data = VARs[var][where_updraft]\n",
    "        np.add.at(profiles[var][:, 0], z_ind, masked_data)\n",
    "        np.add.at(profiles[var][:, 1], z_ind, 1)\n",
    "\n",
    "        # if var=='profile_array_d_c': #TESTING\n",
    "        #     print(1)\n",
    "        #     print(np.where(where_updraft))\n",
    "        #     print(z_ind)\n",
    "        #     print(VARs[var][where_updraft])\n",
    "        #     print(np.where(VARs[var][where_updraft]))\n",
    "\n",
    "    return profiles\n",
    "# profiles=DomainProfile(VARs,data_type,A_g,A_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eea278-1264-470c-8078-2a11a4595fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveProfile(Dictionary, data_type, job_id):\n",
    "    dir2=dir+'Project_Algorithms/Domain_Profiles/'\n",
    "    if data_type == \"general\":\n",
    "        output_file = dir2+f'job_out/general_eulerian_entrainment_profiles_{res}_{t_res}_{Np_str}'\n",
    "    elif data_type == \"cloudy\":\n",
    "        output_file = dir2+f'job_out/cloudy_eulerian_entrainment_profiles_{res}_{t_res}_{Np_str}'\n",
    "    output_file+=f'_{job_id}.h5'\n",
    "    \n",
    "    with h5py.File(output_file, 'w') as f:\n",
    "        for var in Dictionary:\n",
    "            profile_var = Dictionary[var]\n",
    "            f.create_dataset(f'{var}', data=profile_var, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ef7e90-4686-45ae-b163-bcb92ff3bc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "#Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f62556-0857-4335-840f-7aea704c963b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[start_slurm_job,end_slurm_job]=StartSlurmJobArray(num_jobs=num_jobs,num_slurm_jobs=num_slurm_jobs,ISRUN=True) #if ISRUN is False, then will not run using slurm_job_array\n",
    "\n",
    "print(f\"Running on Slurm_Jobs for Slurm_Job_Ids: {(start_slurm_job,end_slurm_job-1)}\")\n",
    "\n",
    "job_id_list=np.arange(start_slurm_job,end_slurm_job)\n",
    "for job_id in job_id_list:\n",
    "    if job_id % 1 == 0: print(f'current job_id = {job_id}')\n",
    "    [start_job,end_job,index_adjust]=StartJobArray(job_id,num_jobs)\n",
    "\n",
    "    #SLICING DATA\n",
    "    ED_ds=GetData(ED_ds1,start_job,end_job)\n",
    "    [A_g,A_c]=GetA(A_ds1,start_job,end_job)\n",
    "\n",
    "    #GETTING VARIABLES DICTIONARY\n",
    "    VARs=CallVariables(ED_ds)\n",
    "    \n",
    "    for data_type in ['general','cloudy']:\n",
    "        #GETTING DATA AND PUTTING IN A DICTIONARY    \n",
    "        Dictionary=DomainProfile(VARs,data_type,A_g,A_c)\n",
    "        SaveProfile(Dictionary,data_type,job_id)\n",
    "        # check_memory(globals())\n",
    "        del Dictionary\n",
    "    del VARs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9f83bc-91fe-4564-8a13-c7dafe46db7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "#RECOMBINE SEPERATE JOB_ARRAYS AFTER\n",
    "recombine=False #KEEP FALSE WHEN JOB ARRAY IS RUNNING\n",
    "# recombine=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4be274b-b52e-43da-84ca-34a3f80cc678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recombine(num_jobs):\n",
    "    dir2=dir+'Project_Algorithms/Domain_Profiles/'\n",
    "    var_names = ['profile_array_e_g','profile_array_e_c',\n",
    "             'profile_array_d_g','profile_array_d_c',\n",
    "             'profile_array_net_g','profile_array_net_c']\n",
    "    \n",
    "    data_types=['general','cloudy']\n",
    "    for data_type in data_types:\n",
    "        #MAKING OUTPUT FILE PATH\n",
    "        if data_type == \"general\":\n",
    "            output_file = dir2+f'job_out/general_eulerian_entrainment_profiles_{res}_{t_res}_{Np_str}.h5' \n",
    "        elif data_type == \"cloudy\":\n",
    "            output_file = dir2+f'job_out/cloudy_eulerian_entrainment_profiles_{res}_{t_res}_{Np_str}.h5'\n",
    "        \n",
    "        #MAKING PROFILES DICTIONARY\n",
    "        zhs = data1['zh'].values\n",
    "        profiles = {}  #  Store profiles for all variables\n",
    "        for var in var_names:\n",
    "            profiles[var] = np.zeros((len(zhs), 3))  # column 1: var, column 2: counter, column 3: list of zhs\n",
    "            profiles[var][:, 2] = zhs \n",
    "\n",
    "        for job_id in np.arange(1,num_jobs+1):\n",
    "            if np.mod(job_id,10)==0: print(f\"job_id = {job_id}\")\n",
    "            #CALLING IN DATA\n",
    "            if data_type == \"general\":\n",
    "                input_file = dir2+f'job_out/general_eulerian_entrainment_profiles_{res}_{t_res}_{Np_str}_{job_id}.h5' \n",
    "            elif data_type == \"cloudy\":\n",
    "                input_file = dir2+f'job_out/cloudy_eulerian_entrainment_profiles_{res}_{t_res}_{Np_str}_{job_id}.h5'\n",
    "    \n",
    "            #COMPILING PROFILES\n",
    "            with h5py.File(input_file, 'r') as f:\n",
    "                for var in var_names:  \n",
    "                    profiles[var][:,0:1+1]+=f[f'{var}'][:,0:1+1]\n",
    "        \n",
    "        #SAVING INTO FINAL FORM\n",
    "        with h5py.File(output_file, 'w') as f:\n",
    "            for var in profiles:\n",
    "                profile_var = profiles[var]\n",
    "                f.create_dataset(f'{var}', data=profile_var, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c248898-04d4-44e8-8286-afb2a03986a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if recombine==True:\n",
    "    Recombine(num_jobs=num_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73d86f4-860f-49a8-9c0e-e7a77df6fdc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9bbe06-67c3-4cfa-80df-e8702c5b52ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
