{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e060f99-930f-46c0-a15c-1523220ea183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TrackingAlgorithms_DataLoading_Class\n",
    "# ============================================================\n",
    "\n",
    "#Libraries\n",
    "import os\n",
    "import h5py \n",
    "\n",
    "class TrackingAlgorithms_DataLoading_Class:\n",
    "    \"\"\"\n",
    "    A utility class for saving and loading Tracking Algorithm results\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def SaveData(ModelData,DataManager, Dictionary, timeString): \n",
    "        \"\"\"\n",
    "        Save tracking algorithm results to an HDF5 file.\n",
    "        \"\"\"\n",
    "        \n",
    "        fileName = f\"{DataManager.dataName}_{ModelData.res}_{ModelData.t_res}_{ModelData.Nz_str}nz_{timeString}.h5\"\n",
    "        filePath = os.path.join(DataManager.outputDataDirectory,fileName)\n",
    "        \n",
    "    \n",
    "        with h5py.File(filePath, 'w') as f:\n",
    "            for varName, varData in Dictionary.items():\n",
    "                f.create_dataset(f\"{varName}\", data=varData, compression=\"gzip\")\n",
    "    \n",
    "        print(f\"Saved output to {filePath}\",\"\\n\")\n",
    "\n",
    "    @staticmethod\n",
    "    def LoadData(ModelData, DataManager, timeString,\n",
    "                 dataName=None,outputDataDirectory=None,\n",
    "                 printstatement=True):\n",
    "        \"\"\"\n",
    "        Load tracking algorithm results from an HDF5 file.\n",
    "        \"\"\"\n",
    "        if dataName is None:\n",
    "            dataName = DataManager.dataName\n",
    "        if outputDataDirectory is None:\n",
    "            outputDataDirectory = DataManager.outputDataDirectory\n",
    "        \n",
    "        fileName = f\"{dataName}_{ModelData.res}_{ModelData.t_res}_{ModelData.Nz_str}nz_{timeString}.h5\"\n",
    "        filePath = os.path.join(outputDataDirectory, fileName)\n",
    "\n",
    "        if not os.path.exists(filePath):\n",
    "            raise FileNotFoundError(f\"HDF5 file not found:\\n{filePath}\")\n",
    "\n",
    "        Dictionary = {}\n",
    "        with h5py.File(filePath, 'r') as f:\n",
    "            for key in f.keys():\n",
    "                Dictionary[key] = f[key][:]\n",
    "\n",
    "        if printstatement == True:\n",
    "            print(f\"Loaded data from {filePath} ({len(Dictionary)} variables)\\n\")\n",
    "        return Dictionary\n",
    "\n",
    "# #HOW TO LOAD\n",
    "# #IMPORT CLASSES\n",
    "# sys.path.append(os.path.join(mainCodeDirectory,\"3_Project_Algorithms\",\"2_Tracking_Algorithms\"))\n",
    "# from CLASSES_TrackingAlgorithms import TrackingAlgorithms_DataLoading_Class\n",
    "        \n",
    "# #EXAMPLE USAGE\n",
    "# TrackingAlgorithms_DataLoading_Class.SaveData(ModelData,DataManager, Dictionary, timeString)\n",
    "# Dictionary = TrackingAlgorithms_DataLoading_Class.LoadData(ModelData,DataManager, timeString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3b8c83-69d2-409e-b24f-626c5b6ae054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SlurmJobArray_Class\n",
    "# ============================================================\n",
    "\n",
    "class SlurmJobArray_Class:\n",
    "    \n",
    "    @staticmethod\n",
    "    def StartSlurmJobArray(num_jobs,num_slurm_jobs, ISRUN):\n",
    "        job_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0)) #this is the current SBATCH job id\n",
    "        if job_id==0: job_id=1\n",
    "        if ISRUN==False:\n",
    "            start_job=1;end_job=num_jobs+1\n",
    "            return start_job,end_job\n",
    "        total_elements=num_jobs #total num of variables\n",
    "    \n",
    "        job_range = total_elements // num_slurm_jobs  # Base size for each chunk\n",
    "        remaining = total_elements % num_slurm_jobs   # Number of chunks with 1 extra \n",
    "        \n",
    "        # Function to compute the start and end for each job_id\n",
    "        def get_job_range(job_id, num_slurm_jobs):\n",
    "            job_id-=1\n",
    "            # Add one extra element to the first 'remaining' chunks\n",
    "            start_job = job_id * job_range + min(job_id, remaining)\n",
    "            end_job = start_job + job_range + (1 if job_id < remaining else 0)\n",
    "        \n",
    "            if job_id == num_slurm_jobs - 1: \n",
    "                end_job = total_elements \n",
    "            return start_job, end_job\n",
    "        # def job_testing():\n",
    "        #     #TESTING\n",
    "        #     start=[];end=[]\n",
    "        #     for job_id in range(1,num_slurm_jobs+1):\n",
    "        #         start_job, end_job = get_job_range(job_id)\n",
    "        #         print(start_job,end_job)\n",
    "        #         start.append(start_job)\n",
    "        #         end.append(end_job)\n",
    "        #     print(np.all(start!=end))\n",
    "        #     print(len(np.unique(start))==len(start))\n",
    "        #     print(len(np.unique(end))==len(end))\n",
    "        # job_testing()\n",
    "        # if sbatch==True:\n",
    "            \n",
    "        start_job, end_job = get_job_range(job_id, num_slurm_jobs)\n",
    "        index_adjust=start_job\n",
    "        # print(f'start_job = {start_job}, end_job = {end_job}')\n",
    "        if start_job==0: start_job=1\n",
    "        if end_job==total_elements: end_job+=1\n",
    "        return start_job,end_job\n",
    "    \n",
    "    @staticmethod\n",
    "    def StartJobArray(ModelData, job_id,num_jobs):\n",
    "        total_elements=ModelData.Np #total num of variables\n",
    "    \n",
    "        if num_jobs >= total_elements:\n",
    "            raise ValueError(\"Number of jobs cannot be greater than or equal to total elements.\")\n",
    "        \n",
    "        job_range = total_elements // num_jobs  # Base size for each chunk\n",
    "        remaining = total_elements % num_jobs   # Number of chunks with 1 extra \n",
    "        \n",
    "        # Function to compute the start and end for each job_id\n",
    "        def get_job_range(job_id, num_jobs):\n",
    "            job_id-=1\n",
    "            # Add one extra element to the first 'remaining' chunks\n",
    "            start_job = job_id * job_range + min(job_id, remaining)\n",
    "            end_job = start_job + job_range + (1 if job_id < remaining else 0)\n",
    "        \n",
    "            if job_id == num_jobs - 1: \n",
    "                end_job = total_elements #- 1\n",
    "            return start_job, end_job\n",
    "        # def job_testing():\n",
    "        #     #TESTING\n",
    "        #     start=[];end=[]\n",
    "        #     for job_id in range(1,num_jobs+1):\n",
    "        #         start_job, end_job = get_job_range(job_id)\n",
    "        #         print(start_job,end_job)\n",
    "        #         start.append(start_job)\n",
    "        #         end.append(end_job)\n",
    "        #     print(np.all(start!=end))\n",
    "        #     print(len(np.unique(start))==len(start))\n",
    "        #     print(len(np.unique(end))==len(end))\n",
    "        # job_testing()\n",
    "    \n",
    "        # if sbatch==True:\n",
    "        #     job_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0)) #this is the current SBATCH job id\n",
    "        #     if job_id==0: job_id=1\n",
    "            \n",
    "        start_job, end_job = get_job_range(job_id, num_jobs)\n",
    "        index_adjust=start_job\n",
    "        # print(f'start_job = {start_job}, end_job = {end_job}')\n",
    "        return start_job,end_job,index_adjust\n",
    "    \n",
    "    @staticmethod\n",
    "    def job_filter(arr, start_job,end_job):\n",
    "        return arr[(arr[:,0]>=start_job)&(arr[:,0]<end_job)]\n",
    "    \n",
    "    @staticmethod\n",
    "    def ApplyJobArray_Nested(trackedArrays, start_job, end_job):\n",
    "        \"\"\"\n",
    "        Apply job-array filtering to all arrays inside the nested trackedArrays dictionary\n",
    "        \"\"\"\n",
    "    \n",
    "        trackedArrays_filtered = {}\n",
    "    \n",
    "        for main_key, sub_dict in trackedArrays.items():\n",
    "            trackedArrays_filtered[main_key] = {}\n",
    "    \n",
    "            for sub_key, arr in sub_dict.items():\n",
    "                # Apply job filtering\n",
    "                filteredArray = job_filter(arr, start_job, end_job)\n",
    "                trackedArrays_filtered[main_key][sub_key] = filteredArray\n",
    "    \n",
    "        print(f\"Completed job filter for {len(trackedArrays_filtered)} main categories ({start_job} â†’ {end_job})\")\n",
    "        return trackedArrays_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4f4b5b-27cf-44b0-ad1b-65867131f6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Results_InputOutput_Class\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "class Results_InputOutput_Class:\n",
    "    \"\"\"\n",
    "    A static utility class for saving and loading tracking algorithm results.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def SaveOutFile(ModelData,DataManager, Dictionary,job_id): \n",
    "        \"\"\"\n",
    "        Save tracking algorithm results to an HDF5 file.\n",
    "        \"\"\"\n",
    "        \n",
    "        fileName = f\"{DataManager.dataName}_{ModelData.res}_{ModelData.t_res}_{ModelData.Nz_str}nz_job{job_id}.h5\"\n",
    "        filePath = os.path.join(DataManager.outputDataDirectory,fileName)\n",
    "        \n",
    "    \n",
    "        with h5py.File(filePath, 'w') as f:\n",
    "            for varName, varData in Dictionary.items():\n",
    "                f.create_dataset(f\"{varName}\", data=varData, compression=\"gzip\")\n",
    "    \n",
    "        print(f\"Saved output to {filePath}\",\"\\n\")\n",
    "\n",
    "    @staticmethod\n",
    "    def LoadOutFile(ModelData, DataManager, job_id, varName=None, printstatement=False): \n",
    "        \"\"\"\n",
    "        Load tracking algorithm results from an HDF5 file and return as a dictionary.\n",
    "        \"\"\"\n",
    "    \n",
    "        fileName = f\"{DataManager.dataName}_{ModelData.res}_{ModelData.t_res}_{ModelData.Nz_str}nz_job{job_id}.h5\"\n",
    "        filePath = os.path.join(DataManager.outputDataDirectory, fileName)\n",
    "    \n",
    "        if printstatement==True:\n",
    "            print(f\"Loading output from {filePath}\\n\")\n",
    "    \n",
    "        Dictionary = {}\n",
    "        with h5py.File(filePath, 'r') as f:\n",
    "            if varName is None:\n",
    "                # Load all variables\n",
    "                Dictionary = {name: f[name][:] for name in f.keys()}\n",
    "                return Dictionary\n",
    "            else:\n",
    "                if varName not in f:\n",
    "                    raise KeyError(f\"{varName} not found in {filePath}\")\n",
    "                arr = f[varName][:]\n",
    "                return arr\n",
    "\n",
    "    @staticmethod\n",
    "    def SaveAllCloudBase_Job(ModelData,DataManager,\n",
    "                             all_cloudbase,job_id):\n",
    "        Dictionary = {\"all_cloudbase\": all_cloudbase}\n",
    "        Results_InputOutput_Class.SaveOutFile(ModelData,DataManager, Dictionary,f\"{job_id}_all_cloudbase\")\n",
    "\n",
    "    @staticmethod\n",
    "    def SaveAllCloudBase_Combined(ModelData,DataManager,\n",
    "                                  all_cloudbase):\n",
    "        Dictionary = {\"all_cloudbase\": all_cloudbase}\n",
    "        Results_InputOutput_Class.SaveOutFile(ModelData,DataManager, Dictionary,f\"combined_all_cloudbase\")\n",
    "\n",
    "    @staticmethod\n",
    "    def LoadAllCloudBase_Job(ModelData,DataManager,\n",
    "                             job_id):\n",
    "        out = Results_InputOutput_Class.LoadOutFile(ModelData,DataManager,f\"{job_id}_all_cloudbase\")\n",
    "    \n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def LoadAllCloudBase_Combined(ModelData,DataManager):\n",
    "        out = Results_InputOutput_Class.LoadOutFile(ModelData,DataManager,f\"combined_all_cloudbase\")\n",
    "        return out\n",
    "\n",
    "    #######################################################\n",
    "    \n",
    "    @staticmethod\n",
    "    def SaveLFC_Profile_Job(ModelData,DataManager,\n",
    "                            LFC_profile,job_id, Ltype):\n",
    "        #Ltype in LFC or LCL\n",
    "        Dictionary = {f\"{Ltype}_profile\": LFC_profile} \n",
    "        Results_InputOutput_Class.SaveOutFile(ModelData,DataManager, Dictionary,f\"{job_id}_{Ltype}_profile\")\n",
    "\n",
    "    @staticmethod\n",
    "    def SaveLFC_Profile_Combined(ModelData,DataManager,\n",
    "                            LFC_profile, Ltype):\n",
    "        #Ltype in LFC or LCL\n",
    "        Dictionary = {f\"{Ltype}_profile\": LFC_profile} \n",
    "        Results_InputOutput_Class.SaveOutFile(ModelData,DataManager, Dictionary,f\"combined_{Ltype}_profile\")\n",
    "\n",
    "    @staticmethod\n",
    "    def LoadLFC_Profile_Job(ModelData,DataManager,\n",
    "                            job_id, Ltype):\n",
    "        #Ltype in LFC or LCL\n",
    "        out = Results_InputOutput_Class.LoadOutFile(ModelData,DataManager,f\"{job_id}_{Ltype}_profile\")\n",
    "    \n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def LoadLFC_Profile_Combined(ModelData,DataManager, Ltype):\n",
    "        #Ltype in LFC or LCL\n",
    "        out = Results_InputOutput_Class.LoadOutFile(ModelData,DataManager,f\"combined_{Ltype}_profile\")\n",
    "    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9bd7dca-82ab-47e2-9c58-b28e25c908df",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (4226921863.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 12\u001b[0;36m\u001b[0m\n\u001b[0;31m    return Dictionary\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TrackedParcel_Loading_Class\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class TrackedParcel_Loading_Class:\n",
    "\n",
    "    @staticmethod\n",
    "    def LoadFinalData(ModelData,DataManager,Results_InputOutput_Class):    \n",
    "        Dictionary = Results_InputOutput_Class.LoadOutFile(ModelData,DataManager,job_id=\"combined_SUBSET\")\n",
    "        return Dictionary\n",
    "\n",
    "    @staticmethod\n",
    "    def GetTrackedParcelArrays(Dictionary):\n",
    "        \"\"\"\n",
    "        Extract all tracked parcel arrays (CL, nonCL, SBF, ColdPool) from Dictionary.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Nested dictionary of arrays, grouped by type and category.\n",
    "        \"\"\"\n",
    "        trackedArrays = {\n",
    "            \"CL\": {\n",
    "                \"ALL\": Dictionary[\"CL_ALL_out_arr\"],\n",
    "                \"SHALLOW\": Dictionary[\"CL_SHALLOW_out_arr\"],\n",
    "                \"DEEP\": Dictionary[\"CL_DEEP_out_arr\"],\n",
    "            },\n",
    "            \"nonCL\": {\n",
    "                \"ALL\": Dictionary[\"nonCL_ALL_out_arr\"],\n",
    "                \"SHALLOW\": Dictionary[\"nonCL_SHALLOW_out_arr\"],\n",
    "                \"DEEP\": Dictionary[\"nonCL_DEEP_out_arr\"],\n",
    "            },\n",
    "            \"SBF\": {\n",
    "                \"ALL\": Dictionary[\"SBF_ALL_out_arr\"],\n",
    "                \"SHALLOW\": Dictionary[\"SBF_SHALLOW_out_arr\"],\n",
    "                \"DEEP\": Dictionary[\"SBF_DEEP_out_arr\"],\n",
    "            },\n",
    "            \"nonSBF\": {\n",
    "                \"ALL\": Dictionary[\"nonSBF_ALL_out_arr\"],\n",
    "                \"SHALLOW\": Dictionary[\"nonSBF_SHALLOW_out_arr\"],\n",
    "                \"DEEP\": Dictionary[\"nonSBF_DEEP_out_arr\"],\n",
    "            },\n",
    "            \"ColdPool\": {\n",
    "                \"ALL\": Dictionary[\"ColdPool_ALL_out_arr\"],\n",
    "                \"SHALLOW\": Dictionary[\"ColdPool_SHALLOW_out_arr\"],\n",
    "                \"DEEP\": Dictionary[\"ColdPool_DEEP_out_arr\"],\n",
    "            }\n",
    "        }\n",
    "    \n",
    "        # concise summary\n",
    "        print(f\"CL: ALL={len(trackedArrays['CL']['ALL'])}, SHALLOW={len(trackedArrays['CL']['SHALLOW'])}, DEEP={len(trackedArrays['CL']['DEEP'])}\")\n",
    "        print(f\"nonCL: ALL={len(trackedArrays['nonCL']['ALL'])}, SHALLOW={len(trackedArrays['nonCL']['SHALLOW'])}, DEEP={len(trackedArrays['nonCL']['DEEP'])}\")\n",
    "        print(f\"SBF: ALL={len(trackedArrays['SBF']['ALL'])}, SHALLOW={len(trackedArrays['SBF']['SHALLOW'])}, DEEP={len(trackedArrays['SBF']['DEEP'])}\")\n",
    "        print(f\"ColdPool: ALL={len(trackedArrays['ColdPool']['ALL'])}, SHALLOW={len(trackedArrays['ColdPool']['SHALLOW'])}, DEEP={len(trackedArrays['ColdPool']['DEEP'])}\")\n",
    "    \n",
    "        return trackedArrays\n",
    "    \n",
    "    \n",
    "    #Reading In Final Results from SubsetParcels\n",
    "    @staticmethod\n",
    "    def LoadingSubsetParcelData(ModelData,DataManager,Results_InputOutput_Class):\n",
    "    \n",
    "        #Loading Tracked Parcel Data\n",
    "        Dictionary = TrackedParcel_Loading_Class.LoadFinalData(ModelData,DataManager,Results_InputOutput_Class)\n",
    "        trackedArrays = TrackedParcel_Loading_Class.GetTrackedParcelArrays(Dictionary)\n",
    "        \n",
    "        #cloudbase\n",
    "        all_cloudbase = Results_InputOutput_Class.LoadAllCloudBase_Combined(ModelData,DataManager)[\"all_cloudbase\"]\n",
    "    \n",
    "        mean_all_cloudbase = np.nanmean(all_cloudbase)\n",
    "        min_all_cloudbase = np.nanmin(all_cloudbase)\n",
    "        print(f\"Mean Cloudbase is: {mean_all_cloudbase:.2f} km\\n\")\n",
    "        print(f\"Min Cloudbase is: {min_all_cloudbase:.2f} km\\n\")\n",
    "    \n",
    "        #lfc and lcl\n",
    "        LFC_profile = Results_InputOutput_Class.LoadLFC_Profile_Combined(ModelData,DataManager,Ltype='LFC')[\"LFC_profile\"]\n",
    "        LCL_profile = Results_InputOutput_Class.LoadLFC_Profile_Combined(ModelData,DataManager,Ltype='LCL')[\"LCL_profile\"]\n",
    "        \n",
    "        #LFC and LCL\n",
    "        MeanLFC=np.mean(LFC_profile)\n",
    "        MeanLCL=np.mean(LCL_profile)\n",
    "        MinLFC=np.min(LFC_profile)\n",
    "        MinLCL=np.min(LCL_profile)\n",
    "        print(f\"Mean LFC is: {MeanLFC:.2f} km\\n\")\n",
    "        print(f\"Mean LCL is: {MeanLCL:.2f} km\\n\")\n",
    "        print(f\"Min LFC is: {MinLFC:.2f} km\\n\")\n",
    "        print(f\"Min LCL is: {MinLCL:.2f} km\\n\")\n",
    "        \n",
    "    \n",
    "        #combining all level data into dictionary\n",
    "        LevelsDictionary = {\"all_cloudbase\": all_cloudbase,\n",
    "                            \"mean_all_cloudbase\": mean_all_cloudbase,\n",
    "                            \"min_all_cloudbase\": min_all_cloudbase,\n",
    "    \n",
    "                            \"LFC_profile\": LFC_profile,\n",
    "                            \"LCL_profile\": LCL_profile,\n",
    "                            \"MeanLFC\": MeanLFC,\n",
    "                            \"MeanLCL\": MeanLCL,\n",
    "                            \"MinLFC\": MinLFC,\n",
    "                            \"MinLCL\": MinLCL}\n",
    "                            \n",
    "                            \n",
    "        return trackedArrays,LevelsDictionary\n",
    "\n",
    "# #Example Calls\n",
    "# trackedArrays,LevelsDictionary = TrackedParcel_Loading_Class.LoadingSubsetParcelData(ModelData,DataManager,\n",
    "#                                                          Results_InputOutput_Class)\n",
    "    \n",
    "# CL_ALL = trackedArrays[\"CL\"][\"ALL\"]\n",
    "# SBF_DEEP = trackedArrays[\"SBF\"][\"DEEP\"]\n",
    "# ColdPool_SHALLOW = trackedArrays[\"ColdPool\"][\"SHALLOW\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
