{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abb5f4a-a6ab-46a5-8719-9c51d5fcecac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS FUNCTION IS FOR RUNNING WITH SLURM JOB ARRAY\n",
    "#(SPLITS UP JOB_ARRAY BELOW INTO EVEN MORE TASKS)\n",
    "def StartSlurmJobArray(num_jobs,num_slurm_jobs, ISRUN):\n",
    "    job_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0)) #this is the current SBATCH job id\n",
    "    if job_id==0: job_id=1\n",
    "    if ISRUN==False:\n",
    "        start_job=1;end_job=num_jobs+1\n",
    "        return start_job,end_job\n",
    "    total_elements=num_jobs #total num of variables\n",
    "\n",
    "    job_range = total_elements // num_slurm_jobs  # Base size for each chunk\n",
    "    remaining = total_elements % num_slurm_jobs   # Number of chunks with 1 extra \n",
    "    \n",
    "    # Function to compute the start and end for each job_id\n",
    "    def get_job_range(job_id, num_slurm_jobs):\n",
    "        job_id-=1\n",
    "        # Add one extra element to the first 'remaining' chunks\n",
    "        start_job = job_id * job_range + min(job_id, remaining)\n",
    "        end_job = start_job + job_range + (1 if job_id < remaining else 0)\n",
    "    \n",
    "        if job_id == num_slurm_jobs - 1: \n",
    "            end_job = total_elements \n",
    "        return start_job, end_job\n",
    "    # def job_testing():\n",
    "    #     #TESTING\n",
    "    #     start=[];end=[]\n",
    "    #     for job_id in range(1,num_slurm_jobs+1):\n",
    "    #         start_job, end_job = get_job_range(job_id)\n",
    "    #         print(start_job,end_job)\n",
    "    #         start.append(start_job)\n",
    "    #         end.append(end_job)\n",
    "    #     print(np.all(start!=end))\n",
    "    #     print(len(np.unique(start))==len(start))\n",
    "    #     print(len(np.unique(end))==len(end))\n",
    "    # job_testing()\n",
    "    # if sbatch==True:\n",
    "        \n",
    "    start_job, end_job = get_job_range(job_id, num_slurm_jobs)\n",
    "    index_adjust=start_job\n",
    "    # print(f'start_job = {start_job}, end_job = {end_job}')\n",
    "    if start_job==0: start_job=1\n",
    "    if end_job==total_elements: end_job+=1\n",
    "    return start_job,end_job\n",
    "\n",
    "# job_id=1\n",
    "# [start_slurm_job,end_slurm_job,slurm_index_adjust]=StartSlurmJobArray(num_jobs,num_slurm_jobs,ISRUN)\n",
    "# parcel=parcel1.isel(xh=slice(start_job,end_job))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceeecbb2-8c23-4058-a16c-1975f4ec70ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.lines as mlines\n",
    "import xarray as xr\n",
    "import os; import time\n",
    "import pickle\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b81c58-468e-45db-a885-d66eb92ab0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAIN DIRECTORIES\n",
    "mainDirectory='/mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/'\n",
    "scratchDirectory='/home/air673/koa_scratch/'\n",
    "codeDirectory='/mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/Project_Algorithms/Tracking_Algorithms'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486889b9-f836-4eac-a7b1-1baefca7feb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADING DATA\n",
    "def GetDataDirectories(simulationNumber):\n",
    "    if simulationNumber == 1:\n",
    "        Directory=os.path.join(mainDirectory,'Model/cm1r20.3/run')\n",
    "        res='1km'; t_res='5min'; Np_str='1e6'; Nz_str='34'\n",
    "    elif simulationNumber == 2:\n",
    "        Directory=scratchDirectory\n",
    "        res='1km'; t_res='1min'; Np_str='50e6'; Nz_str='95'\n",
    "    elif simulationNumber == 3:\n",
    "        Directory=scratchDirectory\n",
    "        res='250m'; t_res='1min'; Np_str='50e6'; Nz_str='95'\n",
    "        \n",
    "    dataDirectory = os.path.join(Directory, f\"cm1out_{res}_{t_res}_{Nz_str}nz.nc\")\n",
    "    parcelDirectory = os.path.join(Directory,f\"cm1out_pdata_{res}_{t_res}_{Np_str}np.nc\")\n",
    "    return dataDirectory, parcelDirectory, res,t_res,Np_str,Nz_str\n",
    "    \n",
    "def GetData(dataDirectory, parcelDirectory):\n",
    "    dataNC = xr.open_dataset(dataDirectory, decode_timedelta=True) \n",
    "    parcelNC = xr.open_dataset(parcelDirectory, decode_timedelta=True) \n",
    "    return dataNC,parcelNC\n",
    "\n",
    "def SubsetDataVars(dataNC):\n",
    "    varList = [\"thflux\", \"qvflux\", \"tsk\", \"cape\", \n",
    "               \"cin\", \"lcl\", \"lfc\", \"th\",\n",
    "               \"prs\", \"rho\", \"qv\", \"qc\",\n",
    "               \"qr\", \"qi\", \"qs\",\"qg\", \n",
    "               \"buoyancy\", \"uinterp\", \"vinterp\", \"winterp\",]\n",
    "    \n",
    "    varList += [\"ptb_hadv\", \"ptb_vadv\", \"ptb_hidiff\", \"ptb_vidiff\",\n",
    "                \"ptb_hturb\", \"ptb_vturb\", \"ptb_mp\", \"ptb_rdamp\", \n",
    "                \"ptb_rad\", \"ptb_div\", \"ptb_diss\",]\n",
    "    \n",
    "    varList += [\"qvb_hadv\", \"qvb_vadv\", \"qvb_hidiff\", \"qvb_vidiff\", \n",
    "                \"qvb_hturb\", \"qvb_vturb\", \"qvb_mp\",]\n",
    "    \n",
    "    varList += [\"wb_hadv\", \"wb_vadv\", \"wb_hidiff\", \"wb_vidiff\",\n",
    "                \"wb_hturb\", \"wb_vturb\", \"wb_pgrad\", \"wb_rdamp\", \"wb_buoy\",]\n",
    "\n",
    "    return dataNC[varList]\n",
    "\n",
    "[dataDirectory,parcelDirectory, res,t_res,Np_str,Nz_str] = GetDataDirectories(simulationNumber=1)\n",
    "[data1,parcel1] = GetData(dataDirectory, parcelDirectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666f6349-17ae-426f-af05-7b014e1bce7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "job_array=False;index_adjust=0\n",
    "ocean_fraction=2/8\n",
    "start_time = time.time();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cab135f-f15b-4b64-a284-584972a5993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58086e0-c188-48e9-99e2-967ffdec6a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "#FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03709899-a386-4590-9c22-db440ac2bbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JOB ARRAY SETUP\n",
    "def StartJobArray(job_id):\n",
    "    total_elements=len(parcel1['xh']) #total num of variables\n",
    "    \n",
    "    if num_jobs >= total_elements:\n",
    "        raise ValueError(\"Number of jobs cannot be greater than or equal to total elements.\")\n",
    "    \n",
    "    job_range = total_elements // num_jobs  # Base size for each chunk\n",
    "    remaining = total_elements % num_jobs   # Number of chunks with 1 extra \n",
    "    \n",
    "    # Function to compute the start and end for each job_id\n",
    "    def get_job_range(job_id):\n",
    "        job_id-=1\n",
    "        # Add one extra element to the first 'remaining' chunks\n",
    "        start_job = job_id * job_range + min(job_id, remaining)\n",
    "        end_job = start_job + job_range + (1 if job_id < remaining else 0)\n",
    "    \n",
    "        if job_id == num_jobs - 1: \n",
    "            end_job = total_elements #- 1\n",
    "        return start_job, end_job\n",
    "    # def job_testing():\n",
    "    #     #TESTING\n",
    "    #     start=[];end=[]\n",
    "    #     for job_id in range(1,num_jobs+1):\n",
    "    #         start_job, end_job = get_job_range(job_id)\n",
    "    #         print(start_job,end_job)\n",
    "    #         start.append(start_job)\n",
    "    #         end.append(end_job)\n",
    "    #     print(np.all(start!=end))\n",
    "    #     print(len(np.unique(start))==len(start))\n",
    "    #     print(len(np.unique(end))==len(end))\n",
    "    # job_testing()\n",
    "    \n",
    "    # job_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0)) #this is the current SBATCH job id\n",
    "    # if job_id==0: job_id=1\n",
    "    start_job, end_job = get_job_range(job_id)\n",
    "    print(f'\\nstart_job = {start_job}, end_job = {end_job}')\n",
    "\n",
    "    index_adjust=start_job\n",
    "    return start_job,end_job,index_adjust\n",
    "# job_id=1\n",
    "# [start_job,end_job,index_adjust] = StartJobArray(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae653ba6-a6be-49e5-b779-d01162191ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetData(start_job,end_job):\n",
    "    # parcel=parcel1.isel(xh=slice(start_job,end_job))\n",
    "    \n",
    "    dir2=dir+'Project_Algorithms/Lagrangian_Arrays/OUTPUT/'\n",
    "    in_file=dir2+f'lagrangian_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "    with h5py.File(in_file, 'r') as f:\n",
    "        # Load the dataset by its name\n",
    "        W = f['W'][:,start_job:end_job]\n",
    "        QCQI = f['QCQI'][:,start_job:end_job]\n",
    "        Z = f['Z'][:,start_job:end_job]\n",
    "        Y = f['Y'][:,start_job:end_job]\n",
    "        X = f['X'][:,start_job:end_job]\n",
    "        parcel_z=f['z'][:,start_job:end_job]\n",
    "\n",
    "    return W,QCQI,Z,Y,X,parcel_z\n",
    "\n",
    "def GetLFCData(start_job, end_job):\n",
    "    dir2=dir+'Project_Algorithms/Lagrangian_Arrays/OUTPUT/'\n",
    "    in_file=dir2+f'lagrangian_LFC_LCL_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "    with h5py.File(in_file, 'r') as f:\n",
    "        # Load the dataset by its name\n",
    "        LFC_FULL = f['LFC_FULL'][:,start_job:end_job]\n",
    "    return LFC_FULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc25c858-06e9-443f-a662-44a99a74c832",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "#MORE FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1558707d-5a43-4ba8-9adf-031787b1ad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetSubsetParams(data1):\n",
    "    #DOMAIN SUBSETTING\n",
    "    ############################################################\n",
    "    #FINDING NEW LEFT T-BOUNDARY\n",
    "    dt=data1['time'][1].item()/1e9 #seconds per timestep\n",
    "    dhours=(dt/60**2) #hours per timestep\n",
    "    start_hour=4 #10:00am\n",
    "    t_start=int(start_hour/dhours)\n",
    "    #FINDING NEW RIGHT T-BOUNDARY\n",
    "    end_hour=11 #5pm\n",
    "    t_end=int(end_hour/dhours)\n",
    "    #PRINTING\n",
    "    # print(f't in {t_start}:{t_end}')\n",
    "    \n",
    "    \n",
    "    #FINDING NEW TOP Z-BOUNDARY\n",
    "    dzh=data1['zh']\n",
    "    zh_bottom=0; zh_top=np.where(dzh>=19)[0][0]\n",
    "    dzf=data1['zf']\n",
    "    zf_bottom=0; zf_top=np.where(dzf>=20)[0][0]\n",
    "    #PRINTING\n",
    "    # print(f'zh_top at {zh_bottom}:{zh_top}')\n",
    "    \n",
    "    #FINDING THE NEW LEFT X-BOUNDARY\n",
    "    ocean_percent=2/8\n",
    "    left_to_coast=data1['xh'][0]+(data1['xh'][-1]-data1['xh'][0])*ocean_percent\n",
    "    where_coast_xh=np.where(data1['xh']>=left_to_coast)[0][0]#-25\n",
    "    where_coast_xf=np.where(data1['xf']>=left_to_coast)[0][0]#-25\n",
    "    #FINDING THE NEW RIGHT X-BOUNDARY\n",
    "    right_fraction=80/100\n",
    "    # Normalize to start from zero\n",
    "    xf = data1['xf'] - data1['xf'][0]\n",
    "    xh = data1['xh'] - data1['xh'][0]\n",
    "\n",
    "    # Total physical length\n",
    "    xf_max = xf[-1]\n",
    "    xh_max = xh[-1]\n",
    "\n",
    "    # Find index where physical location exceeds 80% of domain\n",
    "    end_xf = np.where(xf > right_fraction * xf_max)[0][0]\n",
    "    end_xh = np.where(xh > right_fraction * xh_max)[0][0]\n",
    "    #PRINTING\n",
    "    # print(f'x in {0}:{where_coast_xh-1} FOR SEA')\n",
    "    # print(f'x in {where_coast_xh}:{end_xh} FOR LAND')\n",
    "    ############################################################\n",
    "    return (\n",
    "        t_start, t_end,\n",
    "        zh_bottom, zf_bottom,\n",
    "        zh_top, zf_top,\n",
    "        where_coast_xh, where_coast_xf,\n",
    "        end_xh, end_xf\n",
    "    )\n",
    "# [t_start, t_end, zh_bottom, zf_bottom, zh_top, zf_top, where_coast_xh, where_coast_xf, end_xh, end_xf] = GetSubsetParams(data1)\n",
    "# GetSubsetParams(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f14fb21-a5b9-4737-a448-fc5f23e4e6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Back Data Later\n",
    "##################################################################\n",
    "#DOMAIN SUBSETTING\n",
    "def DOMAIN_SUBSET(out_arr,index_adjust):\n",
    "    print(f'length before: {len(out_arr)}')\n",
    "\n",
    "    [t_start, t_end, zh_bottom, zf_bottom, zh_top, zf_top, where_coast_xh, where_coast_xf, end_xh, end_xf] = GetSubsetParams(data1)\n",
    "    #SUBSETTING CODE\n",
    "    ################\n",
    "    t,p=out_arr[:,1],out_arr[:,0]\n",
    "    # if 'job_array' in globals():\n",
    "    #     p -= index_adjust\n",
    "\n",
    "    #GETTING X VALUES OF EACH PARCEL \n",
    "    zs=Z[t,p-index_adjust]\n",
    "    xs=X[t,p-index_adjust]\n",
    "\n",
    "    #GETTING SUBSET CONDITIONS\n",
    "    cond1=(xs>=where_coast_xh)&(xs<=end_xh)\n",
    "    cond2=(out_arr[:,1]>=t_start)&(out_arr[:,1]<=t_end)\n",
    "    cond3=(zs>=zh_bottom)&(zs<=zh_top)\n",
    "    combined_conds=cond1&cond2&cond3\n",
    "\n",
    "    #SUBSETTING\n",
    "    where=np.where(combined_conds)\n",
    "    out_arr=out_arr[where]\n",
    "\n",
    "    print(f'==> length after: {len(out_arr)}'+'\\n')\n",
    "    return out_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923dbd13-76cb-4e71-9949-016b528a6ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetALLArrays_CL(start_job,end_job,index_adjust):\n",
    "    #LOADING BACK IN\n",
    "    def load_file():\n",
    "        in_file=dir+f'Project_Algorithms/Tracking_Algorithms/OUTPUT/parcel_tracking_{res}_{t_res}_{Np_str}.h5'\n",
    "        with h5py.File(in_file, 'r') as hf:\n",
    "            out_arr=hf['out_arr'][:]\n",
    "            save_arr=hf['save_arr'][:]\n",
    "            save2_arr=hf['save2_arr'][:]\n",
    "        return out_arr,save_arr,save2_arr\n",
    "    [out_arr,save_arr,save2_arr]=load_file()\n",
    "    print(np.where(np.all(out_arr==0,axis=1))) #TESTING#*#*\n",
    "    \n",
    "    print('list of first 10 ignored parcels');\n",
    "    # print(f'there are a total of {len(out_arr)} CL parcels and {len(save_arr)} nonCL parcels'+'\\n')\n",
    "    \n",
    "    # if 'job_array' in globals():\n",
    "    #APPLYING JOB_ARRAY TO PARCEL NUMBER\n",
    "    ####################################\n",
    "    def job_filter(arr):\n",
    "        return arr[(arr[:,0]>=start_job)&(arr[:,0]<end_job)]\n",
    "    print('Applying Job Array')\n",
    "    out_arr=job_filter(out_arr)\n",
    "    save_arr=job_filter(save_arr)\n",
    "\n",
    "    print(np.where(np.all(out_arr==0,axis=1))) #TESTING#*#*\n",
    "    \n",
    "    # print(f'there are a total of {len(out_arr)} CL parcels and {len(save_arr)} nonCL parcels'+'\\n')\n",
    "    \n",
    "    #CHOOSING UNIQUE INDEXES\n",
    "    ###############################################################################\n",
    "    def remove_duplicates(arr):\n",
    "        lst = []\n",
    "        unique_values, counts = np.unique(arr[:, 0], return_counts=True)\n",
    "        duplicates = unique_values[counts > 1]\n",
    "        for elem in duplicates:\n",
    "            idx = np.where(arr[:, 0] == elem)[0]\n",
    "            extras = idx[np.where(arr[idx, 1] != np.min(arr[idx, 1]))]\n",
    "            lst.extend(extras)\n",
    "        mask = np.ones(len(arr), dtype=bool)\n",
    "        mask[lst] = False\n",
    "        return arr[mask]\n",
    "    out_arr=remove_duplicates(out_arr)\n",
    "    save_arr=remove_duplicates(save_arr)\n",
    "    ###############################################################################\n",
    "    # print(np.where(np.all(out_arr==0,axis=1))) #TESTING\n",
    "    \n",
    "    ############################################################ \n",
    "    #SUBSETTING\n",
    "    subset=True\n",
    "    if subset==True:\n",
    "        out_arr_subset=DOMAIN_SUBSET(out_arr,index_adjust)\n",
    "        save_arr_subset=DOMAIN_SUBSET(save_arr,index_adjust)\n",
    "    ############################################################\n",
    "    # print(np.where(np.all(out_arr==0,axis=1))) #TESTING\n",
    "    \n",
    "    ALL_out_arr=out_arr_subset.copy(); ALL_save_arr=save_arr_subset.copy()\n",
    "    return ALL_out_arr,ALL_save_arr\n",
    "# [ALL_out_arr,ALL_save_arr]=GetALLArrays_CL(start_job,end_job,index_adjust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01f8db6-703e-4190-91cf-e261ec79e342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddt(f,dt=1):\n",
    "    ddx = (\n",
    "            f[1:  ]\n",
    "            -\n",
    "            f[0:-1]\n",
    "        ) / (\n",
    "        2 * dt\n",
    "    )\n",
    "    return ddx\n",
    "\n",
    "#search for deep convective parcels within lagrangian tracking output     \n",
    "##############################################################\n",
    "def SHALLOW_threshold(out_arr,zthresh,index_adjust,type):\n",
    "    \n",
    "    deep_out_ind=[]; extendrange=[]\n",
    "    times=data1['time'].values/(1e9 * 60); times=times.astype(float);\n",
    "    for ind in range(len(out_arr)): \n",
    "        # if np.mod(ind,5000)==0: print(f'{ind}/{len(out_arr)}')\n",
    "        #CHECK TO SSEE IF NEXT MOST LOCAL TIME MAX GOES ABOVE ZTHRESHS \n",
    "\n",
    "        #Get Ascending Range Past LFC For Maximum 120 Minutes Simulation Time\n",
    "        nummins=120; numsteps=int(nummins/times[1])\n",
    "        aboverange=np.arange(out_arr[ind,2],out_arr[ind,2]+numsteps,1) #range of times between current time and numsteps later\n",
    "        aboverange=aboverange[aboverange<len(data1['time'])] #caps out at max time\n",
    "        above=parcel_z[aboverange,out_arr[ind,0]-index_adjust]/1000 #JOBARRAY ADJUST\n",
    "    \n",
    "        #Takes The time derivative \n",
    "        ddx=ddt(above)\n",
    "\n",
    "        #Checks whether the Local Time Max Is Located Above zthresh\n",
    "        signs = np.sign(ddx)\n",
    "        signs_diff=np.diff(signs)\n",
    "        local_maxes=np.where((signs_diff != 0) & (signs_diff < 0))[0]+1 #make sure +1 is here\n",
    "        if len(local_maxes)==0:\n",
    "            local_maxes=[0]\n",
    "        elif np.any(above[local_maxes[0]]<=zthresh): #< for SHALLOW, > for DEEP\n",
    "            extendrange.append(local_maxes[0]) #save to extend xlim of plot later\n",
    "            deep_out_ind.append(ind)\n",
    "\n",
    "    #SUBSET OUT FOR FINAL RESULT\n",
    "    out_arr=out_arr[deep_out_ind,:]\n",
    "    # print(f'> {zthresh} km. {len(out_arr)} leftover parcels')\n",
    "    return out_arr#, extendrange\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00e1a96-4494-45cf-b85e-e585e35dc3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#search for deep convective parcels within lagrangian tracking output     \n",
    "##############################################################\n",
    "def DEEP_threshold(out_arr,zthresh,index_adjust,type):\n",
    "    \n",
    "    deep_out_ind=[]; extendrange=[]\n",
    "    times=data1['time'].values/(1e9 * 60); times=times.astype(float);\n",
    "    for ind in range(len(out_arr)): \n",
    "        # if np.mod(ind,5000)==0: print(f'{ind}/{len(out_arr)}')\n",
    "        #CHECK TO SSEE IF NEXT MOST LOCAL TIME MAX GOES ABOVE ZTHRESHS \n",
    "\n",
    "        #Get Ascending Range Past LFC For Maximum 120 Minutes Simulation Time\n",
    "        nummins=120; numsteps=int(nummins/times[1])\n",
    "        aboverange=np.arange(out_arr[ind,2],out_arr[ind,2]+numsteps,1) #range of times between current time and numsteps later\n",
    "        aboverange=aboverange[aboverange<len(data1['time'])] #caps out at max time\n",
    "        above=parcel_z[aboverange,out_arr[ind,0]-index_adjust]/1000 #JOBARRAY ADJUST\n",
    "        \n",
    "        #Takes The time derivative \n",
    "        ddx=ddt(above)\n",
    "\n",
    "        #Checks whether the Local Time Max Is Located Above zthresh\n",
    "        signs = np.sign(ddx)\n",
    "        signs_diff=np.diff(signs)\n",
    "        local_maxes=np.where((signs_diff != 0) & (signs_diff < 0))[0]+1 #make sure +1 is here\n",
    "        if len(local_maxes)==0:\n",
    "            local_maxes=[0]\n",
    "        \n",
    "        if np.any(above[local_maxes[0]]>=zthresh): #< for SHALLOW, > for DEEP\n",
    "            extendrange.append(local_maxes[0]) #save to extend xlim of plot later\n",
    "            deep_out_ind.append(ind)\n",
    "\n",
    "    #SUBSET OUT FOR FINAL RESULT\n",
    "    out_arr=out_arr[deep_out_ind,:]\n",
    "    # print(f'> {zthresh} km. {len(out_arr)} leftover parcels')\n",
    "    return out_arr#, extendrange\n",
    "    # print(out_arr)\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264b5df8-9d7f-4fed-bfad-2d13bb896a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHALLOW\n",
    "def GetSHALLOWArrays_CL(ALL_out_arr,ALL_save_arr,index_adjust):\n",
    "    convectivelevel=4 #4km\n",
    "    SHALLOW_out_arr=SHALLOW_threshold(ALL_out_arr,convectivelevel,index_adjust,type='CL')\n",
    "    SHALLOW_save_arr=SHALLOW_threshold(ALL_save_arr,convectivelevel,index_adjust,type='nonCL')\n",
    "    # print('list of first 10 SBZ parcels'); print(out_arr[:15])\n",
    "    # print(f'there are a total of {len(SHALLOW_out_arr)} CL parcels and {len(SHALLOW_save_arr)} nonCL parcels')\n",
    "    return SHALLOW_out_arr,SHALLOW_save_arr\n",
    "# [SHALLOW_out_arr,SHALLOW_save_arr]=GetSHALLOWArrays_CL(ALL_out_arr,ALL_save_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eba61f-2cdd-4c4e-82aa-8c3c719248e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEEP\n",
    "def GetDEEPArrays_CL(ALL_out_arr,ALL_save_arr,index_adjust):    \n",
    "    convectivelevel=6 #4km\n",
    "    DEEP_out_arr=DEEP_threshold(ALL_out_arr,convectivelevel,index_adjust,type='CL')\n",
    "    DEEP_save_arr=DEEP_threshold(ALL_save_arr,convectivelevel,index_adjust,type='nonCL')\n",
    "    # print('list of first 10 SBZ parcels'); print(out_arr[:15])\n",
    "    # print(f'there are a total of {len(DEEP_out_arr)} CL parcels and {len(DEEP_save_arr)} nonCL parcels')\n",
    "    return DEEP_out_arr,DEEP_save_arr\n",
    "# [DEEP_out_arr,DEEP_save_arr]=GetDEEPArrays_CL(ALL_out_arr,ALL_save_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cdb2f1-723b-4fbc-ae1e-a0340b1ed47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetArrays_SBZ(ALL_out_arr,index_adjust):\n",
    "    \n",
    "    \n",
    "    def find_SBZ_xmaxs():\n",
    "        # Define the directory and file path\n",
    "        if res=='1km':\n",
    "            dir2 = '/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "        elif res=='250m':\n",
    "            dir2='/home/air673/koa_scratch/'\n",
    "        file_path = dir2 + 'Variable_Calculation/OUTPUT/' + 'Convergence' + f'_{res}_{t_res}' + '.h5'\n",
    "        \n",
    "        # Open the HDF5 file in read mode\n",
    "        with h5py.File(file_path, 'r') as f:\n",
    "            # Access the 'conv' dataset\n",
    "            conv_dataset = f['conv']\n",
    "            \n",
    "            # Define the vertical level you are interested in\n",
    "            zlev = 4\n",
    "            \n",
    "            # Initialize a list to store the xmaxs for each time step\n",
    "            xmaxs_list = []\n",
    "    \n",
    "            # Loop over each time step (axis=0 corresponds to time)\n",
    "            for t in range(conv_dataset.shape[0]):  # conv_dataset.shape[0] is the time dimension size\n",
    "                # Read the relevant slice for this time step and vertical level\n",
    "                Conv_t_zlev = conv_dataset[t, zlev, :, :]  # Shape should be (y_size, x_size)\n",
    "                \n",
    "                # Calculate the mean across the y-axis\n",
    "                Conv_ymean = np.mean(Conv_t_zlev, axis=0)  # Mean across the y-axis\n",
    "                \n",
    "                # Find the index of the maximum value along the x-axis\n",
    "                xmax = np.argmax(Conv_ymean)\n",
    "                \n",
    "                # Append the result for this time step\n",
    "                xmaxs_list.append(xmax)\n",
    "        \n",
    "        # Convert the list of xmaxs to a numpy array (optional)\n",
    "        xmaxs = np.array(xmaxs_list)\n",
    "    \n",
    "        return xmaxs #returns SBZ x location for each timestep\n",
    "    \n",
    "    \n",
    "    def subset_SBZ(out_arr):\n",
    "        xmaxs=find_SBZ_xmaxs()\n",
    "    \n",
    "        SBZ_subset=[]\n",
    "        # test=[] #TESTING\n",
    "        \n",
    "        for ind in np.arange(out_arr.shape[0]):\n",
    "            \n",
    "            row=out_arr[ind]\n",
    "            p=row[0]\n",
    "            t=row[1]\n",
    "    \n",
    "            kms=np.argmax(data1['xh'].values-data1['xh'][0].values >= 1)\n",
    "            if X[t,p-index_adjust] in np.arange( (xmaxs[t]-2*kms),(xmaxs[t]+2*kms) +1):\n",
    "                SBZ_subset.append(ind)\n",
    "                # test.append(p) #TESTING\n",
    "        \n",
    "        SBZ_out_arr=out_arr[SBZ_subset]\n",
    "        print(f'there are a total of {len(SBZ_out_arr)} ALL SBZ CL parcels')\n",
    "    \n",
    "        valid_range=np.arange(out_arr.shape[0])\n",
    "        nonSBZ_out_arr=out_arr[list(set(valid_range) - set(SBZ_subset))]\n",
    "        print(f'there are a total of {len(nonSBZ_out_arr)} ALL nonSBZ CL parcels')\n",
    "        return SBZ_out_arr,nonSBZ_out_arr\n",
    "    \n",
    "    \n",
    "    # #LOADING CL MAXS FROM CL TRACKING ALGORITHM\n",
    "    # folder = '/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/Project_Algorithms/Tracking_Algorithms/'\n",
    "    # whereSBZ=xr.open_dataset(folder+f'whereCL_{res}_ONLY_SBZS.nc').load()\n",
    "    # whereSBZ=whereSBZ.isel(time=slice(0,len(data1['time'])))\n",
    "    # whereSBZ=whereSBZ['maxconv_x']\n",
    "    # def Get_SBZ_X(t,z,y):\n",
    "    #     Conv_X_Max=whereSBZ[t,z,y,:].values\n",
    "    #     return Conv_X_Max\n",
    "    # def subset_SBZ(out_arr):\n",
    "    \n",
    "    #     SBZ_subset=[]\n",
    "    #     # test=[] #TESTING\n",
    "        \n",
    "    #     for ind in np.arange(out_arr.shape[0]):\n",
    "            \n",
    "    #         row=out_arr[ind]\n",
    "    #         p=row[0]\n",
    "    #         t=row[1]\n",
    "    \n",
    "    #         kms=np.argmax(data1['xh'].values-data1['xh'][0].values >= 1)\n",
    "    #         value=X[t,p]\n",
    "    #         if np.any((value >= xmaxs - 2*kms) & (value <= xmaxs + 2*kms))==True:\n",
    "    #             SBZ_subset.append(ind)\n",
    "    #             # test.append(p) #TESTING\n",
    "        \n",
    "    #     SBZ_out_arr=out_arr[SBZ_subset]\n",
    "    #     print(f'there are a total of {len(SBZ_out_arr)} ALL SBZ CL parcels')\n",
    "    \n",
    "    #     valid_range=np.arange(out_arr.shape[0])\n",
    "    #     nonSBZ_out_arr=out_arr[list(set(valid_range) - set(SBZ_subset))]\n",
    "    #     print(f'there are a total of {len(nonSBZ_out_arr)} ALL nonSBZ CL parcels')\n",
    "    #     return SBZ_out_arr,nonSBZ_out_arr\n",
    "    \n",
    "    \n",
    "    # print(f'there are a total of {len(SHALLOW_SBZ_out_arr)} SHALLOW SBZ CL parcels')\n",
    "    # print(f'there are a total of {len(SHALLOW_nonSBZ_out_arr)} SHALLOW nonSBZ CL parcels')\n",
    "    # print(f'there are a total of {len(DEEP_SBZ_out_arr)} DEEP SBZ CL parcels')\n",
    "    # print(f'there are a total of {len(DEEP_nonSBZ_out_arr)} DEEP nonSBZ CL parcels')\n",
    "    \n",
    "    #SUBSETTING OUT SHALLOW AND DEEP FROM SBZ AND NONSBZ\n",
    "    [ALL_SBZ_out_arr,ALL_nonSBZ_out_arr]=subset_SBZ(ALL_out_arr)\n",
    "    SHALLOW_SBZ_out_arr=SHALLOW_threshold(ALL_SBZ_out_arr,4,index_adjust,'SBZ')\n",
    "    SHALLOW_nonSBZ_out_arr=SHALLOW_threshold(ALL_nonSBZ_out_arr,4,index_adjust,'nonSBZ')\n",
    "    DEEP_SBZ_out_arr=DEEP_threshold(ALL_SBZ_out_arr,6,index_adjust,'SBZ')\n",
    "    DEEP_nonSBZ_out_arr=DEEP_threshold(ALL_nonSBZ_out_arr,6,index_adjust,'nonSBZ')\n",
    "\n",
    "    return ALL_SBZ_out_arr,ALL_nonSBZ_out_arr,SHALLOW_SBZ_out_arr,SHALLOW_nonSBZ_out_arr,DEEP_SBZ_out_arr,DEEP_nonSBZ_out_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2213a4d8-0641-4b76-9572-a08cf0813bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ColdPool(out_arr1,out_arr2):\n",
    "    arr1 = out_arr1[:,0] #CL\n",
    "    arr2 = out_arr2[:,0] #nonSBZ\n",
    "    common_values = np.intersect1d(arr1, arr2)\n",
    "    indices_arr1 = np.where(np.isin(arr1, common_values))[0]  # Indices in arr1\n",
    "    ColdPool_out_arr=out_arr1[indices_arr1]\n",
    "    return ColdPool_out_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caa9806-d917-4570-b61d-e355340c4a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveData(data_dict, job_id):\n",
    "    dir2 = dir + f'Project_Algorithms/Tracking_Algorithms/subsetting/'\n",
    "    out_file = dir2 + f\"parcel_tracking_SUBSET_{res}_{t_res}_{Np_str}_{job_id}\" #*#*#\n",
    "\n",
    "    # Write the data to HDF5 file\n",
    "    with h5py.File(out_file, 'w') as h5f:\n",
    "        for key, value in data_dict.items():\n",
    "            h5f.create_dataset(key, data=value)\n",
    "\n",
    "#STORING THE MINIMUM CLOUDBASE INFORMATION\n",
    "def SaveAllCloudBase_Job(all_cloudbase,job_id):\n",
    "    dir2 = dir + f'Project_Algorithms/Tracking_Algorithms/subsetting/'\n",
    "    out_file = dir2 + f\"all_cloudbase_{res}_{t_res}_{Np_str}_{job_id}.pkl\"\n",
    "    with open(out_file, 'wb') as f:\n",
    "        pickle.dump(np.array(all_cloudbase), f)\n",
    "\n",
    "#STORING THE LFC PROFILE\n",
    "def SaveLFC_Profile_Job(LFC_profile,job_id):\n",
    "    dir2 = dir + f'Project_Algorithms/Tracking_Algorithms/subsetting/'\n",
    "    out_file = dir2 + f\"LFC_Profile_{res}_{t_res}_{Np_str}_{job_id}.pkl\"\n",
    "    with open(out_file, 'wb') as f:\n",
    "        pickle.dump(np.array(LFC_profile), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db122386-74d7-41ec-a56a-cb0f0a0d65f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts=[[],[],[],[],[],[]]\n",
    "def AddCounts(counts,ALL_out_arr,SHALLOW_out_arr,DEEP_out_arr,ALL_save_arr,SHALLOW_save_arr,DEEP_save_arr, job_id):\n",
    "    counts[0].append(ALL_out_arr.shape[0])\n",
    "    counts[1].append(SHALLOW_out_arr.shape[0])\n",
    "    counts[2].append(DEEP_out_arr.shape[0])\n",
    "    \n",
    "    counts[3].append(ALL_save_arr.shape[0])\n",
    "    counts[4].append(SHALLOW_save_arr.shape[0])\n",
    "    counts[5].append(DEEP_save_arr.shape[0])\n",
    "    return counts\n",
    "# AddCounts(counts,ALL_out_arr,SHALLOW_out_arr,DEEP_out_arr,ALL_save_arr,SHALLOW_save_arr,DEEP_save_arr, job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0c63f4-c7a9-45d7-8749-c3e38dff20ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# EVEN MORE FUNCTIONS\n",
    "def compute_after_arrays(data_dict, W, QCQI, index_adjust):\n",
    "    def find_after_time(out_arr):\n",
    "        wthresh = 0.5\n",
    "        qcqithresh = 1e-6\n",
    "        after_array = np.zeros(len(out_arr), dtype=int)\n",
    "        for count, out_row in enumerate(out_arr):\n",
    "            p = out_row[0]\n",
    "            t2 = out_row[2]\n",
    "            after = np.where((W[t2:, p - index_adjust] < wthresh) |\n",
    "                             (QCQI[t2:, p - index_adjust] < qcqithresh))\n",
    "            if len(after[0]) != 0:\n",
    "                after_array[count] = after[0][0]\n",
    "        return after_array\n",
    "\n",
    "    after_dict = {}\n",
    "    for key, arr in data_dict.items():\n",
    "        if \"SHALLOW\" in key:\n",
    "            after_dict[key.replace('_arr', '_after_array')] = np.zeros(len(arr), dtype=int)\n",
    "        else:\n",
    "            after_dict[key.replace('_arr', '_after_array')] = find_after_time(arr)\n",
    "\n",
    "    return after_dict\n",
    "# after_dict = compute_after_arrays(data_dict, W, QCQI, index_adjust)\n",
    "\n",
    "def AddColumn(data_dict,after_dict):\n",
    "    for (key1,key2) in zip(data_dict,after_dict):\n",
    "        data_dict[key1][:, 3] = after_dict[key2]\n",
    "    return data_dict\n",
    "# data_dict=AddColumn(data_dict,after_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bda937-2a6d-4bfd-8e91-941d55a3e474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_cloud_base(out_arr, Z, Y, X, W, QCQI, index_adjust):\n",
    "    #FINDING MEAN CLOUD BASE \n",
    "    zhs=data1['zh'].values\n",
    "    w_thresh2=0.5\n",
    "    qcqi_thresh=1e-6\n",
    "    type='all'\n",
    "    \n",
    "    profile_array =np.zeros((len(zhs), 2)) #column 1: var, column 2: counter, column 3: list of zhs\n",
    "    profile_array[:,1]=zhs;\n",
    "    \n",
    "    # cloudbase_lst=[]\n",
    "    after=4 #20 minutes\n",
    "    for row in range(out_arr.shape[0]):\n",
    "        if np.mod(row,3000)==0: print(f'{row}/{out_arr.shape[0]}')\n",
    "        p=out_arr[row,0]\n",
    "        \n",
    "        # ts=np.arange(out_arr[row,4],out_arr[row,5]+1 + after)\n",
    "        ts_end = min(out_arr[row, 2] + 1 + after, len(data1['time'])) #this takes care of exceeding buffers\n",
    "        ts = np.arange(out_arr[row, 1], ts_end)\n",
    "        \n",
    "        zs=Z[ts,p-index_adjust] #JOBARRAY INDEX_ADJUST\n",
    "        ys=Y[ts,p-index_adjust] #JOBARRAY INDEX_ADJUST\n",
    "        xs=X[ts,p-index_adjust] #JOBARRAY INDEX_ADJUST\n",
    "    \n",
    "        ws=W[ts,p-index_adjust] #JOBARRAY INDEX_ADJUST\n",
    "        qcqis=QCQI[ts,p-index_adjust] #JOBARRAY INDEX_ADJUST\n",
    "        where=np.where((ws>=w_thresh2) & (qcqis>=qcqi_thresh))\n",
    "        profile_array[zs[where],0]+=1\n",
    "    del after\n",
    "    # all_cloudbase=zhs[np.where(profile_array[:,0]!=0)[0][0]]\n",
    "    nonzero_indices = np.where(profile_array[:, 0] != 0)[0]\n",
    "    if len(nonzero_indices) > 0:\n",
    "        all_cloudbase = zhs[nonzero_indices[0]]\n",
    "    else:\n",
    "        all_cloudbase = np.nan\n",
    "    return all_cloudbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3446b02-1ec7-404d-8ca8-241469c8917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_LFC(out_arr, Z, Y, X, LFC_FULL, index_adjust):\n",
    "    #FINDING MEAN CLOUD BASE \n",
    "    zhs=data1['zh'].values\n",
    "    w_thresh2=0.5\n",
    "    qcqi_thresh=1e-6\n",
    "    type='all'\n",
    "    \n",
    "    lfc_array =np.zeros((1, 2)) #column 1: var, column 2: counter, column 3: list of zhs\n",
    "    \n",
    "    # cloudbase_lst=[]\n",
    "    after=4 #20 minutes\n",
    "    for row in range(out_arr.shape[0]):\n",
    "        if np.mod(row,3000)==0: print(f'{row}/{out_arr.shape[0]}')\n",
    "        p=out_arr[row,0]\n",
    "        \n",
    "        # ts=np.arange(out_arr[row,4],out_arr[row,5]+1 + after)\n",
    "        ts_end = min(out_arr[row, 2] + 1 + after, len(data1['time'])) #this takes care of exceeding buffers\n",
    "        ts = np.arange(out_arr[row, 1], ts_end)\n",
    "        \n",
    "        zs=Z[ts,p-index_adjust] #JOBARRAY INDEX_ADJUST\n",
    "        ys=Y[ts,p-index_adjust] #JOBARRAY INDEX_ADJUST\n",
    "        xs=X[ts,p-index_adjust] #JOBARRAY INDEX_ADJUST\n",
    "    \n",
    "        lfcs=LFC_FULL[ts,p-index_adjust] #JOBARRAY INDEX_ADJUST #*******\n",
    "        lfcs=lfcs[lfcs>0]\n",
    "        lfc_array[0,0]+=np.sum(lfcs);lfc_array[0,1]+=len(lfcs)\n",
    "    del after\n",
    "    return lfc_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58807ef-676b-4bbf-9f58-50788e788cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_dict = {key: [] for key in [\n",
    "    'CL_ALL_out_arr', 'CL_SHALLOW_out_arr', 'CL_DEEP_out_arr',\n",
    "    'nonCL_ALL_out_arr', 'nonCL_SHALLOW_out_arr', 'nonCL_DEEP_out_arr',\n",
    "\n",
    "    'SBZ_ALL_out_arr', 'nonSBZ_ALL_out_arr',\n",
    "    'SBZ_SHALLOW_out_arr', 'nonSBZ_SHALLOW_out_arr',\n",
    "    'SBZ_DEEP_out_arr', 'nonSBZ_DEEP_out_arr',\n",
    "\n",
    "    'ColdPool_ALL_out_arr', 'ColdPool_SHALLOW_out_arr',\n",
    "    'ColdPool_DEEP_out_arr'\n",
    "]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff7faf1-c8f8-4d96-ad44-a989d8e75ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "#RUNNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd2d365-cb07-4f96-9bc7-04d3466aff89",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "#JOB ARRAY SETUP\n",
    "################################\n",
    "#*#*\n",
    "# how many total jobs are being run? i.e. array=1-100 ==> num_jobs=100\n",
    "if Np_str=='1e6':\n",
    "    num_jobs=60 #1M parcels\n",
    "    num_slurm_jobs=10\n",
    "if Np_str=='50e6':\n",
    "    num_jobs=200 #50M parcels\n",
    "    num_slurm_jobs=60\n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f9472a-64c6-402e-8ae3-fe171756508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "#RUNNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894a2aaf-d436-43d7-951f-0a30fb1d9671",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[start_slurm_job,end_slurm_job]=StartSlurmJobArray(num_jobs=num_jobs,num_slurm_jobs=num_slurm_jobs,ISRUN=True) #if ISRUN is False, then will not run using slurm_job_array\n",
    "print(f\"Running on Slurm_Jobs for Slurm_Job_Ids: {(start_slurm_job,end_slurm_job-1)}\")\n",
    "\n",
    "all_cloudbase=[]\n",
    "job_id_list=np.arange(start_slurm_job,end_slurm_job)\n",
    "for job_id in job_id_list:\n",
    "    if job_id % 10: print(f\"current job_id: {job_id}\")\n",
    "    \n",
    "    [start_job, end_job, index_adjust] = StartJobArray(job_id)\n",
    "    [W, QCQI, Z, Y, X, parcel_z] = GetData(start_job, end_job) \n",
    "    LFC_FULL = GetLFCData(start_job, end_job) \n",
    "\n",
    "    #CL vs nonCL\n",
    "    [CL_ALL_out_arr, nonCL_ALL_out_arr] = GetALLArrays_CL(start_job,end_job,index_adjust)\n",
    "    [CL_SHALLOW_out_arr, nonCL_SHALLOW_out_arr] = GetSHALLOWArrays_CL(CL_ALL_out_arr, nonCL_ALL_out_arr,index_adjust)\n",
    "    [CL_DEEP_out_arr, nonCL_DEEP_out_arr] = GetDEEPArrays_CL(CL_ALL_out_arr, nonCL_ALL_out_arr,index_adjust)\n",
    "\n",
    "    #SBZ vs nonSBZ\n",
    "    [SBZ_ALL_out_arr, nonSBZ_ALL_out_arr,\n",
    "     SBZ_SHALLOW_out_arr, nonSBZ_SHALLOW_out_arr,\n",
    "     SBZ_DEEP_out_arr, nonSBZ_DEEP_out_arr] = GetArrays_SBZ(CL_ALL_out_arr, index_adjust)\n",
    "    \n",
    "    # ColdPool\n",
    "    ColdPool_ALL_out_arr = get_ColdPool(CL_ALL_out_arr, nonSBZ_ALL_out_arr)\n",
    "    ColdPool_SHALLOW_out_arr = get_ColdPool(CL_SHALLOW_out_arr, nonSBZ_SHALLOW_out_arr)\n",
    "    ColdPool_DEEP_out_arr = get_ColdPool(CL_DEEP_out_arr, nonSBZ_DEEP_out_arr)\n",
    "\n",
    "    # Create a dictionary of arrays to save (including SBZ arrays)\n",
    "    data_dict = {\n",
    "        'CL_ALL_out_arr': CL_ALL_out_arr,\n",
    "        'CL_SHALLOW_out_arr': CL_SHALLOW_out_arr,\n",
    "        'CL_DEEP_out_arr': CL_DEEP_out_arr,\n",
    "        'nonCL_ALL_out_arr': nonCL_ALL_out_arr,\n",
    "        'nonCL_SHALLOW_out_arr': nonCL_SHALLOW_out_arr,\n",
    "        'nonCL_DEEP_out_arr': nonCL_DEEP_out_arr,\n",
    "    \n",
    "        'SBZ_ALL_out_arr': SBZ_ALL_out_arr,\n",
    "        'nonSBZ_ALL_out_arr': nonSBZ_ALL_out_arr,\n",
    "        'SBZ_SHALLOW_out_arr': SBZ_SHALLOW_out_arr,\n",
    "        'nonSBZ_SHALLOW_out_arr': nonSBZ_SHALLOW_out_arr,\n",
    "        'SBZ_DEEP_out_arr': SBZ_DEEP_out_arr,\n",
    "        'nonSBZ_DEEP_out_arr': nonSBZ_DEEP_out_arr,\n",
    "    \n",
    "        'ColdPool_ALL_out_arr': ColdPool_ALL_out_arr,\n",
    "        'ColdPool_SHALLOW_out_arr': ColdPool_SHALLOW_out_arr,\n",
    "        'ColdPool_DEEP_out_arr': ColdPool_DEEP_out_arr\n",
    "    }\n",
    "\n",
    "    #ADDING ANOTHER COLUMN TO STORE THE AFTER ARRAYS\n",
    "    for key, arr in data_dict.items():\n",
    "        new_column = np.zeros((arr.shape[0], 1), dtype=int)\n",
    "        data_dict[key] = np.hstack((arr, new_column))\n",
    "\n",
    "    # Compute after-arrays\n",
    "    after_dict = compute_after_arrays(data_dict, W, QCQI, index_adjust)\n",
    "    # Adding to Fourth Column\n",
    "    data_dict=AddColumn(data_dict,after_dict)\n",
    "\n",
    "    #GETTING THE COUNT\n",
    "    for key in count_dict:\n",
    "        count_dict[key].append(data_dict[key].shape[0])\n",
    "\n",
    "    #GETTING CLOUDBASE ZLEVEL\n",
    "    cloudbase=get_mean_cloud_base(CL_ALL_out_arr, Z, Y, X, W, QCQI, index_adjust)\n",
    "    all_cloudbase.append(cloudbase)\n",
    "\n",
    "    #GETTING LFC PROFILE\n",
    "    LFC_profile=get_mean_LFC(CL_ALL_out_arr, Z, Y, X, LFC_FULL, index_adjust)\n",
    "    \n",
    "    # Call SaveData with the dictionary\n",
    "    SaveAllCloudBase_Job(all_cloudbase,job_id)\n",
    "    SaveLFC_Profile_Job(LFC_profile,job_id)\n",
    "    SaveData({**data_dict, **after_dict}, job_id) # SaveData(data_dict, job_id) \n",
    "\n",
    "combined_counts={key: sum(counts) for key, counts in count_dict.items()}\n",
    "print(combined_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00f1f3b-3402-4a18-a582-5c6df6fa2e15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2010b8-e058-4cda-b36f-fdd2c3a61f3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4008baac-8f32-4d3d-be88-509797b5b36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "#RECOMBINING\n",
    "recombine=False #KEEP FALSE WHEN RUNNING\n",
    "# recombine=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70796f21-cc5d-49ba-b938-67d173a0df9e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def ReadData(var_name,job_id):\n",
    "    dir2 = dir + f'Project_Algorithms/Tracking_Algorithms/subsetting/'\n",
    "    out_file=dir2+f\"parcel_tracking_SUBSET_{res}_{t_res}_{Np_str}_{job_id}\"\n",
    "    with h5py.File(out_file, 'r') as f:\n",
    "        out = f[var_name][:]\n",
    "    return out\n",
    "# ReadData(var_name,job_id)\n",
    "def SaveFinalData(dict,out_file):    \n",
    "    with h5py.File(out_file,'w') as f:\n",
    "        for key in dict:\n",
    "            print(key)\n",
    "            f.create_dataset(key, data=dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc16b17-520c-4f2a-aa36-e56dcc0e4674",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if recombine==True:\n",
    "    #GETTING COUNTS FOR MAKING INITIAL RECOMBINED ARRAYS LATER\n",
    "    count_dict = {key: [] for key in [\n",
    "        'CL_ALL_out_arr', 'CL_SHALLOW_out_arr', 'CL_DEEP_out_arr',\n",
    "        'nonCL_ALL_out_arr', 'nonCL_SHALLOW_out_arr', 'nonCL_DEEP_out_arr',\n",
    "    \n",
    "        'SBZ_ALL_out_arr', 'nonSBZ_ALL_out_arr',\n",
    "        'SBZ_SHALLOW_out_arr', 'nonSBZ_SHALLOW_out_arr',\n",
    "        'SBZ_DEEP_out_arr', 'nonSBZ_DEEP_out_arr',\n",
    "    \n",
    "        'ColdPool_ALL_out_arr', 'ColdPool_SHALLOW_out_arr',\n",
    "        'ColdPool_DEEP_out_arr'\n",
    "    ]}\n",
    "    def MakeCount(count_dict):\n",
    "        print('Getting Tracked Parcel Count')\n",
    "        for job_id in np.arange(1,num_jobs+1):\n",
    "            if job_id % 10==0: print(f\"current job_id: {job_id}\")\n",
    "            for key in count_dict:\n",
    "                data_dict_key=ReadData(key,job_id)\n",
    "                count_dict[key].append(data_dict_key.shape[0])\n",
    "        combined_counts={key: sum(counts) for key, counts in count_dict.items()}\n",
    "        return combined_counts\n",
    "    combined_counts=MakeCount(count_dict)\n",
    "    print(combined_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5b1c84-0ff2-44ad-aa98-e016d20cb977",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if recombine==True:\n",
    "    def MakeEmpty(counts_dict):\n",
    "        empty_dict = {}\n",
    "        for key, count in counts_dict.items():\n",
    "            empty_dict[key] = np.zeros((count, 4), dtype=int)\n",
    "        return empty_dict\n",
    "    dict=MakeEmpty(combined_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbfc370-40af-4038-ad44-de3a8e934d97",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if recombine==True:\n",
    "    for job_id in np.arange(1,num_jobs+1):\n",
    "        if job_id % 10==0: print(f\"current job_id: {job_id}\")\n",
    "            \n",
    "        for key in dict:\n",
    "            var=ReadData(key,job_id)\n",
    "            if var.size!=0:\n",
    "                a=np.where(np.all(dict[key] == 0, axis=1))[0][0]\n",
    "                b=a+var.shape[0]\n",
    "                # print(key,a,b) #TESTING\n",
    "                dict[key][a:b]=var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc55e99-3946-432b-a02a-cc4dce153fad",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if recombine==True:\n",
    "    dir2 = dir + f'Project_Algorithms/Tracking_Algorithms/OUTPUT/'\n",
    "    out_file=dir2+f\"parcel_tracking_SUBSET_{res}_{t_res}_{Np_str}.h5\"\n",
    "    SaveFinalData(dict,out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f64a9c-90df-4b9c-a7d2-7ac87f1b505b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#RECOMBINING AVERAGE CLOUDBASE\n",
    "if recombine==True:\n",
    "    #STORING THE MINIMUM CLOUDBASE INFORMATION\n",
    "    def SaveAllCloudBase_Final(all_cloudbase):\n",
    "        dir2 = dir + f'Project_Algorithms/Tracking_Algorithms/OUTPUT/'\n",
    "        out_file = dir2 + f\"all_cloudbase_{res}_{t_res}_{Np_str}.pkl\"\n",
    "        with open(out_file, 'wb') as f:\n",
    "            pickle.dump(np.array(all_cloudbase), f)\n",
    "    def GetAllCloudBase(job_id):\n",
    "        dir2 = dir + f'Project_Algorithms/Tracking_Algorithms/'\n",
    "        in_file=dir2+f'subsetting/all_cloudbase_{res}_{t_res}_{Np_str}_{job_id}.pkl'\n",
    "        with open(in_file, 'rb') as f:\n",
    "            cloudbase_job=pickle.load(f)\n",
    "        return cloudbase_job\n",
    "\n",
    "    #############\n",
    "    def CombineCloudBase():\n",
    "        all_cloudbase=[]\n",
    "        for job_id in np.arange(1,num_jobs+1):\n",
    "            all_cloudbase+=list(GetAllCloudBase(job_id))\n",
    "        all_cloudbase=np.array(all_cloudbase)\n",
    "        SaveAllCloudBase_Final(all_cloudbase)\n",
    "        return all_cloudbase\n",
    "    #############\n",
    "    #RUNNING\n",
    "    all_cloudbase=CombineCloudBase()\n",
    "    print(np.mean(all_cloudbase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992c7ee5-2581-4cab-a516-71080bfe5012",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#RECOMBINING AVERAGE LFC\n",
    "if recombine==True:\n",
    "    #STORING THE MINIMUM CLOUDBASE INFORMATION\n",
    "    def SaveMeanLFC_Final(all_cloudbase):\n",
    "        dir2 = dir + f'Project_Algorithms/Tracking_Algorithms/OUTPUT/'\n",
    "        out_file = dir2 + f\"MeanLFC_{res}_{t_res}_{Np_str}.pkl\"\n",
    "        with open(out_file, 'wb') as f:\n",
    "            pickle.dump(np.array(all_cloudbase), f)\n",
    "    def GetMeanLFCBase(job_id):\n",
    "        dir2 = dir + f'Project_Algorithms/Tracking_Algorithms/'\n",
    "        in_file=dir2+f'subsetting/LFC_Profile_{res}_{t_res}_{Np_str}_{job_id}.pkl'\n",
    "        with open(in_file, 'rb') as f:\n",
    "            LFC_profile_job=pickle.load(f)\n",
    "        return LFC_profile_job\n",
    "\n",
    "    #############\n",
    "    def CombineCloudBase():\n",
    "        MeanLFC=np.zeros((1,2))\n",
    "        for job_id in np.arange(1,num_jobs+1):\n",
    "            MeanLFC+=GetMeanLFCBase(job_id)\n",
    "        MeanLFC_Final=(MeanLFC[:,0]/MeanLFC[:,1])[0]\n",
    "        SaveMeanLFC_Final(MeanLFC_Final)\n",
    "        return MeanLFC_Final\n",
    "    #############\n",
    "    #RUNNING\n",
    "    MeanLFC_Final=CombineCloudBase()\n",
    "    print(np.mean(MeanLFC_Final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1d9500-58f1-415f-bfbc-f2d081f8ebbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77967e2c-5792-4d03-8b7f-42b10388ed33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e6a6a6-4dc3-483f-bbb0-a2dfb895e09c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c105c157-f0c9-48e4-b290-4ad0db81d4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#READING BACK IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a0a11b-0696-4445-86a1-aface69db5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#READING BACK IN\n",
    "########################\n",
    "def LoadFinalData(in_file):\n",
    "    dict = {}\n",
    "    with h5py.File(in_file, 'r') as f:\n",
    "        for key in f.keys():\n",
    "            dict[key] = f[key][:]\n",
    "    return dict\n",
    "\n",
    "def LoadAllCloudBase():\n",
    "    dir2 = dir + f'Project_Algorithms/Tracking_Algorithms/OUTPUT/'\n",
    "    in_file = dir2 + f\"all_cloudbase_{res}_{t_res}_{Np_str}.pkl\"\n",
    "    with open(in_file, 'rb') as f:\n",
    "        all_cloudbase = pickle.load(f)\n",
    "    return(all_cloudbase)\n",
    "min_all_cloudbase=np.nanmin(LoadAllCloudBase())\n",
    "print(f\"Minimum Cloudbase is: {min_all_cloudbase*1000:.0f} m\")\n",
    "\n",
    "def LoadMeanLFC():\n",
    "    dir2 = dir + f'Project_Algorithms/Tracking_Algorithms/OUTPUT/'\n",
    "    in_file = dir2 + f\"MeanLFC_{res}_{t_res}_{Np_str}.pkl\"\n",
    "    with open(in_file, 'rb') as f:\n",
    "        MeanLFC = pickle.load(f)\n",
    "    return MeanLFC\n",
    "MeanLFC=LoadMeanLFC()\n",
    "print(f\"Mean LFC is: {MeanLFC:.0f} m\\n\")\n",
    "\n",
    "dir2 = dir + f'Project_Algorithms/Tracking_Algorithms/OUTPUT/'\n",
    "in_file=dir2+f\"parcel_tracking_SUBSET_{res}_{t_res}_{Np_str}.h5\"\n",
    "final_dict=LoadFinalData(in_file)\n",
    "\n",
    "\n",
    "#DYNAMICALLY CREATING VARIABLES\n",
    "for key, value in final_dict.items():\n",
    "    globals()[key] = value\n",
    "\n",
    "# #DYNAMICALLY PRINTING VARIABLE SIZES\n",
    "# for key in final_dict:\n",
    "#     print(f\"{key} has {final_dict[key].shape[0]} parcels\")\n",
    "\n",
    "# PRINTING VARIABLE SIZES (ONE BY ONE)\n",
    "print(f'ALL: {len(CL_ALL_out_arr)} CL parcels and {len(nonCL_ALL_out_arr)} nonCL parcels')\n",
    "print(f'SHALLOW: {len(CL_SHALLOW_out_arr)} CL parcels and {len(nonCL_SHALLOW_out_arr)} nonCL parcels')\n",
    "print(f'DEEP: {len(CL_DEEP_out_arr)} CL parcels and {len(nonCL_DEEP_out_arr)} nonCL parcels')\n",
    "print('\\n')\n",
    "print(f'ALL: {len(SBZ_ALL_out_arr)} SBZ parcels and {len(nonSBZ_ALL_out_arr)} nonSBZ parcels')\n",
    "print(f'SHALLOW: {len(SBZ_SHALLOW_out_arr)} SBZ parcels and {len(nonSBZ_SHALLOW_out_arr)} nonSBZ parcels')\n",
    "print(f'DEEP: {len(SBZ_DEEP_out_arr)} SBZ parcels and {len(nonSBZ_DEEP_out_arr)} nonSBZ parcels')\n",
    "print('\\n')\n",
    "print(f'ALL: {len(ColdPool_ALL_out_arr)} ColdPool parcels')\n",
    "print(f'SHALLOW: {len(ColdPool_SHALLOW_out_arr)} ColdPool parcels')\n",
    "print(f'DEEP: {len(ColdPool_DEEP_out_arr)} ColdPool parcels')\n",
    "\n",
    "\n",
    "def apply_job_array_filter(start_job, end_job):\n",
    "    # print(\"APPLYING JOB ARRAY\")\n",
    "\n",
    "    def job_filter(arr):\n",
    "        return arr[(arr[:, 0] >= start_job) & (arr[:, 0] < end_job)]\n",
    "\n",
    "    target_names = [\n",
    "        'CL_ALL_out_arr', 'nonCL_ALL_out_arr',\n",
    "        'CL_SHALLOW_out_arr', 'nonCL_SHALLOW_out_arr',\n",
    "        'CL_DEEP_out_arr', 'nonCL_DEEP_out_arr',\n",
    "        'SBZ_ALL_out_arr', 'nonSBZ_ALL_out_arr',\n",
    "        'SBZ_SHALLOW_out_arr', 'nonSBZ_SHALLOW_out_arr',\n",
    "        'SBZ_DEEP_out_arr', 'nonSBZ_DEEP_out_arr',\n",
    "        'ColdPool_ALL_out_arr', 'ColdPool_SHALLOW_out_arr', 'ColdPool_DEEP_out_arr'\n",
    "    ]\n",
    "\n",
    "    for name in target_names:\n",
    "        globals()[name+'_2'] = job_filter(globals()[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5eb9b9-0d52-4727-867e-111d8e298bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf938bb1-d68e-4f2d-9c70-145c4d9f28e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "#TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3701c0-023b-447b-9a8d-1433666c789e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # dx = 1 km; Np = 1M; Nt = 5 min\n",
    "# data1=xr.open_dataset(dir+'../cm1r20.3/run/cm1out_1km_5min.nc') #***\n",
    "# parcel1=xr.open_dataset(dir+'../cm1r20.3/run/cm1out_pdata_1km_5min_1e6.nc') #***\n",
    "# res='1km';t_res='5min'\n",
    "# Np_str='1e6'\n",
    "\n",
    "# # # dx = 1km; Np = 50M\n",
    "# # #Importing Model Data\n",
    "# # dir2='/home/air673/koa_scratch/'\n",
    "# # data1=xr.open_dataset(dir2+'cm1out_1km_1min.nc') #***\n",
    "# # parcel1=xr.open_dataset(dir2+'cm1out_pdata_1km_1min_50M.nc') #***\n",
    "# # res='1km'; t_res='1min'; Np_str='50e6'\n",
    "\n",
    "# # # dx = 1km; Np = 50M; Nz = 95\n",
    "# # #Importing Model Data\n",
    "# # dir2='/home/air673/koa_scratch/'\n",
    "# # data1=xr.open_dataset(dir2+'cm1out_1km_1min_95nz.nc') #***\n",
    "# # parcel1=xr.open_dataset(dir2+'cm1out_pdata_1km_1min_95nz.nc') #***\n",
    "# # res='1km'; t_res='1min_95nz'; Np_str='50e6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e18937-0013-45f0-bcba-1e5c54d4f964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #LOADING BACK IN\n",
    "# def load_file():\n",
    "#     in_file=dir+f'Project_Algorithms/Tracking_Algorithms/parcel_tracking_{res}_{t_res}_{Np_str}.h5'\n",
    "#     with h5py.File(in_file, 'r') as hf:\n",
    "#         out_arr=hf['out_arr'][:]\n",
    "#         save_arr=hf['save_arr'][:]\n",
    "#         save2_arr=hf['save2_arr'][:]\n",
    "#     return out_arr,save_arr,save2_arr\n",
    "# [out_arr,save_arr,save2_arr]=load_file()\n",
    "# arr=out_arr.copy()\n",
    "# # print(np.where(np.all(arr==0,axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2bd2f1-9d4f-4fa4-af27-b99d62730ae0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def LoadAllCloudBase():\n",
    "#     dir2 = dir + f'Project_Algorithms/Tracking_Algorithms/'\n",
    "#     in_file = dir2 + f\"all_cloudbase_{res}_{t_res}_{Np_str}.pkl\"\n",
    "#     with open(in_file, 'rb') as f:\n",
    "#         all_cloudbase = pickle.load(f)\n",
    "#     return(all_cloudbase)\n",
    "# min_all_cloudbase=np.nanmin(LoadAllCloudBase())\n",
    "# all_cloudbase=min_all_cloudbase\n",
    "# print(f\"Minimum Cloudbase is: {min_all_cloudbase}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32151f29-e489-4808-8b75-846919e380a1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def LoadMeanLFC():\n",
    "#     dir2 = dir + f'Project_Algorithms/Tracking_Algorithms/'\n",
    "#     in_file = dir2 + f\"MeanLFC_{res}_{t_res}_{Np_str}.pkl\"\n",
    "#     with open(in_file, 'rb') as f:\n",
    "#         MeanLFC = pickle.load(f)\n",
    "#     return MeanLFC\n",
    "# MeanLFC=LoadMeanLFC()\n",
    "# print(f\"Mean LFC is: {MeanLFC}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "work"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
