{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6227fd-3528-4a7c-a01c-766f1cc4449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.lines as mlines\n",
    "import xarray as xr\n",
    "import os; import time\n",
    "import pickle\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bb50bb-c237-444e-a36c-10760712ea28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAIN DIRECTORIES\n",
    "mainDirectory='/mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/'\n",
    "scratchDirectory='/home/air673/koa_scratch/'\n",
    "codeDirectory='/mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/Project_Algorithms/Tracking_Algorithms'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5ba24c-18e9-461c-bb8b-398089866560",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADING DATA\n",
    "def GetDataDirectories(simulationNumber):\n",
    "    if simulationNumber == 1:\n",
    "        Directory=os.path.join(mainDirectory,'Model/cm1r20.3/run')\n",
    "        res='1km'; t_res='5min'; Np_str='1e6'; Nz_str='34'\n",
    "    elif simulationNumber == 2:\n",
    "        Directory=scratchDirectory\n",
    "        res='1km'; t_res='1min'; Np_str='50e6'; Nz_str='95'\n",
    "    elif simulationNumber == 3:\n",
    "        Directory=scratchDirectory\n",
    "        res='250m'; t_res='1min'; Np_str='50e6'; Nz_str='95'\n",
    "        \n",
    "    dataDirectory = os.path.join(Directory, f\"cm1out_{res}_{t_res}_{Nz_str}nz.nc\")\n",
    "    parcelDirectory = os.path.join(Directory,f\"cm1out_pdata_{res}_{t_res}_{Np_str}np.nc\")\n",
    "    return dataDirectory, parcelDirectory, res,t_res,Np_str,Nz_str\n",
    "    \n",
    "def GetData(dataDirectory, parcelDirectory):\n",
    "    dataNC = xr.open_dataset(dataDirectory, decode_timedelta=True) \n",
    "    parcelNC = xr.open_dataset(parcelDirectory, decode_timedelta=True) \n",
    "    return dataNC,parcelNC\n",
    "\n",
    "def SubsetDataVars(dataNC):\n",
    "    varList = [\"thflux\", \"qvflux\", \"tsk\", \"cape\", \n",
    "               \"cin\", \"lcl\", \"lfc\", \"th\",\n",
    "               \"prs\", \"rho\", \"qv\", \"qc\",\n",
    "               \"qr\", \"qi\", \"qs\",\"qg\", \n",
    "               \"buoyancy\", \"uinterp\", \"vinterp\", \"winterp\",]\n",
    "    \n",
    "    varList += [\"ptb_hadv\", \"ptb_vadv\", \"ptb_hidiff\", \"ptb_vidiff\",\n",
    "                \"ptb_hturb\", \"ptb_vturb\", \"ptb_mp\", \"ptb_rdamp\", \n",
    "                \"ptb_rad\", \"ptb_div\", \"ptb_diss\",]\n",
    "    \n",
    "    varList += [\"qvb_hadv\", \"qvb_vadv\", \"qvb_hidiff\", \"qvb_vidiff\", \n",
    "                \"qvb_hturb\", \"qvb_vturb\", \"qvb_mp\",]\n",
    "    \n",
    "    varList += [\"wb_hadv\", \"wb_vadv\", \"wb_hidiff\", \"wb_vidiff\",\n",
    "                \"wb_hturb\", \"wb_vturb\", \"wb_pgrad\", \"wb_rdamp\", \"wb_buoy\",]\n",
    "\n",
    "    return dataNC[varList]\n",
    "\n",
    "[dataDirectory,parcelDirectory, res,t_res,Np_str,Nz_str] = GetDataDirectories(simulationNumber=1)\n",
    "[data,parcel] = GetData(dataDirectory, parcelDirectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f46b96-75d6-46ad-8485-036542d177bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "job_array=False;index_adjust=0\n",
    "ocean_fraction=2/8\n",
    "start_time = time.time();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6d70b6-cd5a-426d-8fa3-6397ccb77b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dc51b8-3c08-4410-b67a-b9dde326adeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "dir2='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "path=dir2+'../Functions/'\n",
    "sys.path.append(path)\n",
    "\n",
    "import NumericalFunctions\n",
    "from NumericalFunctions import * # import NumericalFunctions \n",
    "import PlottingFunctions\n",
    "from PlottingFunctions import * # import PlottingFunctions\n",
    "\n",
    "\n",
    "# # Get all functions in NumericalFunctions\n",
    "# import inspect\n",
    "# functions = [f[0] for f in inspect.getmembers(NumericalFunctions, inspect.isfunction)]\n",
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25327d59-57b4-4898-94d8-9e75e77f7079",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Reading Back Data Later\n",
    "# ##################################################################\n",
    "# import h5py\n",
    "# dir2=dir+'Project_Algorithms/Lagrangian_Binary_Array/'\n",
    "# binary_array_path=dir2+f'lagrangian_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "# def read_ZYX(t,p):\n",
    "#     with h5py.File(binary_array_path, 'r') as f:\n",
    "#         # Load the dataset by its name\n",
    "#         Z_tp = f['Z'][t,p]\n",
    "#         Y_tp = f['Y'][t,p]\n",
    "#         X_tp = f['X'][t,p]\n",
    "#     return Z_tp,Y_tp,X_tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc579a53-f1a1-4cf4-ad9c-87947cfc415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CALLING IN TRACKED PARCELS DATA\n",
    "##################################################################\n",
    "job_array=False\n",
    "start_job=500_000;end_job=600_000;index_adjust=start_job\n",
    "parcel=parcel.isel(xh=slice(start_job,end_job))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27e378a-d959-4770-ba08-c474f8571553",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir2=dir+'Project_Algorithms/Lagrangian_Arrays/OUTPUT/'\n",
    "in_file=dir2+f'lagrangian_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "with h5py.File(in_file, 'r') as f:\n",
    "    # Load the dataset by its name\n",
    "    W = f['W'][:,start_job:end_job]\n",
    "    Z = f['Z'][:,start_job:end_job]\n",
    "    Y = f['Y'][:,start_job:end_job]\n",
    "    X = f['X'][:,start_job:end_job]\n",
    "    parcel_z=f['z'][:,start_job:end_job]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c56fdc-88f8-457f-9f2a-208ee2aa0484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Back Data Later\n",
    "##############\n",
    "import h5py\n",
    "dir2=dir+'Project_Algorithms/Lagrangian_Arrays/OUTPUT/'\n",
    "open_file=dir2+f'lagrangian_LFC_LCL_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "with h5py.File(open_file, 'r') as f:\n",
    "    # Load the dataset by its name\n",
    "    LFC = f['LFC'][:,start_job:end_job]\n",
    "    LCL = f['LCL'][:,start_job:end_job]\n",
    "\n",
    "LFC_data=data['lfc'].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc4aa7e-dce6-482b-ae50-84ce48903e46",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #USING ALL TRACKED PARCELS\n",
    "# ################################################\n",
    "# dir2 = dir + f'Project_Algorithms/Tracking_Algorithms/'\n",
    "# in_file=dir2+f\"parcel_tracking_{res}_{t_res}_{Np_str}.h5\"\n",
    "# final_dict=LoadFinalData(in_file)\n",
    "# ALL_out_arr=final_dict['out_arr']\n",
    "# def job_filter(arr):\n",
    "#     return arr[(arr[:, 0] >= start_job) & (arr[:, 0] < end_job)]\n",
    "# ALL_out_arr=job_filter(ALL_out_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8008f5-ae52-4759-ab6d-be6cd99769f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "#READING BACK IN\n",
    "def LoadFinalData(in_file):\n",
    "    dict = {}\n",
    "    with h5py.File(in_file, 'r') as f:\n",
    "        for key in f.keys():\n",
    "            dict[key] = f[key][:]\n",
    "    return dict\n",
    "\n",
    "def LoadAllCloudBase():\n",
    "    dir2 = dir + f'Project_Algorithms/Tracking_Algorithms/OUTPUT/'\n",
    "    in_file = dir2 + f\"all_cloudbase_{res}_{t_res}_{Np_str}.pkl\"\n",
    "    with open(in_file, 'rb') as f:\n",
    "        all_cloudbase = pickle.load(f)\n",
    "    return(all_cloudbase)\n",
    "min_all_cloudbase=np.nanmin(LoadAllCloudBase())\n",
    "print(f\"Minimum Cloudbase is: {min_all_cloudbase}\\n\")\n",
    "\n",
    "dir2 = dir + f'Project_Algorithms/Tracking_Algorithms/OUTPUT/'\n",
    "in_file=dir2+f\"parcel_tracking_SUBSET_{res}_{t_res}_{Np_str}.h5\"\n",
    "final_dict=LoadFinalData(in_file)\n",
    "\n",
    "\n",
    "#DYNAMICALLY CREATING VARIABLES\n",
    "for key, value in final_dict.items():\n",
    "    globals()[key] = value\n",
    "\n",
    "# #DYNAMICALLY PRINTING VARIABLE SIZES\n",
    "# for key in final_dict:\n",
    "#     print(f\"{key} has {final_dict[key].shape[0]} parcels\")\n",
    "\n",
    "# PRINTING VARIABLE SIZES (ONE BY ONE)\n",
    "print(f'ALL: {len(CL_ALL_out_arr)} CL parcels and {len(nonCL_ALL_out_arr)} nonCL parcels')\n",
    "print(f'SHALLOW: {len(CL_SHALLOW_out_arr)} CL parcels and {len(nonCL_SHALLOW_out_arr)} nonCL parcels')\n",
    "print(f'DEEP: {len(CL_DEEP_out_arr)} CL parcels and {len(nonCL_DEEP_out_arr)} nonCL parcels')\n",
    "print('\\n')\n",
    "print(f'ALL: {len(SBZ_ALL_out_arr)} SBZ parcels and {len(nonSBZ_ALL_out_arr)} nonSBZ parcels')\n",
    "print(f'SHALLOW: {len(SBZ_SHALLOW_out_arr)} SBZ parcels and {len(nonSBZ_SHALLOW_out_arr)} nonSBZ parcels')\n",
    "print(f'DEEP: {len(SBZ_DEEP_out_arr)} SBZ parcels and {len(nonSBZ_DEEP_out_arr)} nonSBZ parcels')\n",
    "print('\\n')\n",
    "print(f'ALL: {len(ColdPool_ALL_out_arr)} ColdPool parcels')\n",
    "print(f'SHALLOW: {len(ColdPool_SHALLOW_out_arr)} ColdPool parcels')\n",
    "print(f'DEEP: {len(ColdPool_DEEP_out_arr)} ColdPool parcels')\n",
    "\n",
    "\n",
    "#APPLYING JOB ARRAY\n",
    "job_array=True\n",
    "if job_array==True:\n",
    "    print('APPLYING JOB ARRAY')\n",
    "    def job_filter(arr):\n",
    "        return arr[(arr[:,0]>=start_job)&(arr[:,0]<end_job)]\n",
    "    for name in [\n",
    "        'CL_ALL_out_arr', 'nonCL_ALL_out_arr',\n",
    "        'CL_SHALLOW_out_arr', 'nonCL_SHALLOW_out_arr',\n",
    "        'CL_DEEP_out_arr', 'nonCL_DEEP_out_arr',\n",
    "        'SBZ_ALL_out_arr', 'nonSBZ_ALL_out_arr',\n",
    "        'SBZ_SHALLOW_out_arr', 'nonSBZ_SHALLOW_out_arr',\n",
    "        'SBZ_DEEP_out_arr', 'nonSBZ_DEEP_out_arr',\n",
    "        'ColdPool_ALL_out_arr', 'ColdPool_SHALLOW_out_arr', 'ColdPool_DEEP_out_arr'\n",
    "    ]:\n",
    "        globals()[name] = job_filter(globals()[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac81b0f-c5c3-43a6-a800-2bd863e90b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "#PLOTTING TRAJECTORIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7788a8-f19a-47a3-b520-b161b0662d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_arr = CL_ALL_out_arr\n",
    "# out_arr = CL_SHALLOW_out_arr\n",
    "out_arr = CL_DEEP_out_arr\n",
    "\n",
    "# ---- SBZ options (corrected)\n",
    "# out_arr = SBZ_ALL_out_arr\n",
    "# out_arr = SBZ_SHALLOW_out_arr\n",
    "# out_arr = SBZ_DEEP_out_arr\n",
    "\n",
    "# ---- ColdPool options (corrected)\n",
    "# out_arr = ColdPool_ALL_out_arr\n",
    "# out_arr = ColdPool_SHALLOW_out_arr\n",
    "# out_arr = ColdPool_DEEP_out_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3613b725-f088-47d9-8469-16dd13ab0c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def TrajectoryPlot(i):\n",
    "    \n",
    "    # Assuming 'data', 'ALL_out_arr', 'parcel', 'LFC_data', 'Z', 'T', 'Y', 'X', etc., are already defined\n",
    "    \n",
    "    # Define grid for the plot and legend\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    gs = gridspec.GridSpec(1, 2, width_ratios=[0.85, 0.15])  # 1 row, 2 columns, second column is smaller for legend\n",
    "    \n",
    "    # Define the first subplot for the trajectory plot\n",
    "    ax1 = plt.subplot(gs[0])\n",
    "    \n",
    "    # Define time steps and other data\n",
    "    num_mins=1 #5\n",
    "    tsteps = np.arange(len(data['time'])) * num_mins / 60\n",
    "    tsteps = np.round(tsteps, 2)\n",
    "    \n",
    "    p = out_arr[i, 0]\n",
    "    t1 = out_arr[i, 1]\n",
    "    t2 = out_arr[i, 2]\n",
    "    \n",
    "    z1 = parcel_z[t1, p-index_adjust]\n",
    "    z2 = parcel_z[t2, p-index_adjust]\n",
    "    \n",
    "    zs = parcel_z[:, p-index_adjust]\n",
    "    zgrids = data['zh'][Z[:, p-index_adjust]] * 1000\n",
    "    \n",
    "    ts=np.arange(len(parcel['time']))\n",
    "    lfcs = LFC_data[ts, Y[:, p-index_adjust], X[:, p-index_adjust]]\n",
    "    lfcs2 = LFC[:,p-index_adjust]*1000; #print(lfcs)\n",
    "    \n",
    "    \n",
    "    # Plot parcel trajectory, grid, and LFC\n",
    "    ax1.plot(tsteps, zs, color='black', label='parcel location')\n",
    "    ax1.plot(tsteps, zgrids, linestyle='dashed', color='grey', label='parcel gridbox')\n",
    "    ax1.plot(tsteps, lfcs, color='green', label='LFC')\n",
    "    # ax1.plot(tsteps, lfcs2, color='green', label='LFC')\n",
    "    ax1.scatter(t1 * num_mins / 60, z1, color='red', s=50, label='first time in BL')\n",
    "    ax1.scatter(t2 * num_mins / 60, z2, color='green', s=50, label='first time above LFC')\n",
    "    \n",
    "    # Labels and title\n",
    "    ax1.set_xlabel('timestep (hr)')\n",
    "    ax1.set_ylabel('z (km)')\n",
    "    ax1.set_title(f'Parcel #{p} Trajectory')\n",
    "    \n",
    "    # Set up legend in the second subplot (outside of the plot)\n",
    "    # Adjusting bbox_to_anchor to move legend outside\n",
    "    ax1.legend(bbox_to_anchor=(1.05, 0.5), loc='center left', fontsize=10)\n",
    "    \n",
    "    # Adjust layout to prevent overlap\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "i=1\n",
    "i+=1\n",
    "TrajectoryPlot(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a039922b-3cff-41d0-b2c0-d54425c482b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrajectoryPlot(out_arr, p, title, ax, ax_legend=None):\n",
    "    num_mins = 1\n",
    "    tsteps = np.round(np.arange(len(data['time'])) * num_mins / 60, 2)\n",
    "\n",
    "    # p = out_arr[i, 0]\n",
    "    # print(np.where(out_arr[:, 0]==p))\n",
    "    i=np.where(out_arr[:, 0]==p)[0][0]\n",
    "    t1 = out_arr[i, 1]\n",
    "    t2 = out_arr[i, 2]\n",
    "\n",
    "    z1 = parcel_z[t1, p-index_adjust]\n",
    "    z2 = parcel_z[t2, p-index_adjust]\n",
    "\n",
    "    zs = parcel_z[:, p-index_adjust]\n",
    "    zgrids = data['zh'][Z[:, p-index_adjust]] * 1000\n",
    "    ts = np.arange(len(parcel['time']))\n",
    "    lfcs = LFC_data[ts, Y[:, p-index_adjust], X[:, p-index_adjust]]\n",
    "\n",
    "    # Main plot\n",
    "    ax.plot(tsteps, zs, color='black', label='parcel location')\n",
    "    ax.plot(tsteps, zgrids, linestyle='dashed', color='grey', label='parcel gridbox')\n",
    "    ax.plot(tsteps, lfcs, color='green', label='LFC')\n",
    "    ax.scatter(t1 * num_mins / 60, z1, color='red', s=50, label='first time in BL')\n",
    "    ax.scatter(t2 * num_mins / 60, z2, color='green', s=50, label='first time above LFC')\n",
    "\n",
    "    ax.set_xlabel('timestep (hr)')\n",
    "    ax.set_ylabel('z (km)')\n",
    "    # ax.set_title(f'{title} Parcel #{p} Trajectory')\n",
    "    ax.set_title(f'{title} Trajectory (p = {p})')\n",
    "\n",
    "    # Skip internal legend if shared\n",
    "    if ax_legend is not None:\n",
    "        ax_legend.axis('off')\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        ax_legend.legend(handles, labels, loc='center', fontsize=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1a78c6-d29f-4259-ba21-39b5e530dde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CL TRACKING PLOT FROM \"eulerian_CL_tracking_Plotting.ipynb\"\n",
    "\n",
    "def get_conv(t,z):\n",
    "    import h5py\n",
    "    # print('calculating convergence and taking mean')\n",
    "    if res=='1km':\n",
    "        dir2='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "    elif res=='250m':\n",
    "        dir2='/home/air673/koa_scratch/'\n",
    "    \n",
    "    file_path = dir2 + 'Variable_Calculation/OUTPUT/' + 'Convergence' + f'_{res}_{t_res}' + '.h5'\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        Conv = f['conv'][t,z]\n",
    "    return Conv\n",
    "\n",
    "def LoadTrackedData(ONLY_SBZ):\n",
    "    # Define the path to your output file\n",
    "    ONLY_SBZ = False  # or True\n",
    "    varname = 'ONLY_SBZS' if ONLY_SBZ else 'ALL_CLS'\n",
    "    \n",
    "    # File path\n",
    "    file_path = f'/mnt/lustre/koa/koastore/torri_group/air_directory/' \\\n",
    "                f'DCI-Project/Project_Algorithms/Tracking_Algorithms/' \\\n",
    "                f'CL_Tracking_Out/' \\\n",
    "                f'whereCL_{res}_{t_res}_{varname}.h5'\n",
    "    \n",
    "    # Open dataset (as it's valid NetCDF)\n",
    "    whereCL = xr.open_dataset(file_path, engine='netcdf4')['maxconv_x'] #chunks=dict(time=1, z=-1, y=-1,x=-1)\n",
    "    return whereCL\n",
    "\n",
    "ONLY_SBZ=False\n",
    "if ONLY_SBZ==True:\n",
    "    whereCL_ONLY_SBZS=LoadTrackedData(ONLY_SBZ)\n",
    "elif ONLY_SBZ==False:\n",
    "    whereCL_ALL_CLS=LoadTrackedData(ONLY_SBZ)\n",
    "####PLOT FORM /FIGURES.ipynb\n",
    "def CL_plotting(data1, t, zlev, ax, ONLY_SBZ, font_size=12, index_adjust=0):\n",
    "    if np.mod(t, 10) == 0:\n",
    "        print(f'Current time step: {t}/{len(data1[\"time\"])}')\n",
    "\n",
    "    print('Loading Data')\n",
    "    conv_z = get_conv(t, zlev)\n",
    "    print('Plotting')\n",
    "\n",
    "    dx = np.round(data1['xf'][1] - data1['xf'][0], 2).item()\n",
    "    dy = dx\n",
    "\n",
    "    xh = data1['xh'] - data1['xh'][0]\n",
    "    yh = data1['yh'] - data1['yh'][0]\n",
    "\n",
    "    contour = ax.contourf(xh, yh, conv_z * 1000, levels=40)\n",
    "\n",
    "    #COLORBAR THINGS\n",
    "    # cbar = plt.colorbar(contour, ax=ax, pad=0)\n",
    "    # cbar.set_label(r'$-\\nabla \\cdot \\vec{V}_H\\ (s^{-1})$', fontsize=font_size)\n",
    "    # cbar.ax.tick_params(labelsize=font_size)\n",
    "    # cbar.ax.yaxis.label.set_size(font_size)\n",
    "\n",
    "    ##########\n",
    "    from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "    # Create inset axes for vertical colorbar inside 'ax'\n",
    "    cax = inset_axes(ax,\n",
    "                     width=\"2%\",        # narrow colorbar width\n",
    "                     height=\"100%\",     # full height of main axis\n",
    "                     loc='center right',# vertically centered on right edge\n",
    "                     bbox_to_anchor=(0, 0, 1, 1),\n",
    "                     bbox_transform=ax.transAxes,\n",
    "                     borderpad=0)\n",
    "    \n",
    "    # Add colorbar in the inset axis\n",
    "    cbar = plt.colorbar(contour, cax=cax)\n",
    "    ##########\n",
    "    \n",
    "    # Set label and tick params as before\n",
    "    cbar.set_label(r'$-\\nabla \\cdot \\vec{V}_H\\ (s^{-1})$', fontsize=font_size)\n",
    "    cbar.ax.tick_params(labelsize=font_size)\n",
    "    cbar.ax.yaxis.label.set_size(font_size)\n",
    "\n",
    "\n",
    "    #TIME\n",
    "    ([days, hours, mins], _) = get_time(data1, t + index_adjust, (0, 6, 0))\n",
    "    z_value = data1['zf'][zlev].values * 1000\n",
    "\n",
    "    # ax.set_title(f'Horizontal Convergence at t = {t + index_adjust} = {days}:{hours}:{mins}, z = {z_value:.0f} m\\nWith Tracked Convergence Local Y-Maxima Overlayed',fontsize=font_size)\n",
    "    ax.set_xlabel('x (km)', fontsize=font_size)\n",
    "    ax.set_ylabel('y (km)', fontsize=font_size)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=font_size)\n",
    "\n",
    "    # Scatter max convergence points\n",
    "    for yind in range(len(data1['yh'])):\n",
    "        if ONLY_SBZ:\n",
    "            local_maxes = whereCL_ONLY_SBZS[t, zlev, yind].data\n",
    "        else:\n",
    "            local_maxes = whereCL_ALL_CLS[t, zlev, yind].data\n",
    "        local_maxes = local_maxes[local_maxes != -1]\n",
    "        local_maxes = local_maxes.astype(int)\n",
    "        # ax.scatter(local_maxes, [yind] * len(local_maxes), color='red', s=1)\n",
    "        ax.scatter(xh[local_maxes], [yh[yind]] * len(local_maxes), color='red', s=1)\n",
    "\n",
    "\n",
    "    # Coastline\n",
    "    # ax.axvline(x=len(data1['xf']) * ocean_fraction, color='black', linewidth=1.5, label='Coastline')\n",
    "    ax.axvline(x=data['xf'].data[-1] * ocean_fraction, color='black', linewidth=1.5, label='Coastline')\n",
    "    \n",
    "\n",
    "    # Legend\n",
    "    handle_pts = mlines.Line2D([], [], color='red', marker='o', linestyle='None', markersize=6, label='Convergence Local Y-Maxima')\n",
    "    handle_time = mlines.Line2D([], [], color='none', label=f't = {t + index_adjust} = {days}:{hours}:{mins}')\n",
    "    handle_z = mlines.Line2D([], [], color='none', label=f'z = {data1[\"zh\"][zlev].item() * 1000:.0f} m')\n",
    "    handle_c = mlines.Line2D([], [], color='black', lw=3, label='Coastline')\n",
    "    legend = ax.legend(handles=[handle_pts, handle_c, handle_time, handle_z], loc='upper left', fontsize=font_size)\n",
    "    for text in legend.get_texts():\n",
    "        text.set_fontsize(font_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d3ffd9-02f4-4575-9044-e333285f8d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parcel indices\n",
    "parcel_ids = [500502, 510091, 506551, 599889]\n",
    "out_arrs = [CL_SHALLOW_out_arr, SBZ_SHALLOW_out_arr, CL_DEEP_out_arr, SBZ_DEEP_out_arr]\n",
    "titles = ['CL SHALLOW', 'SBZ SHALLOW', 'CL DEEP', 'SBZ DEEP']\n",
    "\n",
    "# Create figure with 2 rows, 3 columns\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "gs = gridspec.GridSpec(2, 5, hspace=0.4, wspace=0.3,\n",
    "                      width_ratios=[1, 1, 0.35, 1, 1]) #column ratios\n",
    "\n",
    "# Define axes for the 2x2 parcel plots\n",
    "plot_positions = [(1, 0), (1, 1), (1, 3), (1, 4)]\n",
    "axs = [plt.subplot(gs[row, col]) for row, col in plot_positions]\n",
    "\n",
    "# Create the convergence plot axis spanning bottom row, cols 2-3\n",
    "ax_convergence = plt.subplot(gs[0,:])\n",
    "\n",
    "# Create the legend axis (top right)\n",
    "ax_legend = plt.subplot(gs[1, 2])\n",
    "\n",
    "#### CONVERGENCE PLOT\n",
    "# Plot convergence in its own axis (add your plotting function here)\n",
    "ocean_fraction=2/8\n",
    "t = 100 if t_res == '5min' else 100 * 5\n",
    "if (res=='1km' and t_res=='5min') or (res=='1km' and t_res=='1min'):\n",
    "    zlev=3\n",
    "else: \n",
    "    zlev=7\n",
    "CL_plotting(data, t, zlev, ax_convergence, ONLY_SBZ=False)\n",
    "ax_convergence.set_title(\"Convergence Plot\")  # Customize as needed\n",
    "####\n",
    "\n",
    "# TRAJECTORY PLOTS\n",
    "for p, ax, out_arr, title in zip(parcel_ids, axs, out_arrs, titles):\n",
    "    TrajectoryPlot(out_arr, p, title, ax, ax_legend=None)\n",
    "\n",
    "\n",
    "# Shared legend setup\n",
    "handles, labels = axs[0].get_legend_handles_labels()\n",
    "ax_legend.axis('off')\n",
    "ax_legend.legend(handles, labels, loc='center', fontsize=11, frameon=False)\n",
    "\n",
    "#SAVING PLOT\n",
    "fig.savefig(f\"PLOTS/ConvergenceContour_TrackedTrajectory_{res}_{t_res}_{Np_str}.jpg\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a16f34-ba74-49b7-a867-3fbfad18241f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da90ad4b-4689-4a7e-955e-9d1d10730b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9daa00-aee6-4c72-8079-ae4d519cd3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d764a1-1ffb-447e-937a-83489448ae33",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#VARIABLE TRAJECTORIES (OLD)\n",
    "##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bb0d91-e36e-4258-96c0-757cf74d6303",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "##OLD\n",
    "\n",
    "# #VARIABLES AND GET UNITED FUNCTION\n",
    "# variables = data.variables\n",
    "# long_names = {var: {\n",
    "#         'long_name': data[var].attrs.get('long_name', ''),\n",
    "#         'units': data[var].attrs.get('units', '')\n",
    "#     } for var in variables}\n",
    "\n",
    "\n",
    "# w_vars = {var: long_name for var, long_name in long_names.items() if var.startswith('wb') or var in ['w']}\n",
    "# ptb_vars = {var: long_name for var, long_name in long_names.items() if var.startswith('ptb') or var in ['th']}\n",
    "# qvb_vars = {var: long_name for var, long_name in long_names.items() if var.startswith('qvb') or var in ['qv']}\n",
    "# vars = {}\n",
    "# vars.update(w_vars)\n",
    "# vars.update(ptb_vars)\n",
    "# vars.update(qvb_vars)\n",
    "\n",
    "# var_names=list(vars.keys())\n",
    "# long_names = [info['long_name'] for info in vars.values()]\n",
    "# units = [info['units'] for info in vars.values()]\n",
    "# def convert_to_latex(unit):\n",
    "#     parts = unit.split('/')  # Split the unit into parts\n",
    "#     latex_unit = parts[0]  # Start with the first part\n",
    "#     for part in parts[1:]:  # Iterate over the remaining parts\n",
    "#         latex_unit += r\"\\ \" + part + r\"^{-1}\"  # Add the part with a '^{-1}' exponent\n",
    "#     return r\"$\" + latex_unit + r\"$\"\n",
    "# latex_units = [convert_to_latex(unit) for unit in units]\n",
    "\n",
    "# var_names.append(\"qv'\")\n",
    "# long_names.append('perturbation water vapor mixing ratio')\n",
    "# latex_units.append(r'$kg\\ kg^{-1}')\n",
    "\n",
    "# var_names.append(\"th'\")\n",
    "# long_names.append('perturbation theta')\n",
    "# latex_units.append(r'$K')\n",
    "\n",
    "# def get_unit(var,var_names):\n",
    "#     where=np.where(np.array(var_names)==var)[0]\n",
    "#     final_unit=np.array(latex_units)[where][0]\n",
    "#     if ('wb' in var) or ('qvb' in var) or ('thb' in var) or (\"'\" in var):\n",
    "#         final_unit+=' /1000'\n",
    "#     return final_unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6b877c-7567-43a0-aad1-0859bcb3fbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #PLOTTING FUNCTION\n",
    "# def plot_variable(var_box_interp, var, Ng, var_names):\n",
    "#     def add_grid(Ng,ax):\n",
    "#         # #adding grid lines\n",
    "#         # ygrids=np.arange(0,(Ng-1),0.5)\n",
    "#         # xgrids=np.arange(0,(Ng-1),0.5)\n",
    "#         ywhich=np.linspace(0,(Ng-1),Ng+1)\n",
    "#         xwhich=np.linspace(0,(Ng-1),Ng+1)\n",
    "       \n",
    "#         # ywhich=ygrids[np.where((ygrids % 1) == 0.5)]\n",
    "#         # xwhich=xgrids[np.where((xgrids % 1) == 0.5)]\n",
    "        \n",
    "#         for (xind, yind) in zip(xwhich, ywhich):\n",
    "#             ax.axhline(yind, color='white',linestyle='--',label=None) #add grid\n",
    "#             ax.axvline(xind, color='white',linestyle='--',label=None) #add grid\n",
    "    \n",
    "#     # Create figure and main gridspec (1 row, 3 columns)\n",
    "#     fig = plt.figure(figsize=(14, 6),constrained_layout=False)\n",
    "#     gs = GridSpec(1, 3, width_ratios=[2, 0.1, 1.5], figure=fig, wspace=0.3)  # Reduce space between left and middle panel\n",
    "    \n",
    "#     # Create a 3x3 gridspec within the left panel\n",
    "#     gs_left = GridSpecFromSubplotSpec(3, 3, subplot_spec=gs[0, 0], wspace=0.1, hspace=0.2)\n",
    "#     # gs_left=gs[0].subgridspec(3, 3, wspace=0, hspace=0)\n",
    "    \n",
    "#     # Loop over timesteps and create subplots in 3x3 format (left to right, top to bottom)\n",
    "    \n",
    "#     if var=='qv':\n",
    "#         cmap='Blues'\n",
    "#     elif var==\"qv'\":\n",
    "#         cmap='RdBu_r'\n",
    "#     else:\n",
    "#         cmap='viridis' #cmap='RdBu_r'\n",
    "    \n",
    "#     n_levels=30\n",
    "#     vmin=var_box_interp.min();vmax=var_box_interp.max()#;vmin=-vmax\n",
    "#     levels = np.linspace(vmin, vmax, n_levels)\n",
    "#     # import matplotlib.colors as mcolors\n",
    "#     # # norm = Normalize(vmin=vmin, vmax=vmax)\n",
    "#     # norm = mcolors.BoundaryNorm(boundaries=levels, ncolors=n_levels)\n",
    "#     # cmap = plt.get_cmap(cmap, n_levels)\n",
    "    \n",
    "#     for t in range(9):\n",
    "#         row, col = divmod(t, 3)  # Get row, column indices for 3x3 layout\n",
    "#         ax = fig.add_subplot(gs_left[row, col])\n",
    "#         im = ax.contourf(var_box_interp[t],cmap=cmap,levels=levels,vmin=vmin,vmax=vmax)#,norm=norm)\n",
    "#         add_grid(Ng,ax)\n",
    "#         ax.set_title(f't = {t-1}', fontsize=10)\n",
    "            \n",
    "#         ax.set_xticks(range(var_box_interp.shape[2]))\n",
    "#         ax.set_yticks(range(var_box_interp.shape[1]))    \n",
    "#         ax.set_xlabel('x (km)');ax.set_ylabel('y (km)')\n",
    "        \n",
    "#         # ax.set_xticks(np.arange(-2, 3)) \n",
    "#         # ax.set_yticks(np.arange(-2, 3))\n",
    "        \n",
    "#         # Hide major tick labels selectively (not the ticks themselves)\n",
    "#         if col != 0:\n",
    "#             ax.set_yticklabels([])\n",
    "#             ax.set_ylabel('')\n",
    "#         if row != 2:\n",
    "#             ax.set_xticklabels([])\n",
    "#             ax.set_xlabel('')\n",
    "    \n",
    "        \n",
    "    \n",
    "#     # Middle panel: Create a subplot for the colorbar\n",
    "#     ax_cbar = fig.add_subplot(gs[0, 1])  # Use the middle narrow panel\n",
    "#     cbar = fig.colorbar(im, cax=ax_cbar)\n",
    "#     cbar.ax.yaxis.set_ticks_position('left')\n",
    "#     # ax_cbar.set_title(label=get_unit(var,var_names), fontsize=10)\n",
    "    \n",
    "#     # Right panel placeholder\n",
    "#     ax_right = fig.add_subplot(gs[0, 2])\n",
    "#     ax_right.set_title(f\"{var} over Avg CL-LFC Trajectory\")\n",
    "    \n",
    "#     ax_right.plot(var_box_interp[:,2,2],color='black');\n",
    "#     ax_right.set_ylabel(get_unit(var,var_names))\n",
    "#     ax_right.set_xlabel(f'timesteps (5 mins)')\n",
    "\n",
    "#     fig.show()\n",
    "#     return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3633ca7c-56a0-4554-9419-c3fd09fdb258",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def save_plot(fig, filename, dir='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/Project_Algorithms/Tracking_Algorithms/plots/'):\n",
    "#     # Ensure the directory exists\n",
    "#     if not os.path.exists(dir):\n",
    "#         os.makedirs(dir)\n",
    "\n",
    "#     # Construct the full file path\n",
    "#     file_path = os.path.join(dir, filename)\n",
    "\n",
    "#     # Save the figure to the file\n",
    "#     fig.savefig(file_path, dpi=300)\n",
    "#     print(f\"plot saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c47afb2-4b9b-40b6-acbd-942e641b41ac",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "##OLD\n",
    "#  # t_mean=int(np.floor(np.mean(out_arr[:,2]+after+1-(out_arr[:,1]+1))))\n",
    "# t_mean=9\n",
    "# Nx=len(data['xh'])\n",
    "# Ny=len(data['yh'])\n",
    "\n",
    "# box_size = 2\n",
    "# Ng = 2*box_size+1\n",
    "        \n",
    "# def get_var(var,t_val,z_val,y_slice,x_slice):\n",
    "#     if var in  np.array(list(w_vars.keys())):\n",
    "#         var_data=data[var].isel(time=t_val,\n",
    "#                                 yh=y_slice, xh=x_slice).interp(zf=data['zh']).isel(zh=z_val)\n",
    "#     elif var==\"qv'\" or var==\"th'\":\n",
    "#         var_data=data['qv'].isel(time=t_val,zh=z_val)\n",
    "#         var_data-=var_data.mean(dim=('yh','xh'))\n",
    "#         var_data=var_data.isel(yh=y_slice,xh=x_slice)\n",
    "#     else:\n",
    "#         var_data=data[var].isel(time=t_val,zh=z_val,yh=y_slice,xh=x_slice)\n",
    "\n",
    "#     if ('wb' in var) or ('qvb' in var) or ('thb' in var) or (\"'\" in var):\n",
    "#         var_data*=1000\n",
    "#     return var_data\n",
    "    \n",
    "# def get_trajectory(var,row):\n",
    "#     #SETUP\n",
    "#     ######\n",
    "#     out_arr=ALL_out_arr\n",
    "#     box_size = 2\n",
    "#     Ng = 2*box_size+1\n",
    "#     # after=4 #20 mins\n",
    "#     after=0 #TESTING***\n",
    "    \n",
    "#     #GETTING INDICES\n",
    "#     ################\n",
    "#     p=out_arr[row,0]\n",
    "#     t1=out_arr[row,1]\n",
    "#     t2=out_arr[row,2]\n",
    "    \n",
    "#     ts=np.arange(t1-1,t2+after+1)\n",
    "\n",
    "#     # zs=Z[ts,p]\n",
    "#     # ys=Y[ts,p]\n",
    "#     # xs=X[ts,p]\n",
    "\n",
    "#     [zs,ys,xs]=read_ZYX(ts,p) #NEW\n",
    "    \n",
    "#     #MAKING DATA\n",
    "#     ############\n",
    "#     var_box=np.zeros((len(ts),Ng,Ng))\n",
    "\n",
    "#     # Skip parcels that cross the boundary\n",
    "#     if np.any((ys - box_size < 0) | (ys + box_size >= Ny) | \n",
    "#           (xs - box_size < 0) | (xs + box_size >= Nx)):\n",
    "#         raise ValueError('Crossed Boundary')\n",
    "        \n",
    "#     # Loop through each (xs, ys) pair and slice the data\n",
    "#     for i in range(len(ts)):\n",
    "#         # Extract the variable for the valid parcel\n",
    "#         var_box[i] = get_var(\n",
    "#             var,\n",
    "#             ts[i],\n",
    "#             zs[i], \n",
    "#             slice(ys[i] - box_size, ys[i] + box_size + 1), \n",
    "#             slice(xs[i] - box_size, xs[i] + box_size + 1)\n",
    "#         )\n",
    "    \n",
    "#     #TIME INTERPOLATION\n",
    "#     ####################\n",
    "#     # Create interpolation function along axis 0 (time)\n",
    "#     interp_func = interp1d(np.linspace(0, 1, var_box.shape[0]), var_box, axis=0, kind='linear', fill_value='extrapolate')\n",
    "    \n",
    "#     # Interpolate to new time steps\n",
    "#     var_box_interp = interp_func(np.linspace(0, 1, t_mean))\n",
    "    \n",
    "#     return var_box,var_box_interp\n",
    "\n",
    "# print(var_names)\n",
    "# var=var_names[10]\n",
    "# row=10;[var_box1,var_box_interp1]=get_trajectory(var=var,row=row)\n",
    "# #PLOTTING\n",
    "# fig=plot_variable(var_box_interp1, var, Ng, var_names)\n",
    "# #SAVING\n",
    "# save_plot(fig=fig,filename=f\"Parcel_{row}_Trajectory_{var}_{res}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322c81b0-129f-4c2d-b712-9f66190caa58",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #######################################################################\n",
    "# RUNNING FOR MEAN OF ALL TRACKED TRAJECTORY (OLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9515202e-faeb-4e48-8551-16a2bc2cd818",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # t_mean=int(np.floor(np.mean(out_arr[:,2]+after+1-(out_arr[:,1]+1))))\n",
    "# t_mean=9\n",
    "# Nx=len(data['xh'])\n",
    "# Ny=len(data['yh'])\n",
    "# def get_var(var,t_val,z_val,y_slice,x_slice):\n",
    "#     if var in  np.array(list(w_vars.keys())):\n",
    "#         var_data=data[var].isel(time=t_val,\n",
    "#                                 yh=y_slice, xh=x_slice).interp(zf=data['zh']).isel(zh=z_val)\n",
    "#     elif var==\"qv'\" or var==\"th'\":\n",
    "#         var_data=data['qv'].isel(time=t_val,zh=z_val)\n",
    "#         var_data-=var_data.mean(dim=('yh','xh'))\n",
    "#         var_data=var_data.isel(yh=y_slice,xh=x_slice)\n",
    "#     else:\n",
    "#         var_data=data[var].isel(time=t_val,zh=z_val,yh=y_slice,xh=x_slice)\n",
    "\n",
    "#     if ('wb' in var) or ('qvb' in var) or ('thb' in var) or (\"'\" in var):\n",
    "#         var_data*=1000\n",
    "#     return var_data\n",
    "\n",
    "# #SETUP\n",
    "# ######\n",
    "# # out_arr=ALL_out_arr\n",
    "# # out_arr=SHALLOW_out_arr\n",
    "# out_arr=DEEP_out_arr\n",
    "# box_size = 2\n",
    "# Ng = 2*box_size+1\n",
    "# # after=4 #20 mins\n",
    "# after=0\n",
    "# def get_mean_trajectory(var):\n",
    "#     var_box_interp=np.zeros((t_mean,Ng,Ng))\n",
    "\n",
    "#     for row in np.arange(out_arr.shape[0]):\n",
    "#         if np.mod(row,10)==0: print(f'current row {row}')\n",
    "        \n",
    "#         #GETTING INDICES\n",
    "#         ################\n",
    "#         p=out_arr[row,0]\n",
    "#         t1=out_arr[row,1]\n",
    "#         t2=out_arr[row,2]\n",
    "        \n",
    "#         ts=np.arange(t1-1,t2+after+1)\n",
    "#         # zs=Z[ts,p]\n",
    "#         # ys=Y[ts,p]\n",
    "#         # xs=X[ts,p]\n",
    "\n",
    "#         [zs,ys,xs]=read_ZYX(ts,p) #NEW\n",
    "        \n",
    "#         #MAKING DATA\n",
    "#         ############\n",
    "#         var_box=np.zeros((len(ts),Ng,Ng))\n",
    "    \n",
    "#         # Skip parcels that cross the boundary\n",
    "#         if np.any((ys - box_size < 0) | (ys + box_size >= Ny) | \n",
    "#               (xs - box_size < 0) | (xs + box_size >= Nx)):\n",
    "#             continue\n",
    "            \n",
    "#         # Loop through each (xs, ys) pair and slice the data\n",
    "#         for i in range(len(ts)):\n",
    "#             # Extract the variable for the valid parcel\n",
    "#             var_box[i] = get_var(\n",
    "#                 var, \n",
    "#                 ts[i],\n",
    "#                 zs[i],\n",
    "#                 slice(ys[i] - box_size, ys[i] + box_size + 1), \n",
    "#                 slice(xs[i] - box_size, xs[i] + box_size + 1))\n",
    "            \n",
    "#         #TIME INTERPOLATION\n",
    "#         ####################\n",
    "#         # Create interpolation function along axis 0 (time)\n",
    "#         interp_func = interp1d(np.linspace(0, 1, var_box.shape[0]), var_box, axis=0, kind='linear', fill_value='extrapolate')\n",
    "        \n",
    "#         # Interpolate to new time steps\n",
    "#         var_box_interp += interp_func(np.linspace(0, 1, t_mean))\n",
    "\n",
    "#     var_box_interp/=out_arr.shape[0]\n",
    "#     return var_box_interp\n",
    "\n",
    "# var=\"w\"\n",
    "# var_box_interp2=get_mean_trajectory(var=var)\n",
    "# fig=plot_variable(var_box_interp2, var, Ng, var_names)\n",
    "# save_plot(fig=fig,filename=f\"Mean_Trajectory_{var}_{res}\")\n",
    "\n",
    "# # for var in var_names:\n",
    "# #     print(var)\n",
    "# #     var_box_interp2=get_mean_trajectory(var=var)\n",
    "# #     #PLOTTING\n",
    "# #     fig=plot_variable(var_box_interp2, var, Ng, var_names)\n",
    "# #     save_plot(fig=fig,filename=f\"Mean_Trajectory_{var}_{res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3b683f-498a-4e1c-b5ea-3b2e8eafa51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1cd5df-e7d7-450e-97d2-c243fcaaec2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reading Back Data Later\n",
    "# ##################################################################\n",
    "# #DOMAIN SUBSETTING\n",
    "# def DOMAIN_SUBSET(out_arr):\n",
    "#     print(f'length before: {len(out_arr)}')\n",
    "\n",
    "#     #SETTING UP\n",
    "#     ###########\n",
    "#     ocean_percent=2/8\n",
    "#     left_to_coast=data['xh'][0]+(data['xh'][-1]-data['xh'][0])*ocean_percent\n",
    "    \n",
    "#     where_coast_xh=np.where(data['xh']>=left_to_coast)[0][0]#-25\n",
    "#     where_coast_xf=np.where(data['xf']>=left_to_coast)[0][0]#-25\n",
    "#     end_xh=len(data['xh'])-1-50\n",
    "#     end_xf=len(data['xf'])-1-50\n",
    "    \n",
    "#     print(f'x in {0}:{where_coast_xh-1} FOR SEA')\n",
    "#     print(f'x in {where_coast_xh}:{end_xh} FOR LAND')\n",
    "\n",
    "#     if res=='1km':\n",
    "#         t_start=36 \n",
    "#     elif res=='250m':\n",
    "#         t_start=36*5 \n",
    "#     t_end=len(data['time']) \n",
    "#     print(f't in {t_start}:end (8 hours)')\n",
    "\n",
    "#     #SUBSETTING CODE\n",
    "#     ################\n",
    "#     t,p=out_arr[:,1],out_arr[:,0]\n",
    "#     if 'job_array' in globals():\n",
    "#         p -= index_adjust\n",
    "\n",
    "#     #GETTING X VALUES OF EACH PARCEL \n",
    "#     fancy_index=True\n",
    "#     # fancy_index=False\n",
    "#     if fancy_index==True:\n",
    "#         xs=X[t,p] #FANCY INDEXING\n",
    "#     elif fancy_index==False: #(slightly longer, but avoids loading X into memory)\n",
    "#         dir2=dir+'Project_Algorithms/Lagrangian_Arrays/'\n",
    "#         in_file=dir2+f'lagrangian_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "#         with h5py.File(in_file, 'r') as f:\n",
    "#             xs=[]\n",
    "#             for i, j in zip(t, p):\n",
    "#                 xs.append(f['X'][i, j])\n",
    "#             xs=np.array(xs)\n",
    "#     ################\n",
    "    \n",
    "#     out_arr=out_arr[np.where((xs>=where_coast_xh)&(xs<=end_xh))]\n",
    "#     out_arr=out_arr[np.where(out_arr[:,1]<=t_end)]\n",
    "\n",
    "#     print(f'==> length after: {len(out_arr)}'+'\\n')\n",
    "#     return out_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d48d4d-1a2b-40fd-9dec-11e57226add6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#SOME BACKUP ITEMS (OLD)\n",
    "####################################################################################################\n",
    "#READING OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e7690f-1e28-455c-a4b9-f85a9d0b889a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Reading Back Data Later\n",
    "# ##############\n",
    "# import h5py\n",
    "# dir2=dir+'Project_Algorithms/Lagrangian_Binary_Array/'\n",
    "# open_file=dir2+f'lagrangian_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "# with h5py.File(open_file, 'r') as f:\n",
    "#     # Load the dataset by its name\n",
    "\n",
    "#     W = f['W'][:]\n",
    "#     Z = f['Z'][:]\n",
    "#     Y = f['Y'][:]\n",
    "#     X = f['X'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c688f7-21a3-46d6-bffa-6c550e0a61bf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #DOMAIN SUBSETTING\n",
    "# def DOMAIN_SUBSET(out_arr):\n",
    "#     print(f'length before: {len(out_arr)}')\n",
    "\n",
    "#     ocean_percent=2/8\n",
    "#     left_to_coast=data['xh'][0]+(data['xh'][-1]-data['xh'][0])*ocean_percent\n",
    "    \n",
    "#     where_coast_xh=np.where(data['xh']>=left_to_coast)[0][0]#-25\n",
    "#     where_coast_xf=np.where(data['xf']>=left_to_coast)[0][0]#-25\n",
    "#     end_xh=len(data['xh'])-1-50\n",
    "#     end_xf=len(data['xf'])-1-50\n",
    "    \n",
    "#     print(f'x in {0}:{where_coast_xh-1} FOR SEA')\n",
    "#     print(f'x in {where_coast_xh}:{end_xh} FOR LAND')\n",
    "#     # t_end=78 \n",
    "#     # if res=='250m':t_end=410\n",
    "#     # print(f't in {0}:{t_end} (6.5 hours)')\n",
    "#     t_start=36 \n",
    "#     t_end=len(data['time'])\n",
    "#     print(f't in {t_start}:end (8 hours)')\n",
    "\n",
    "#     #SUBSETTING CODE\n",
    "#     xs=X[list(out_arr[:,1]),list(out_arr[:,0])]\n",
    "    \n",
    "#     out_arr=out_arr[np.where((xs>=where_coast_xh)&(xs<=end_xh))]\n",
    "#     out_arr=out_arr[np.where(out_arr[:,1]<=t_end)]\n",
    "\n",
    "#     print(f'length after: {len(out_arr)}')\n",
    "#     return out_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e25c97-8ca4-49a3-a6d3-98d08f2873d5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# open_file = dir+f'Project_Algorithms/Tracking_Algorithms/trackout/parcel_tracking_{res}_{t_res}_{Np_str}.nc'\n",
    "\n",
    "# out=xr.open_dataset(open_file)['out_arr'].values;out=out.astype(object);out[:, [0,1,2]] = out[:, [0,1,2]].astype(int) #***\n",
    "# save=xr.open_dataset(open_file)['save_arr'].values;save=save.astype(object);save[:, [0,1,2]] = save[:, [0,1,2]].astype(int) #***\n",
    "\n",
    "# out_arr=out[~np.all(out == 0, axis=1)];#print('list of first 10 SBZ parcels'); print(out_arr[:15])\n",
    "# save_arr=save[~np.all(save == 0, axis=1)];save_arr=save_arr[np.where(np.unique(save_arr[1:-1,0]))];#print('list of first 10 ignored parcels');print(save_arr[:5])\n",
    "\n",
    "# ###############################################################################\n",
    "# #remove duplicates\n",
    "# lst=[]\n",
    "# unique_values, counts = np.unique(out_arr[:,0], return_counts=True); duplicates = unique_values[counts > 1]\n",
    "# for elem in duplicates:\n",
    "#     idx = np.where(out_arr[:,0] == elem)[0] \n",
    "#     extras=idx[np.where(out_arr[idx,2]!=np.min(out_arr[idx,2]))]\n",
    "#     lst.extend([x for x in extras])\n",
    "# mask=np.ones(len(out_arr), dtype=bool); mask[lst] = False\n",
    "# out_arr=out_arr[mask]; \n",
    "# ###############################################################################\n",
    "\n",
    "# out_arr=out[~np.all(out == 0, axis=1)];print('list of first 10 SBZ parcels'); print(out_arr[:15])\n",
    "# save_arr=save[~np.all(save == 0, axis=1)];save_arr=save_arr[np.where(np.unique(save_arr[1:-1,0]))];print('list of first 10 ignored parcels');print(save_arr[:5])\n",
    "# placeholder=out_arr.copy(); run=True\n",
    "# ############################################################\n",
    "# print(f'there are a total of {len(out_arr)} CL parcels and {len(save_arr)} nonCL parcels')\n",
    "\n",
    "# ############################################################\n",
    "# #SUBSETTING\n",
    "# subset=True\n",
    "# if subset==True:\n",
    "#     out_arr=DOMAIN_SUBSET(out_arr)\n",
    "#     save_arr=DOMAIN_SUBSET(save_arr)\n",
    "# ############################################################\n",
    "\n",
    "# ALL_out_arr=out_arr.copy(); ALL_save_arr=save_arr.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f842536-6b47-4565-9ba3-3d12e2ffb0a8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #SHALLOW\n",
    "# parcel_z=parcel['z'].data\n",
    "\n",
    "# def ddt(f,dt=1):\n",
    "#     ddx = (\n",
    "#             f[1:  ]\n",
    "#             -\n",
    "#             f[0:-1]\n",
    "#         ) / (\n",
    "#         2 * dt\n",
    "#     )\n",
    "#     return ddx\n",
    "\n",
    "# #search for deep convective parcels within lagrangian tracking output     \n",
    "# ##############################################################\n",
    "# def SHALLOW_threshold(zthresh,type):\n",
    "\n",
    "#     if type=='CL':\n",
    "#         out_arr=ALL_out_arr.copy()\n",
    "#     elif type=='nonCL':\n",
    "#         out_arr=ALL_save_arr.copy()\n",
    "    \n",
    "#     deep_out_ind=[]; extendrange=[]\n",
    "#     times=data['time'].values/(1e9 * 60); times=times.astype(float);\n",
    "#     for ind in range(len(out_arr)): \n",
    "#         if np.mod(ind,5000)==0: print(f'{ind}/{len(out_arr)}')\n",
    "#         #CHECK TO SSEE IF NEXT MOST LOCAL TIME MAX GOES ABOVE ZTHRESHS \n",
    "\n",
    "#         #Get Ascending Range Past LFC For Maximum 120 Minutes Simulation Time\n",
    "#         nummins=120; numsteps=int(nummins/times[1])\n",
    "#         aboverange=np.arange(out_arr[ind,2],out_arr[ind,2]+numsteps,1) #range of times between current time and numsteps later\n",
    "#         aboverange=aboverange[aboverange<len(data['time'])] #caps out at max time\n",
    "#         above=parcel_z[aboverange,out_arr[ind,0]]/1000\n",
    "    \n",
    "#         #Takes The time derivative \n",
    "#         ddx=ddt(above)\n",
    "\n",
    "#         #Checks whether the Local Time Max Is Located Above zthresh\n",
    "#         signs = np.sign(ddx)\n",
    "#         signs_diff=np.diff(signs)\n",
    "#         local_maxes=np.where((signs_diff != 0) & (signs_diff < 0))[0]+1 #make sure +1 is here\n",
    "#         if len(local_maxes)==0:\n",
    "#             local_maxes=[0]\n",
    "#         elif np.any(above[local_maxes[0]]<=zthresh): #< for SHALLOW, > for DEEP\n",
    "#             extendrange.append(local_maxes[0]) #save to extend xlim of plot later\n",
    "#             deep_out_ind.append(ind)\n",
    "\n",
    "#     #SUBSET OUT FOR FINAL RESULT\n",
    "#     out_arr=out_arr[deep_out_ind,:]\n",
    "#     # print(f'> {zthresh} km. {len(out_arr)} leftover parcels')\n",
    "#     return out_arr#, extendrange\n",
    "#     # print(out_arr)\n",
    "# ##############################################################\n",
    "\n",
    "# convectivelevel=4 #4km\n",
    "# SHALLOW_out_arr=SHALLOW_threshold(convectivelevel,type='CL')\n",
    "# SHALLOW_save_arr=SHALLOW_threshold(convectivelevel,type='nonCL')\n",
    "\n",
    "# print('list of first 10 SBZ parcels'); print(out_arr[:15])\n",
    "# print(f'there are a total of {len(SHALLOW_out_arr)} CL parcels and {len(SHALLOW_save_arr)} nonCL parcels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028018c9-5fe5-4859-ba71-6fa57217cd87",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #DEEP\n",
    "# parcel_z=parcel['z'].data\n",
    "\n",
    "# #search for deep convective parcels within lagrangian tracking output     \n",
    "# ##############################################################\n",
    "# def DEEP_threshold(zthresh,type):\n",
    "#     if type=='CL':\n",
    "#         out_arr=ALL_out_arr.copy()\n",
    "#     elif type=='nonCL':\n",
    "#         out_arr=ALL_save_arr.copy()\n",
    "    \n",
    "#     deep_out_ind=[]; extendrange=[]\n",
    "#     times=data['time'].values/(1e9 * 60); times=times.astype(float);\n",
    "#     for ind in range(len(out_arr)): \n",
    "#         if np.mod(ind,5000)==0: print(f'{ind}/{len(out_arr)}')\n",
    "#         #CHECK TO SSEE IF NEXT MOST LOCAL TIME MAX GOES ABOVE ZTHRESHS \n",
    "\n",
    "#         #Get Ascending Range Past LFC For Maximum 120 Minutes Simulation Time\n",
    "#         nummins=120; numsteps=int(nummins/times[1])\n",
    "#         aboverange=np.arange(out_arr[ind,2],out_arr[ind,2]+numsteps,1) #range of times between current time and numsteps later\n",
    "#         aboverange=aboverange[aboverange<len(data['time'])] #caps out at max time\n",
    "#         above=parcel_z[aboverange,out_arr[ind,0]]/1000\n",
    "        \n",
    "#         #Takes The time derivative \n",
    "#         ddx=ddt(above)\n",
    "\n",
    "#         #Checks whether the Local Time Max Is Located Above zthresh\n",
    "#         signs = np.sign(ddx)\n",
    "#         signs_diff=np.diff(signs)\n",
    "#         local_maxes=np.where((signs_diff != 0) & (signs_diff < 0))[0]+1 #make sure +1 is here\n",
    "#         if len(local_maxes)==0:\n",
    "#             local_maxes=[0]\n",
    "        \n",
    "#         if np.any(above[local_maxes[0]]>=zthresh): #< for SHALLOW, > for DEEP\n",
    "#             extendrange.append(local_maxes[0]) #save to extend xlim of plot later\n",
    "#             deep_out_ind.append(ind)\n",
    "\n",
    "#     #SUBSET OUT FOR FINAL RESULT\n",
    "#     out_arr=out_arr[deep_out_ind,:]\n",
    "#     # print(f'> {zthresh} km. {len(out_arr)} leftover parcels')\n",
    "#     return out_arr#, extendrange\n",
    "#     # print(out_arr)\n",
    "# ##############################################################\n",
    "\n",
    "# convectivelevel=6 #4km\n",
    "# DEEP_out_arr=DEEP_threshold(convectivelevel,type='CL')\n",
    "# DEEP_save_arr=DEEP_threshold(convectivelevel,type='nonCL')\n",
    "\n",
    "# print('list of first 10 SBZ parcels'); print(out_arr[:15])\n",
    "# print(f'there are a total of {len(DEEP_out_arr)} CL parcels and {len(DEEP_save_arr)} nonCL parcels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575702e3-2682-4c24-b4f3-0badb1bf7169",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #FINDING SBZ CONVERGENCE\n",
    "# parcel_z=parcel['z'].data\n",
    "\n",
    "# #FIND ALL X MAXES\n",
    "# import sys\n",
    "# dir='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "# path=dir+'../Functions'\n",
    "# sys.path.append(path)\n",
    "# # import inspect\n",
    "# # functions = [f[0] for f in inspect.getmembers(NumericalFunctions, inspect.isfunction)]\n",
    "# # functions\n",
    "\n",
    "# # TF=False\n",
    "# # if TF==False:\n",
    "# #     print('Loading In Data')\n",
    "# #     u_data=data['u'].interp(xf=data['xh']).data\n",
    "# #     v_data=data['v'].interp(yf=data['yh']).data\n",
    "# #     TF=True\n",
    "\n",
    "\n",
    "# from NumericalFunctions import *\n",
    "# def find_SBZ_xmaxs():\n",
    "    \n",
    "#     # print('calculating convergence and taking mean')\n",
    "#     # Conv=-(Ddx(u_data,1000)+Ddy(v_data,1000))\n",
    "#     dir2='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "#     file_path = dir2 + 'Variable_Calculation/' + 'Convergence' + f'_{res}_{t_res}' + '.h5'\n",
    "#     with h5py.File(file_path, 'r') as f:\n",
    "#         Conv = f['conv'][:]\n",
    "    \n",
    "#     zlev=4\n",
    "#     Conv_ymean=np.mean(Conv[:,zlev],axis=1)\n",
    "#     xmaxs=np.argmax(Conv_ymean, axis=1)\n",
    "\n",
    "#     return xmaxs\n",
    "# def find_SBZ_xmaxs():\n",
    "#     # Define the directory and file path\n",
    "#     dir2 = '/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "#     file_path = dir2 + 'Variable_Calculation/' + 'Convergence' + f'_{res}_{t_res}' + '.h5'\n",
    "    \n",
    "#     # Open the HDF5 file in read mode\n",
    "#     with h5py.File(file_path, 'r') as f:\n",
    "#         # Access the 'conv' dataset\n",
    "#         conv_dataset = f['conv']\n",
    "        \n",
    "#         # Define the vertical level you are interested in\n",
    "#         zlev = 4\n",
    "        \n",
    "#         # Initialize a list to store the xmaxs for each time step\n",
    "#         xmaxs_list = []\n",
    "\n",
    "#         # Loop over each time step (axis=0 corresponds to time)\n",
    "#         for t in range(conv_dataset.shape[0]):  # conv_dataset.shape[0] is the time dimension size\n",
    "#             # Read the relevant slice for this time step and vertical level\n",
    "#             Conv_t_zlev = conv_dataset[t, zlev, :, :]  # Shape should be (y_size, x_size)\n",
    "            \n",
    "#             # Calculate the mean across the y-axis\n",
    "#             Conv_ymean = np.mean(Conv_t_zlev, axis=0)  # Mean across the y-axis\n",
    "            \n",
    "#             # Find the index of the maximum value along the x-axis\n",
    "#             xmax = np.argmax(Conv_ymean)\n",
    "            \n",
    "#             # Append the result for this time step\n",
    "#             xmaxs_list.append(xmax)\n",
    "    \n",
    "#     # Convert the list of xmaxs to a numpy array (optional)\n",
    "#     xmaxs = np.array(xmaxs_list)\n",
    "\n",
    "#     return xmaxs #returns SBZ x location for each timestep\n",
    "\n",
    "\n",
    "# def subset_SBZ(out_arr):\n",
    "#     xmaxs=find_SBZ_xmaxs()\n",
    "\n",
    "#     SBZ_subset=[]\n",
    "#     # test=[] #TESTING\n",
    "    \n",
    "#     for ind in np.arange(out_arr.shape[0]):\n",
    "        \n",
    "#         row=out_arr[ind]\n",
    "#         p=row[0]\n",
    "#         t=row[1]\n",
    "\n",
    "#         kms=np.argmax(data['xh'].values-data['xh'][0].values >= 1)\n",
    "#         if X[t,p] in np.arange( (xmaxs[t]-2*kms),(xmaxs[t]+2*kms) +1):\n",
    "#             SBZ_subset.append(ind)\n",
    "#             # test.append(p) #TESTING\n",
    "    \n",
    "#     SBZ_out_arr=out_arr[SBZ_subset]\n",
    "#     print(f'there are a total of {len(SBZ_out_arr)} ALL SBZ CL parcels')\n",
    "\n",
    "#     valid_range=np.arange(out_arr.shape[0])\n",
    "#     nonSBZ_out_arr=out_arr[list(set(valid_range) - set(SBZ_subset))]\n",
    "#     print(f'there are a total of {len(nonSBZ_out_arr)} ALL nonSBZ CL parcels')\n",
    "#     return SBZ_out_arr,nonSBZ_out_arr\n",
    "\n",
    "\n",
    "# # #LOADING CL MAXS FROM CL TRACKING ALGORITHM\n",
    "# # folder = '/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/Project_Algorithms/Tracking_Algorithms/'\n",
    "# # whereSBZ=xr.open_dataset(folder+f'whereCL_{res}_{t_res}_ONLY_SBZS.nc').load()\n",
    "# # whereSBZ=whereSBZ.isel(time=slice(0,len(data['time'])))\n",
    "# # whereSBZ=whereSBZ['maxconv_x']\n",
    "# # def Get_SBZ_X(t,z,y):\n",
    "# #     Conv_X_Max=whereSBZ[t,z,y,:].values\n",
    "# #     return Conv_X_Max\n",
    "# # def subset_SBZ(out_arr):\n",
    "\n",
    "# #     SBZ_subset=[]\n",
    "# #     # test=[] #TESTING\n",
    "    \n",
    "# #     for ind in np.arange(out_arr.shape[0]):\n",
    "        \n",
    "# #         row=out_arr[ind]\n",
    "# #         p=row[0]\n",
    "# #         t=row[1]\n",
    "\n",
    "# #         kms=np.argmax(data['xh'].values-data['xh'][0].values >= 1)\n",
    "# #         value=X[t,p]\n",
    "# #         if np.any((value >= xmaxs - 2*kms) & (value <= xmaxs + 2*kms))==True:\n",
    "# #             SBZ_subset.append(ind)\n",
    "# #             # test.append(p) #TESTING\n",
    "    \n",
    "# #     SBZ_out_arr=out_arr[SBZ_subset]\n",
    "# #     print(f'there are a total of {len(SBZ_out_arr)} ALL SBZ CL parcels')\n",
    "\n",
    "# #     valid_range=np.arange(out_arr.shape[0])\n",
    "# #     nonSBZ_out_arr=out_arr[list(set(valid_range) - set(SBZ_subset))]\n",
    "# #     print(f'there are a total of {len(nonSBZ_out_arr)} ALL nonSBZ CL parcels')\n",
    "# #     return SBZ_out_arr,nonSBZ_out_arr\n",
    "\n",
    "# #SUBSETTING OUT SHALLOW AND DEEP FROM SBZ AND NONSBZ\n",
    "# def SHALLOW_threshold(zthresh,type):\n",
    "#     if type=='SBZ':\n",
    "#         out_arr=ALL_SBZ_out_arr.copy()\n",
    "#     elif type=='nonSBZ':\n",
    "#         out_arr=ALL_nonSBZ_out_arr.copy()\n",
    "    \n",
    "#     deep_out_ind=[]; extendrange=[]\n",
    "#     times=data['time'].values/(1e9 * 60); times=times.astype(float);\n",
    "#     for ind in range(len(out_arr)): \n",
    "#         # if np.mod(ind,5000)==0: print(f'{ind}/{len(out_arr)}')\n",
    "#         #CHECK TO SSEE IF NEXT MOST LOCAL TIME MAX GOES ABOVE ZTHRESHS \n",
    "\n",
    "#         #Get Ascending Range Past LFC For Maximum 120 Minutes Simulation Time\n",
    "#         nummins=120; numsteps=int(nummins/times[1])\n",
    "#         aboverange=np.arange(out_arr[ind,2],out_arr[ind,2]+numsteps,1) #range of times between current time and numsteps later\n",
    "#         aboverange=aboverange[aboverange<len(data['time'])] #caps out at max time\n",
    "#         above=parcel_z[aboverange,out_arr[ind,0]]/1000\n",
    "        \n",
    "#         #Takes The time derivative \n",
    "#         ddx=ddt(above)\n",
    "\n",
    "#         #Checks whether the Local Time Max Is Located Above zthresh\n",
    "#         signs = np.sign(ddx)\n",
    "#         signs_diff=np.diff(signs)\n",
    "#         local_maxes=np.where((signs_diff != 0) & (signs_diff < 0))[0]+1 #make sure +1 is here\n",
    "#         if len(local_maxes)==0:\n",
    "#             local_maxes=[0]\n",
    "        \n",
    "#         if np.any(above[local_maxes[0]]<=zthresh): #< for SHALLOW, > for DEEP\n",
    "#             extendrange.append(local_maxes[0]) #save to extend xlim of plot later\n",
    "#             deep_out_ind.append(ind)\n",
    "\n",
    "#     #SUBSET OUT FOR FINAL RESULT\n",
    "#     out_arr=out_arr[deep_out_ind,:]\n",
    "#     # print(f'> {zthresh} km. {len(out_arr)} leftover parcels')\n",
    "#     return out_arr#, extendrange\n",
    "#     # print(out_arr)\n",
    "\n",
    "# def DEEP_threshold(zthresh,type):\n",
    "#     if type=='SBZ':\n",
    "#         out_arr=ALL_SBZ_out_arr.copy()\n",
    "#     elif type=='nonSBZ':\n",
    "#         out_arr=ALL_nonSBZ_out_arr.copy()\n",
    "\n",
    "#     deep_out_ind=[]; extendrange=[]\n",
    "#     times=data['time'].values/(1e9 * 60); times=times.astype(float);\n",
    "#     for ind in range(len(out_arr)): \n",
    "#         # if np.mod(ind,5000)==0: print(f'{ind}/{len(out_arr)}')\n",
    "#         #CHECK TO SSEE IF NEXT MOST LOCAL TIME MAX GOES ABOVE ZTHRESHS \n",
    "        \n",
    "#         #Get Ascending Range Past LFC For Maximum 120 Minutes Simulation Time\n",
    "#         nummins=120; numsteps=int(nummins/times[1])\n",
    "#         aboverange=np.arange(out_arr[ind,2],out_arr[ind,2]+numsteps,1) #range of times between current time and numsteps later\n",
    "#         aboverange=aboverange[aboverange<len(data['time'])] #caps out at max time\n",
    "#         above=parcel_z[aboverange,out_arr[ind,0]]/1000\n",
    "    \n",
    "#         #Takes The time derivative \n",
    "#         ddx=ddt(above)\n",
    "\n",
    "#         #Checks whether the Local Time Max Is Located Above zthresh\n",
    "#         signs = np.sign(ddx)\n",
    "#         signs_diff=np.diff(signs)\n",
    "#         local_maxes=np.where((signs_diff != 0) & (signs_diff < 0))[0]+1 #make sure +1 is here\n",
    "#         if len(local_maxes)==0:\n",
    "#             local_maxes=[0]\n",
    "        \n",
    "#         if np.any(above[local_maxes[0]]>=zthresh): #< for SHALLOW, > for DEEP\n",
    "#             extendrange.append(local_maxes[0]) #save to extend xlim of plot later\n",
    "#             deep_out_ind.append(ind)\n",
    "\n",
    "#     #SUBSET OUT FOR FINAL RESULT\n",
    "#     out_arr=out_arr[deep_out_ind,:]\n",
    "#     # print(f'> {zthresh} km. {len(out_arr)} leftover parcels')\n",
    "#     return out_arr#, extendrange\n",
    "#     # print(out_arr)\n",
    "# ##############################################################\n",
    "# [ALL_SBZ_out_arr,ALL_nonSBZ_out_arr]=subset_SBZ(ALL_out_arr)\n",
    "# SHALLOW_SBZ_out_arr=SHALLOW_threshold(4,'SBZ')\n",
    "# print(f'there are a total of {len(SHALLOW_SBZ_out_arr)} SHALLOW SBZ CL parcels')\n",
    "# SHALLOW_nonSBZ_out_arr=SHALLOW_threshold(4,'nonSBZ')\n",
    "# print(f'there are a total of {len(SHALLOW_nonSBZ_out_arr)} SHALLOW nonSBZ CL parcels')\n",
    "# DEEP_SBZ_out_arr=DEEP_threshold(6,'SBZ')\n",
    "# print(f'there are a total of {len(DEEP_SBZ_out_arr)} DEEP SBZ CL parcels')\n",
    "# DEEP_nonSBZ_out_arr=DEEP_threshold(6,'nonSBZ')\n",
    "# print(f'there are a total of {len(DEEP_nonSBZ_out_arr)} DEEP nonSBZ CL parcels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d63e22-a9f3-4fb2-990a-32208aec5787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a740045-95b4-4b19-899d-3442df1ab7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "#NEWER OLD THINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebad5ad7-f2bd-4180-a5d8-23addf20c442",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ##OLD\n",
    "\n",
    "########################################################################\n",
    "#RUNNING FOR ONE VARIBLE TRAJECTORY ONLY\n",
    "# # Reading Back Data Later\n",
    "# ##############\n",
    "# def make_data_dict(var_names,read_type):\n",
    "#     if read_type=='h5py':\n",
    "#         with h5py.File(in_file, 'r') as f:\n",
    "#             data_dict = {var_name: f[var_name][:] for var_name in var_names}\n",
    "            \n",
    "#     elif read_type=='xarray':\n",
    "#         in_data = xr.open_dataset(\n",
    "#             in_file,\n",
    "#             engine='h5netcdf',\n",
    "#             phony_dims='sort',\n",
    "#             chunks={'phony_dim_0': 100, 'phony_dim_1': 1_000_000} \n",
    "#         )\n",
    "#         data_dict = {k: in_data[k][:].compute().data for k in var_names}\n",
    "#     return data_dict\n",
    "\n",
    "# # read_type='xarray'\n",
    "# read_type='h5py'\n",
    "# import h5py\n",
    "# dir2=dir+'Project_Algorithms/Lagrangian_Arrays/'\n",
    "# in_file=dir2+f'lagrangian_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "\n",
    "# var_names = ['Z', 'Y', 'X']\n",
    "# data_dict = make_data_dict(var_names,read_type)\n",
    "# Z, Y, X = (data_dict[k] for k in var_names)\n",
    "\n",
    "# def read_ZYX(ts,p):\n",
    "#     return Z[ts,p],Y[ts,p],X[ts,p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e5b6bb-35e4-48af-97f3-ed51eed97783",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #TESTING\n",
    "# def load_file():\n",
    "#     in_file=dir+f'Project_Algorithms/Tracking_Algorithms/parcel_tracking_{res}_{t_res}_{Np_str}.h5'\n",
    "#     with h5py.File(in_file, 'r') as hf:\n",
    "#         out_arr=hf['out_arr'][:]\n",
    "#         save_arr=hf['save_arr'][:]\n",
    "#         # save2_arr=hf['save2_arr'][:]\n",
    "#     return out_arr,save_arr\n",
    "# [out_arr,save_arr]=load_file()\n",
    "\n",
    "# out_arr=out_arr[np.where(out_arr[:,1]!=0)] #FOR NC\n",
    "# save_arr=save_arr[np.where(save_arr[:,1]!=0)] #FOR NC\n",
    "\n",
    "# print('list of first 10 ignored parcels');\n",
    "# print(f'there are a total of {len(out_arr)} CL parcels and {len(save_arr)} nonCL parcels'+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337776fe-f5d7-41ab-93bf-cdfe7e4decab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #LOADING BACK IN\n",
    "# def load_file():\n",
    "#     in_file=dir+f'Project_Algorithms/Tracking_Algorithms/parcel_tracking_{res}_{t_res}_{Np_str}.h5'\n",
    "#     with h5py.File(in_file, 'r') as hf:\n",
    "#         out_arr=hf['out_arr'][:]\n",
    "#         save_arr=hf['save_arr'][:]\n",
    "#         save2_arr=hf['save2_arr'][:]\n",
    "#     return out_arr,save_arr,save2_arr\n",
    "# [out_arr,save_arr,save2_arr]=load_file()\n",
    "\n",
    "# print('list of first 10 ignored parcels');\n",
    "# print(f'there are a total of {len(out_arr)} CL parcels and {len(save_arr)} nonCL parcels'+'\\n')\n",
    "\n",
    "# if 'job_array' in globals():\n",
    "#     #APPLYING JOB_ARRAY TO PARCEL NUMBER\n",
    "#     ####################################\n",
    "#     def job_filter(arr):\n",
    "#         return arr[(arr[:,0]>=start_job)&(arr[:,0]<end_job)]\n",
    "#     print('Applying Job Array')\n",
    "#     out_arr=job_filter(out_arr)\n",
    "#     save_arr=job_filter(save_arr)\n",
    "\n",
    "# print(f'there are a total of {len(out_arr)} CL parcels and {len(save_arr)} nonCL parcels'+'\\n')\n",
    "\n",
    "# #CHOOSING UNIQUE INDEXES\n",
    "# ###############################################################################\n",
    "# def remove_duplicates(arr):\n",
    "#     lst = []\n",
    "#     unique_values, counts = np.unique(arr[:, 0], return_counts=True)\n",
    "#     duplicates = unique_values[counts > 1]\n",
    "#     for elem in duplicates:\n",
    "#         idx = np.where(arr[:, 0] == elem)[0]\n",
    "#         extras = idx[np.where(arr[idx, 1] != np.min(arr[idx, 1]))]\n",
    "#         lst.extend(extras)\n",
    "#     mask = np.ones(len(arr), dtype=bool)\n",
    "#     mask[lst] = False\n",
    "#     return arr[mask]\n",
    "# out_arr=remove_duplicates(out_arr)\n",
    "# save_arr=remove_duplicates(save_arr)\n",
    "# ###############################################################################\n",
    "\n",
    "# ############################################################\n",
    "# #SUBSETTING\n",
    "# subset=True\n",
    "# if subset==True:\n",
    "#     out_arr=DOMAIN_SUBSET(out_arr)\n",
    "#     save_arr=DOMAIN_SUBSET(save_arr)\n",
    "# ############################################################\n",
    "\n",
    "# ALL_out_arr=out_arr.copy(); ALL_save_arr=save_arr.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e51387-c99b-4823-bf4f-012e8f390cad",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #OLD BACKUP \n",
    "# #LOADING BACK IN\n",
    "# def load_file():\n",
    "#     in_file=dir+f'Project_Algorithms/Tracking_Algorithms/parcel_tracking_{res}_{t_res}_{Np_str}.h5'\n",
    "#     with h5py.File(in_file, 'r') as hf:\n",
    "#         out_arr=hf['out_arr'][:]\n",
    "#         save_arr=hf['save_arr'][:]\n",
    "#         save2_arr=hf['save2_arr'][:]\n",
    "#     return out_arr,save_arr,save2_arr\n",
    "# [out_arr,save_arr,save2_arr]=load_file()\n",
    "\n",
    "# save_arr=save_arr[np.where(np.unique(save_arr[1:-1,0]))];\n",
    "\n",
    "# ###############################################################################\n",
    "# #remove duplicates\n",
    "# lst=[]\n",
    "# unique_values, counts = np.unique(out_arr[:,0], return_counts=True); duplicates = unique_values[counts > 1]\n",
    "# for elem in duplicates:\n",
    "#     idx = np.where(out_arr[:,0] == elem)[0] \n",
    "#     extras=idx[np.where(out_arr[idx,2]!=np.min(out_arr[idx,2]))]\n",
    "#     lst.extend([x for x in extras])\n",
    "# mask=np.ones(len(out_arr), dtype=bool); mask[lst] = False\n",
    "# out_arr=out_arr[mask]; \n",
    "# ###############################################################################\n",
    "\n",
    "# print('list of first 10 ignored parcels');print(save_arr[:5])\n",
    "# print(f'there are a total of {len(out_arr)} CL parcels and {len(save_arr)} nonCL parcels')\n",
    "\n",
    "# ############################################################\n",
    "# #SUBSETTING\n",
    "# subset=True\n",
    "# if subset==True:\n",
    "#     out_arr=DOMAIN_SUBSET(out_arr)\n",
    "#     save_arr=DOMAIN_SUBSET(save_arr)\n",
    "# ############################################################\n",
    "\n",
    "# ALL_out_arr=out_arr.copy(); ALL_save_arr=save_arr.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410e3543-0c5a-428f-8bc9-de010b13d267",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #SHALLOW\n",
    "# parcel_z=parcel['z'].data #EXTRA\n",
    "\n",
    "# def ddt(f,dt=1):\n",
    "#     ddx = (\n",
    "#             f[1:  ]\n",
    "#             -\n",
    "#             f[0:-1]\n",
    "#         ) / (\n",
    "#         2 * dt\n",
    "#     )\n",
    "#     return ddx\n",
    "\n",
    "# #search for deep convective parcels within lagrangian tracking output     \n",
    "# ##############################################################\n",
    "# def SHALLOW_threshold(zthresh,type):\n",
    "\n",
    "#     if type=='CL':\n",
    "#         out_arr=ALL_out_arr.copy()\n",
    "#     elif type=='nonCL':\n",
    "#         out_arr=ALL_save_arr.copy()\n",
    "    \n",
    "#     deep_out_ind=[]; extendrange=[]\n",
    "#     times=data['time'].values/(1e9 * 60); times=times.astype(float);\n",
    "#     for ind in range(len(out_arr)): \n",
    "#         if np.mod(ind,5000)==0: print(f'{ind}/{len(out_arr)}')\n",
    "#         #CHECK TO SSEE IF NEXT MOST LOCAL TIME MAX GOES ABOVE ZTHRESHS \n",
    "\n",
    "#         #Get Ascending Range Past LFC For Maximum 120 Minutes Simulation Time\n",
    "#         nummins=120; numsteps=int(nummins/times[1])\n",
    "#         aboverange=np.arange(out_arr[ind,2],out_arr[ind,2]+numsteps,1) #range of times between current time and numsteps later\n",
    "#         aboverange=aboverange[aboverange<len(data['time'])] #caps out at max time\n",
    "#         above=parcel_z[aboverange,out_arr[ind,0]]/1000\n",
    "    \n",
    "#         #Takes The time derivative \n",
    "#         ddx=ddt(above)\n",
    "\n",
    "#         #Checks whether the Local Time Max Is Located Above zthresh\n",
    "#         signs = np.sign(ddx)\n",
    "#         signs_diff=np.diff(signs)\n",
    "#         local_maxes=np.where((signs_diff != 0) & (signs_diff < 0))[0]+1 #make sure +1 is here\n",
    "#         if len(local_maxes)==0:\n",
    "#             local_maxes=[0]\n",
    "#         elif np.any(above[local_maxes[0]]<=zthresh): #< for SHALLOW, > for DEEP\n",
    "#             extendrange.append(local_maxes[0]) #save to extend xlim of plot later\n",
    "#             deep_out_ind.append(ind)\n",
    "\n",
    "#     #SUBSET OUT FOR FINAL RESULT\n",
    "#     out_arr=out_arr[deep_out_ind,:]\n",
    "#     # print(f'> {zthresh} km. {len(out_arr)} leftover parcels')\n",
    "#     return out_arr#, extendrange\n",
    "#     # print(out_arr)\n",
    "# ##############################################################\n",
    "\n",
    "# convectivelevel=4 #4km\n",
    "# SHALLOW_out_arr=SHALLOW_threshold(convectivelevel,type='CL')\n",
    "# SHALLOW_save_arr=SHALLOW_threshold(convectivelevel,type='nonCL')\n",
    "\n",
    "# print('list of first 10 SBZ parcels'); print(out_arr[:15])\n",
    "# print(f'there are a total of {len(SHALLOW_out_arr)} CL parcels and {len(SHALLOW_save_arr)} nonCL parcels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb726c3-01e6-4fa6-9d1c-f98827f319b8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #DEEP\n",
    "# # parcel_z=parcel['z'].data #EXTRA\n",
    "\n",
    "# #search for deep convective parcels within lagrangian tracking output     \n",
    "# ##############################################################\n",
    "# def DEEP_threshold(zthresh,type):\n",
    "#     if type=='CL':\n",
    "#         out_arr=ALL_out_arr.copy()\n",
    "#     elif type=='nonCL':\n",
    "#         out_arr=ALL_save_arr.copy()\n",
    "    \n",
    "#     deep_out_ind=[]; extendrange=[]\n",
    "#     times=data['time'].values/(1e9 * 60); times=times.astype(float);\n",
    "#     for ind in range(len(out_arr)): \n",
    "#         if np.mod(ind,5000)==0: print(f'{ind}/{len(out_arr)}')\n",
    "#         #CHECK TO SSEE IF NEXT MOST LOCAL TIME MAX GOES ABOVE ZTHRESHS \n",
    "\n",
    "#         #Get Ascending Range Past LFC For Maximum 120 Minutes Simulation Time\n",
    "#         nummins=120; numsteps=int(nummins/times[1])\n",
    "#         aboverange=np.arange(out_arr[ind,2],out_arr[ind,2]+numsteps,1) #range of times between current time and numsteps later\n",
    "#         aboverange=aboverange[aboverange<len(data['time'])] #caps out at max time\n",
    "#         above=parcel_z[aboverange,out_arr[ind,0]]/1000\n",
    "        \n",
    "#         #Takes The time derivative \n",
    "#         ddx=ddt(above)\n",
    "\n",
    "#         #Checks whether the Local Time Max Is Located Above zthresh\n",
    "#         signs = np.sign(ddx)\n",
    "#         signs_diff=np.diff(signs)\n",
    "#         local_maxes=np.where((signs_diff != 0) & (signs_diff < 0))[0]+1 #make sure +1 is here\n",
    "#         if len(local_maxes)==0:\n",
    "#             local_maxes=[0]\n",
    "        \n",
    "#         if np.any(above[local_maxes[0]]>=zthresh): #< for SHALLOW, > for DEEP\n",
    "#             extendrange.append(local_maxes[0]) #save to extend xlim of plot later\n",
    "#             deep_out_ind.append(ind)\n",
    "\n",
    "#     #SUBSET OUT FOR FINAL RESULT\n",
    "#     out_arr=out_arr[deep_out_ind,:]\n",
    "#     # print(f'> {zthresh} km. {len(out_arr)} leftover parcels')\n",
    "#     return out_arr#, extendrange\n",
    "#     # print(out_arr)\n",
    "# ##############################################################\n",
    "\n",
    "# convectivelevel=6 #4km\n",
    "# DEEP_out_arr=DEEP_threshold(convectivelevel,type='CL')\n",
    "# DEEP_save_arr=DEEP_threshold(convectivelevel,type='nonCL')\n",
    "\n",
    "# print('list of first 10 SBZ parcels'); print(out_arr[:15])\n",
    "# print(f'there are a total of {len(DEEP_out_arr)} CL parcels and {len(DEEP_save_arr)} nonCL parcels')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "work"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
