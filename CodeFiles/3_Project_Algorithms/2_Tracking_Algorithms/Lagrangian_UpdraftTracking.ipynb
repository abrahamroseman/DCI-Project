{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e77ffcf4-41b6-4939-a5fe-71b00310d7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "#ENVIRONMENT SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1db11099-d71a-464f-b01c-34d6797ef72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import matplotlib.gridspec as gridspec\n",
    "import xarray as xr\n",
    "\n",
    "import sys; import os; import time; from datetime import timedelta\n",
    "import pickle\n",
    "import h5py\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "dd960628-c670-49c9-b586-2a1aecc96c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAIN DIRECTORIES\n",
    "def GetDirectories():\n",
    "    mainDirectory='/mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/'\n",
    "    mainCodeDirectory=os.path.join(mainDirectory,\"Code/CodeFiles/\")\n",
    "    scratchDirectory='/mnt/lustre/koa/scratch/air673/'\n",
    "    codeDirectory=os.getcwd()\n",
    "    return mainDirectory,mainCodeDirectory,scratchDirectory,codeDirectory\n",
    "\n",
    "[mainDirectory,mainCodeDirectory,scratchDirectory,codeDirectory] = GetDirectories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a5465be3-bae2-4d9b-9e32-5ae8ef8577a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT CLASSES\n",
    "sys.path.append(os.path.join(mainCodeDirectory,\"2_Variable_Calculation\"))\n",
    "from CLASSES_Variable_Calculation import ModelData_Class, SlurmJobArray_Class, DataManager_Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2750bd32-beb7-4b9e-8a99-6c33e40004d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CM1 Data Summary ===\n",
      " Simulation #:   1\n",
      " Resolution:     1km\n",
      " Time step:      5min\n",
      " Vertical levels:34\n",
      " Parcels:        1e6\n",
      " Data file:      /mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/Model/cm1r20.3/run/cm1out_1km_5min_34nz.nc\n",
      " Parcel file:    /mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/Model/cm1r20.3/run/cm1out_pdata_1km_5min_1e6np.nc\n",
      " Time steps:     133\n",
      "========================= \n",
      "\n",
      "=== DataManager Summary ===\n",
      " inputDirectory #:   /mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/Code/OUTPUT/Variable_Calculation/TimeSplitModelData\n",
      " outputDirectory #:   /mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/Code/OUTPUT/Project_Algorithms/Tracking_Algorithms\n",
      " inputDataDirectory #:   /mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/Code/OUTPUT/Variable_Calculation/TimeSplitModelData/1km_5min_34nz/ModelData\n",
      " inputParcelDirectory #:   /mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/Code/OUTPUT/Variable_Calculation/TimeSplitModelData/1km_5min_34nz/ParcelData\n",
      " outputDataDirectory #:   /mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/Code/OUTPUT/Project_Algorithms/Tracking_Algorithms/1km_5min_34nz/Lagrangian_UpdraftTracking\n",
      "========================= \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#data loading class\n",
    "ModelData = ModelData_Class(mainDirectory, scratchDirectory, simulationNumber=1)\n",
    "#data manager class\n",
    "DataManager = DataManager_Class(mainDirectory, scratchDirectory, ModelData.res, ModelData.t_res, ModelData.Nz_str,\n",
    "                                ModelData.Np_str, dataType=\"Tracking_Algorithms\", dataName=\"Lagrangian_UpdraftTracking\",\n",
    "                                dtype='float32',codeSection = \"Project_Algorithms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "de514d0b-021a-40ac-978d-c2c711c8f773",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT FUNCTIONS\n",
    "sys.path.append(os.path.join(mainCodeDirectory,\"2_Variable_Calculation\"))\n",
    "import FUNCTIONS_Variable_Calculation\n",
    "from FUNCTIONS_Variable_Calculation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "fef86d16-e02e-435d-8878-83c167aeef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "#LOADING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3823a742-d92b-4bd8-920a-e61d00c33597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened dataset: /mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/Model/cm1r20.3/run/cm1out_pdata_1km_5min_1e6np.nc\n"
     ]
    }
   ],
   "source": [
    "parcel1 = ModelData.OpenParcel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ceca7434-3c4d-4e23-9556-e680ccb23524",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "#MODEL AND ALGORITHM NUMERICAL PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4430f783-c58e-4585-ad82-e8a9bd3b018b",
   "metadata": {},
   "outputs": [],
   "source": [
    "times=ModelData.time/(1e9 * 60); times=times.astype(float);\n",
    "minutes=1/times[1] #1 / minutes per timestep = timesteps per minute\n",
    "kms=np.argmax(ModelData.xh-ModelData.xh[0] >= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "64126a32-08a8-49c9-9f3d-ef208371d458",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "#PRINTING FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7335eb9f-8fcf-4e3d-a2f0-aa00e795e64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Global variable to store original stdout\n",
    "_original_stdout = sys.stdout\n",
    "\n",
    "def BlockPrint():\n",
    "    \"\"\"Suppress all print() output.\"\"\"\n",
    "    global _original_stdout\n",
    "    sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "def RestorePrint():\n",
    "    \"\"\"Restore print() output.\"\"\"\n",
    "    global _original_stdout\n",
    "    sys.stdout.close()\n",
    "    sys.stdout = _original_stdout\n",
    "\n",
    "# NO_PRINT=False\n",
    "NO_PRINT=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8cbd538d-a080-402a-a043-9fb34e95a02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "#JOBARRAY SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "f356472e-c20b-4b75-b726-d7d95374737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "#JOB ARRAY SETUP\n",
    "################################\n",
    "# how many total jobs are being run? i.e. array=1-100 ==> num_jobs=100\n",
    "if '1e6' in ModelData.Np_str:\n",
    "    num_jobs=60 #1M parcels\n",
    "    num_slurm_jobs=10\n",
    "if '50e6' in ModelData.Np_str:\n",
    "    num_jobs=200 #50M parcels\n",
    "    num_slurm_jobs=60\n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "6f12749d-e1e4-4d3e-8a09-07bb82a372c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS FUNCTION IS FOR RUNNING WITH SLURM JOB ARRAY\n",
    "#(SPLITS UP JOB_ARRAY BELOW INTO EVEN MORE TASKS)\n",
    "def StartSlurmJobArray(num_jobs,num_slurm_jobs, ISRUN):\n",
    "    job_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0)) #this is the current SBATCH job id\n",
    "    if job_id==0: job_id=8\n",
    "    if ISRUN==False:\n",
    "        start_job=1;end_job=num_jobs+1\n",
    "        return start_job,end_job\n",
    "    total_elements=num_jobs #total num of variables\n",
    "\n",
    "    job_range = total_elements // num_slurm_jobs  # Base size for each chunk\n",
    "    remaining = total_elements % num_slurm_jobs   # Number of chunks with 1 extra \n",
    "    \n",
    "    # Function to compute the start and end for each job_id\n",
    "    def get_job_range(job_id, num_slurm_jobs):\n",
    "        job_id-=1\n",
    "        # Add one extra element to the first 'remaining' chunks\n",
    "        start_job = job_id * job_range + min(job_id, remaining)\n",
    "        end_job = start_job + job_range + (1 if job_id < remaining else 0)\n",
    "    \n",
    "        if job_id == num_slurm_jobs - 1: \n",
    "            end_job = total_elements \n",
    "        return start_job, end_job\n",
    "    # def job_testing():\n",
    "    #     #TESTING\n",
    "    #     start=[];end=[]\n",
    "    #     for job_id in range(1,num_slurm_jobs+1):\n",
    "    #         start_job, end_job = get_job_range(job_id)\n",
    "    #         print(start_job,end_job)\n",
    "    #         start.append(start_job)\n",
    "    #         end.append(end_job)\n",
    "    #     print(np.all(start!=end))\n",
    "    #     print(len(np.unique(start))==len(start))\n",
    "    #     print(len(np.unique(end))==len(end))\n",
    "    # job_testing()\n",
    "    # if sbatch==True:\n",
    "        \n",
    "    start_job, end_job = get_job_range(job_id, num_slurm_jobs)\n",
    "    index_adjust=start_job\n",
    "    # print(f'start_job = {start_job}, end_job = {end_job}')\n",
    "    if start_job==0: start_job=1\n",
    "    if end_job==total_elements: end_job+=1\n",
    "    return start_job,end_job\n",
    "# job_id=1\n",
    "# [start_slurm_job,end_slurm_job,slurm_index_adjust]=StartSlurmJobArray(num_jobs,num_slurm_jobs,ISRUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "72dd8629-f5c6-42d3-9eae-e99629939627",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JOB ARRAY SETUP\n",
    "def StartJobArray(job_id,num_jobs):\n",
    "    total_elements=ModelData.Np #total num of variables\n",
    "\n",
    "    if num_jobs >= total_elements:\n",
    "        raise ValueError(\"Number of jobs cannot be greater than or equal to total elements.\")\n",
    "    \n",
    "    job_range = total_elements // num_jobs  # Base size for each chunk\n",
    "    remaining = total_elements % num_jobs   # Number of chunks with 1 extra \n",
    "    \n",
    "    # Function to compute the start and end for each job_id\n",
    "    def get_job_range(job_id, num_jobs):\n",
    "        job_id-=1\n",
    "        # Add one extra element to the first 'remaining' chunks\n",
    "        start_job = job_id * job_range + min(job_id, remaining)\n",
    "        end_job = start_job + job_range + (1 if job_id < remaining else 0)\n",
    "    \n",
    "        if job_id == num_jobs - 1: \n",
    "            end_job = total_elements #- 1\n",
    "        return start_job, end_job\n",
    "    # def job_testing():\n",
    "    #     #TESTING\n",
    "    #     start=[];end=[]\n",
    "    #     for job_id in range(1,num_jobs+1):\n",
    "    #         start_job, end_job = get_job_range(job_id)\n",
    "    #         print(start_job,end_job)\n",
    "    #         start.append(start_job)\n",
    "    #         end.append(end_job)\n",
    "    #     print(np.all(start!=end))\n",
    "    #     print(len(np.unique(start))==len(start))\n",
    "    #     print(len(np.unique(end))==len(end))\n",
    "    # job_testing()\n",
    "\n",
    "    # if sbatch==True:\n",
    "    #     job_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0)) #this is the current SBATCH job id\n",
    "    #     if job_id==0: job_id=1\n",
    "        \n",
    "    start_job, end_job = get_job_range(job_id, num_jobs)\n",
    "    index_adjust=start_job\n",
    "    # print(f'start_job = {start_job}, end_job = {end_job}')\n",
    "    return start_job,end_job,index_adjust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2e0ad1ff-734d-4814-86ef-8c6d73e52821",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "#SOME DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d00a5ac8-fbca-41b5-a72e-db3e539baff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = f\"/mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/Code/OUTPUT/Variable_Calculation/LagrangianArrays/{ModelData.res}_{ModelData.t_res}_{ModelData.Nz_str}nz/Lagrangian_Binary_Array/\"\n",
    "Lagrangian_Binary_Array_Data,files = OpenMultipleSingleTimes_LagrangianArray(directory, ModelData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a466d50f-37c7-45bf-84c5-f1a1009789c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = f\"/mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/Code/OUTPUT/Variable_Calculation/LagrangianArrays/{ModelData.res}_{ModelData.t_res}_{ModelData.Nz_str}nz/LFC/\"\n",
    "\n",
    "LFC_LCL_Data,files = OpenMultipleSingleTimes_LagrangianArray(directory, ModelData,pattern=\"LFC_*.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "bc6ee962-2536-485e-81ba-4fb5093478af",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "#DATA LOADING FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "1ff859fa-09d1-410f-8c3a-b5f8a840e45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SUBSETTING PARCEL DATA\n",
    "def GetData(parcel1,start_job,end_job):\n",
    "    parcel=parcel1.isel(xh=slice(start_job,end_job))\n",
    "    return parcel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "0a0b814b-8426-4310-a5e1-465fdb1aecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetSpatialData(Lagrangian_Binary_Array_Data, start_job,end_job):\n",
    "    parcel_z = Lagrangian_Binary_Array_Data['z'].isel(p=slice(start_job,end_job)).data.compute()\n",
    "    parcel_x = Lagrangian_Binary_Array_Data['x'].isel(p=slice(start_job,end_job)).data.compute()\n",
    "\n",
    "    parcel_w = Lagrangian_Binary_Array_Data['w'].isel(p=slice(start_job,end_job)).data.compute()\n",
    "    parcel_u = Lagrangian_Binary_Array_Data['u'].isel(p=slice(start_job,end_job)).data.compute()\n",
    "    \n",
    "    # Load the dataset by its name\n",
    "    Z = Lagrangian_Binary_Array_Data['Z'].isel(p=slice(start_job,end_job)).data.compute()\n",
    "    Y = Lagrangian_Binary_Array_Data['Y'].isel(p=slice(start_job,end_job)).data.compute()\n",
    "    X = Lagrangian_Binary_Array_Data['X'].isel(p=slice(start_job,end_job)).data.compute()\n",
    "    W = Lagrangian_Binary_Array_Data['W'].isel(p=slice(start_job,end_job)).data.compute()\n",
    "\n",
    "    return parcel_z,parcel_x,parcel_u,parcel_w,Z,Y,X,W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e428e62d-8d62-4411-b2a4-f6e84aebec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetLFCData(LFC_LCL_Data, start_job,end_job):\n",
    "    LFC = LFC_LCL_Data['LFC'].isel(p=slice(start_job,end_job)).data.compute()\n",
    "    LCL = LFC_LCL_Data['LCL'].isel(p=slice(start_job,end_job)).data.compute()\n",
    "    return LFC,LCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "d870bad5-ac84-40d8-85a4-293750f205fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADING CL MAXS FROM CL TRACKING ALGORITHM\n",
    "def Get_Conv_X(t,z,y):\n",
    "\n",
    "    timeString = ModelData.timeStrings[t]\n",
    "    outputDataDirectory=os.path.normpath(os.path.join(DataManager.outputDataDirectory,\"..\",\"Eulerian_CLTracking\"))\n",
    "    Dictionary = TrackingAlgorithms_DataLoading_Class.LoadData(ModelData, DataManager, timeString,\n",
    "                     dataName=\"Eulerian_CLTracking\",outputDataDirectory=outputDataDirectory)\n",
    "    whereCL = Dictionary[\"maxConvergence_X\"]\n",
    "    Conv_X_Max=whereCL[z,y]\n",
    "    return Conv_X_Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "0dc9aece-9953-47b6-a4a5-0e03463efcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "#ALGORITHM FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "68e02e69-4333-411f-8340-6781385925b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updated Lagrangian Tracking Algorithm\n",
    "\n",
    "#Algorithm Steps:\n",
    "#(1) Find the first time a parcel is above the LFC:\n",
    "#(2) First check if the parcel ascends (w>=0.1) for another 20 minutes\n",
    "#(3) If so, find first time, the parcel slows down (w<0.1)\n",
    "#(4) If that time is when the parcel is above 750m, save it, \"forget\", and move on to next parcel\n",
    "#(5) If that time is when the parcel is below 750m, check if it is within 2km of the CL_Max found from the CL Tracking Algorithm\n",
    "#(6) If the parcel is near the CL, store in, otherwise save it, \"forget\", and move on to next parcel\n",
    "#(7) Continue to next parcel\n",
    "\n",
    "#(Also, if during, traceback, the parcel escapes the x or z boundary, \"forget\" parcel, and move on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8e645e76-ec24-4464-bc25-f4d9632cf0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numerical Settings\n",
    "Nt=ModelData.Ntime\n",
    "dt=times[1]*60\n",
    "#Height parcel must be below to be counted\n",
    "CLmaxheight=750 #750m\n",
    "#BL slow-down-threshold\n",
    "w_thresh=0.1\n",
    "def GetNp(parcel):\n",
    "    Np=len(parcel['xh'])\n",
    "    return Np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "68f5e82e-6419-49c5-901e-dc7de91030da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if ((x + dt*u)==0) or ((z + dt*w)==0)\n",
    "# u=u[t,Z[t,p],Y[t,p],X[t,p]]; W=W[t,p]\n",
    "# [u[t,Z[t,p],Y[t,p],X[t,p]] for t in time_arr] >np.max(data['xf'].values) or < np.min(data['xf'].values)\n",
    "# similarly for w\n",
    "################################################################################################################\n",
    "#BOUNDARY-ESCAPE CONDITION\n",
    "xmin=np.min(ModelData.xf)*1e3\n",
    "xmax=np.max(ModelData.xf)*1e3\n",
    "zmin=np.min(ModelData.zf)*1e3\n",
    "zmax=np.max(ModelData.zf)*1e3\n",
    "\n",
    "def check_boundary(p,where_BL,above_LFC):\n",
    "    time_arr=np.arange(where_BL,above_LFC)\n",
    "\n",
    "    def get_x(t,p):\n",
    "        # return parcel['x'][t,p].item()\n",
    "        return parcel_x[t,p] \n",
    "    def get_u(t,p):\n",
    "        # return data['uinterp'].isel(time=t,zh=Z[t,p],yh=Y[t,p],xh=X[t,p]).item() #TESTING\n",
    "        # return parcel['u'][t,p].item() \n",
    "        return parcel_u[t,p]\n",
    "    def get_z(t,p):\n",
    "        # return parcel['z'][t,p].item()\n",
    "        return parcel_z[t,p]\n",
    "    def get_w(t,p):\n",
    "        # return data['winterp'].isel(time=t,zh=Z[t,p],yh=Y[t,p],xh=X[t,p]).item()\n",
    "        # return parcel['w'][t,p].item()\n",
    "        return parcel_w[t,p]\n",
    "        \n",
    "\n",
    "    # x_tend = [get_x(t, p) + dt * get_u(t, z, y, x)   #THIS IS OLD, LESS IDEAL\n",
    "    #       for (t, z, y, x) in zip(time_arr, Z[time_arr, p], Y[time_arr, p], X[time_arr, p])] \n",
    "    # z_tend = [get_z(t, p) + dt * get_w(t, z, y, x)  \n",
    "    #       for (t, z, y, x) in zip(time_arr, Z[time_arr, p], Y[time_arr, p], X[time_arr, p])] \n",
    "    \n",
    "    x_tend = [get_x(t, p) + dt * get_u(t,p)   \n",
    "          for (t, z, y, x) in zip(time_arr, Z[time_arr, p], Y[time_arr, p], X[time_arr, p])] \n",
    "    z_tend = [get_z(t, p) + dt * get_w(t,p)  \n",
    "          for (t, z, y, x) in zip(time_arr, Z[time_arr, p], Y[time_arr, p], X[time_arr, p])] \n",
    "\n",
    "    x_bound=any(val < xmin or val > xmax for val in x_tend)*1\n",
    "    z_bound=any(val < zmin or val > zmax for val in z_tend)*1\n",
    "\n",
    "    out=(x_bound,z_bound)\n",
    "    if out[0]==1:\n",
    "        print(f'parcel {p} crossed x-boundary between t={where_BL} and t={above_LFC}')\n",
    "    elif out[1]==1:\n",
    "        print(f'parcel {p} crossed z-boundary between t={where_BL} and t={above_LFC}')\n",
    "    return out\n",
    "#############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "387b950e-bac3-4933-9a40-095ca570d4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Output Storage Vector\n",
    "def InitializeData(Np):\n",
    "    out_arr=np.zeros((Np,3),dtype=np.int32) \n",
    "    save_arr=np.zeros((Np,3),dtype=np.int32) #This one is for saving continued-ascent, slow-below-750m parcels that are not with 2 km of CL\n",
    "    save2_arr=np.zeros((Np,3),dtype=np.int32) #This one is for saving continued-ascent, slow-above-750m parcels\n",
    "    return out_arr,save_arr,save2_arr\n",
    "# [out_arr,save_arr,save2_arr]=InitializeData(Np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "1e0eed3f-6b8c-4230-9abc-37e560986dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################################\n",
    "#The Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "5b1f6fe0-558b-4087-8ba1-f6ca642d5b4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ParcelTracking(Np,W,LFC,parcel_z,Z,Y,X,out_arr,save_arr,save2_arr):\n",
    "    #1--------------Looping over each parcel\n",
    "    for count,p in enumerate(np.arange(Np)): \n",
    "        if np.mod(p,5e4)==0: print(f'current parcel: {p}/{Np}')\n",
    "        \n",
    "        W_p = W[:,p]\n",
    "        LFC_p = LFC[:,p] \n",
    "       \n",
    "        #----FIND WHERE PARCEL IS ABOVE LFC----\n",
    "        indices = np.where(LFC_p == 1)[0]; above_LFC = indices[0] if indices.size > 0 else -999; #FIRST TIME ABOVE LFC\n",
    "        if above_LFC ==-999:\n",
    "            # print(f'parcel {p} never above LFC')\n",
    "            continue #if the parcel is never above the LFC, skip the parcel\n",
    "        \n",
    "        #----CHECK IF ASCENDS FOR >= 20 minutes AFTER LFC----\n",
    "        ascend_array=W_p[above_LFC+1:]\n",
    "        indices=np.where(ascend_array==0)[0]; ascend_stop=indices[0] if indices.size > 0 else 10000; #location of where parcel stops ascending (labeled 10000 to mark for future analysis)\n",
    "        # ascend_lst.append(ascend_stop) #(also store for histogram)\n",
    "        if ascend_stop>=20*minutes:\n",
    "        \n",
    "            #----FIND THE FIRST TIME W_p<=w_thresh----\n",
    "            indices=np.where(W_p[0:above_LFC]<w_thresh)[0]\n",
    "            where_BL=indices[-1] if indices.size > 0 else -999 #FIRST PRIOR TIME W<0.1 (IN THE BL) (ADDED 1 TO GET TIME RIGHT AFTER INTERACTION)\n",
    "            if where_BL ==-999:\n",
    "                # print(f'parcel {p} w is never below threshold prior to t={above_LFC}')\n",
    "                continue #if the parcel never slows down backwards in time (unlikely), skip the parcel\n",
    "                \n",
    "            #check for boundary escapes\n",
    "            ################################\n",
    "            future_location=check_boundary(p,where_BL,above_LFC)\n",
    "            if (future_location[0]+future_location[1]>=1): continue #if parcel crosses boundary, skips current parcel\n",
    "            ################################\n",
    "            \n",
    "            #----CHECK IF PARCEL SLOWED DOWN LOW ENOUGH----\n",
    "            if parcel_z[where_BL,p]<=CLmaxheight: #PARCEL MUST BE BELOW 750m WHEN CONTACTING CL #***\n",
    "            # if LCL[where_BL,p]==0: #PARCEL MUST BE BELOW LCL WHEN CONTACTING CL (not recommended)\n",
    "        \n",
    "                #----CHECK IF CL IS WITHIN 2km----\n",
    "                #Find the CL-max x-location\n",
    "                t=where_BL; z=Z[where_BL,p]; y=Y[where_BL,p]; x=X[where_BL,p]\n",
    "                CONV_X=Get_Conv_X(t,z,y)\n",
    "                within_CL=np.any(np.isin(CONV_X, np.arange(x-2*kms,x+3*kms)))\n",
    "                \n",
    "                if within_CL==True:\n",
    "                    #save X's (t,p) \n",
    "                    print(f'Parcel {p} is success at time {where_BL}')\n",
    "                    out_arr[p,0]=p\n",
    "                    out_arr[p,1]=where_BL\n",
    "                    out_arr[p,2]=above_LFC \n",
    "                else: #continued-ascent, slow-below-750m parcels that are not with 2 km of CL\n",
    "                    #SAVE PARCEL\n",
    "                    # print(f'Parcel {p} not near CL at t={where_BL}')\n",
    "                    save_arr[p,0]=p\n",
    "                    save_arr[p,1]=where_BL\n",
    "                    save_arr[p,2]=above_LFC \n",
    "        \n",
    "            else: #continued-ascent, slow-above-750m parcels\n",
    "                #SAVE PARCEL\n",
    "                # print(f'Parcel {p} above {CLmaxheight}m at t={where_BL}')\n",
    "                save2_arr[p,0]=p\n",
    "                save2_arr[p,1]=where_BL\n",
    "                save2_arr[p,2]=above_LFC         \n",
    "                \n",
    "            #END OF LOOP, THEN WE MOVE ON TO NEXT PARCEL p\n",
    "    return out_arr,save_arr,save2_arr\n",
    "# [out_arr,save_arr,save2_arr]=ParcelTracking(Np,W,LFC,parcel_z,Z,Y,X,out_arr,save_arr,save2_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "4eade392-bb34-4c0f-a93a-1cc1da5fea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CorrectParcelID(out_arr,save_arr,save2_arr,index_adjust):\n",
    "    #CORRECTING DATA PARCEL ID BASED ON JOB NUMBER\n",
    "    #####################################################\n",
    "    out_arr[np.where(np.any(out_arr != 0, axis=1))[0],0]+=index_adjust #*needed for job array*+=index_adjust #*needed for job array*\n",
    "    save_arr[np.where(np.any(save_arr != 0, axis=1))[0],0]+=index_adjust #*needed for job array*+=index_adjust #*needed for job array*\n",
    "    save2_arr[np.where(np.any(save2_arr != 0, axis=1))[0],0]+=index_adjust #*needed for job array*+=index_adjust #*needed for job array*\n",
    "    return out_arr,save_arr,save2_arr\n",
    "# [out_arr,save_arr,save2_arr]=CorrectParcelID(out_arr,save_arr,save2_arr,index_adjust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "32b12a89-f869-4b35-81b5-5f1b56cacbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVING BLANK ROWS\n",
    "def RemoveZeroRows(arr):\n",
    "    arr = arr[~np.all(arr == 0, axis=1)]\n",
    "    return arr\n",
    "# out_arr=RemoveZeroRows(out_arr);save_arr=RemoveZeroRows(save_arr);save2_arr=RemoveZeroRows(save2_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "2ee74bae-0bc5-4e8c-834a-f6c10c5ef643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveOutFile(ModelData,DataManager, Dictionary,job_id): \n",
    "    \"\"\"\n",
    "    Save tracking algorithm results to an HDF5 file.\n",
    "    \"\"\"\n",
    "    \n",
    "    fileName = f\"{DataManager.dataName}_{ModelData.res}_{ModelData.t_res}_{ModelData.Nz_str}nz_job{job_id}.h5\"\n",
    "    filePath = os.path.join(DataManager.outputDataDirectory,fileName)\n",
    "    \n",
    "\n",
    "    with h5py.File(filePath, 'w') as f:\n",
    "        for varName, varData in Dictionary.items():\n",
    "            f.create_dataset(f\"{varName}\", data=varData, compression=\"gzip\")\n",
    "\n",
    "    print(f\"Saved output to {filePath}\",\"\\n\")\n",
    "\n",
    "def LoadOutFile(ModelData, DataManager, job_id, varName=None, printstatement=False): \n",
    "    \"\"\"\n",
    "    Load tracking algorithm results from an HDF5 file and return as a dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    fileName = f\"{DataManager.dataName}_{ModelData.res}_{ModelData.t_res}_{ModelData.Nz_str}nz_job{job_id}.h5\"\n",
    "    filePath = os.path.join(DataManager.outputDataDirectory, fileName)\n",
    "\n",
    "    if printstatement==True:\n",
    "        print(f\"Loading output from {filePath}\\n\")\n",
    "\n",
    "    Dictionary = {}\n",
    "    with h5py.File(filePath, 'r') as f:\n",
    "        if varName is None:\n",
    "            # Load all variables\n",
    "            Dictionary = {name: f[name][:] for name in f.keys()}\n",
    "            return Dictionary\n",
    "        else:\n",
    "            if varName not in f:\n",
    "                raise KeyError(f\"{varName} not found in {filePath}\")\n",
    "            arr = f[varName][:]\n",
    "            return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "2b9dd1af-32c7-4389-a1c6-ec1930730bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "#RUNNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "a686e2cd-14ec-4a0c-83dc-844a9ec320eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunAlgorithm(job_id_list):\n",
    "    for job_id in job_id_list:\n",
    "        if job_id % 1 == 0: print(f'current job_id = {job_id}')\n",
    "        [start_job,end_job,index_adjust]=StartJobArray(job_id,num_jobs)\n",
    "        \n",
    "        #SLICING DATA\n",
    "        print(\"SLICING DATA\")\n",
    "        parcel=GetData(parcel1,start_job,end_job)\n",
    "    \n",
    "        #GETTING REQUIRED DATA\n",
    "        print(\"GETTING REQUIRED DATA\")\n",
    "        [parcel_z,parcel_x,parcel_u,parcel_w,Z,Y,X,W] = GetSpatialData(Lagrangian_Binary_Array_Data, start_job,end_job)\n",
    "        LFC,LCL = GetLFCData(LFC_LCL_Data, start_job,end_job)\n",
    "    \n",
    "        #INITIALIZING DATA\n",
    "        print(\"INITIALIZING DATA\")\n",
    "        Np=GetNp(parcel)\n",
    "        [out_arr,save_arr,save2_arr]=InitializeData(Np)\n",
    "    \n",
    "        #RUNNING ALGORITHM\n",
    "        print(\"RUNNING ALGORITHM\")\n",
    "        start_time = time.time()\n",
    "        if NO_PRINT==True: BlockPrint()\n",
    "        [out_arr,save_arr,save2_arr]=ParcelTracking(Np,W,LFC,parcel_z,Z,Y,X,out_arr,save_arr,save2_arr)\n",
    "        if NO_PRINT==True: RestorePrint()\n",
    "        end_time = time.time(); elapsed_time = end_time - start_time; print(f\"Elapsed Time: {elapsed_time} seconds\")  \n",
    "    \n",
    "        #CORRECTING PARCEL ID FOR JOBARRAY\n",
    "        [out_arr,save_arr,save2_arr]=CorrectParcelID(out_arr,save_arr,save2_arr,index_adjust)\n",
    "    \n",
    "        #REMOVING BLANK ROWS FROMRESULTS\n",
    "        out_arr=RemoveZeroRows(out_arr);save_arr=RemoveZeroRows(save_arr);save2_arr=RemoveZeroRows(save2_arr)\n",
    "    \n",
    "        #SAVING\n",
    "        print(\"SAVING\")\n",
    "        Dictionary = {\"out_arr\": out_arr,\n",
    "                      \"save_arr\": save_arr,\n",
    "                      \"save2_arr\": save2_arr}\n",
    "        \n",
    "        SaveOutFile(ModelData,DataManager, Dictionary,job_id)\n",
    "    return Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "2677b5c1-8300-40b4-961f-e8acfb0f98eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on Slurm_Jobs for Slurm_Job_Ids: (42, 47)\n",
      "current job_id = 42\n",
      "SLICING DATA\n",
      "GETTING REQUIRED DATA\n",
      "INITIALIZING DATA\n",
      "RUNNING ALGORITHM\n",
      "Elapsed Time: 0.07561421394348145 seconds\n",
      "SAVING\n",
      "Saved output to /mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/Code/OUTPUT/Project_Algorithms/Tracking_Algorithms/1km_5min_34nz/Lagrangian_UpdraftTracking/Lagrangian_UpdraftTracking_1km_5min_34nz_job42.h5 \n",
      "\n",
      "Total Elapsed Time: 11.536628246307373 seconds\n"
     ]
    }
   ],
   "source": [
    "#starting job arrays\n",
    "[start_slurm_job,end_slurm_job]=StartSlurmJobArray(num_jobs=num_jobs,num_slurm_jobs=num_slurm_jobs,ISRUN=True) #if ISRUN is False, then will not run using slurm_job_array\n",
    "print(f\"Running on Slurm_Jobs for Slurm_Job_Ids: {(start_slurm_job,end_slurm_job-1)}\")\n",
    "StartTime = time.time()\n",
    "job_id_list=np.arange(start_slurm_job,end_slurm_job)\n",
    "\n",
    "#running algorithm\n",
    "Dictionary = RunAlgorithm(job_id_list)\n",
    "\n",
    "EndTime = time.time(); ElapsedTime = EndTime - StartTime; print(f\"Total Elapsed Time: {ElapsedTime} seconds\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bf7a78cc-b1ba-4495-a482-dc791048a1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "#Run after finishing job_array\n",
    "recombine=False #KEEP FALSE WHEN JOB_ARRAY IS RUNNING\n",
    "recombine=True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a834896f-b2e3-4859-8bf5-8f00e42178b7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def MakeUnique(arr):\n",
    "    return np.unique(arr, axis=0)\n",
    "\n",
    "def get_total_count(ModelData, DataManager, var_name, num_jobs):\n",
    "    \"\"\"\n",
    "    Sum the total length (axis 0) of `var_name` across all job output files\n",
    "    using LoadOutFile().\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    for job_id in range(1, num_jobs + 1):\n",
    "        try:\n",
    "            data_dict = LoadOutFile(ModelData, DataManager, job_id)\n",
    "            if var_name in data_dict:\n",
    "                total += data_dict[var_name].shape[0]\n",
    "            else:\n",
    "                print(f\"Warning: {var_name} not found in job {job_id}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Missing file for job {job_id}\")\n",
    "            continue\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1e0c83bc-0063-4eae-80af-317f43baf3a5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Recombine(num_jobs): #*#*\n",
    "    var_names = ['out_arr', 'save_arr', 'save2_arr']\n",
    "    recombined_arrays = {}  # Store final arrays here\n",
    "\n",
    "\n",
    "    # Preallocate arrays\n",
    "    for var_name in var_names:\n",
    "        total_count = get_total_count(ModelData, DataManager, var_name, num_jobs)\n",
    "        recombined_arrays[var_name] = np.zeros((total_count, 3), dtype=np.int32)\n",
    "\n",
    "    # Fill arrays\n",
    "    for var_name in var_names:\n",
    "        print(f\"Combining data for {var_name}\")\n",
    "        left_ind = 0\n",
    "        for job_id in range(1, num_jobs + 1):\n",
    "            # if job_id % 10 == 0: print(f\"{var_name}: processing job {job_id}\")\n",
    "            arr = LoadOutFile(ModelData, DataManager, job_id, varName=var_name)\n",
    "            n_rows = arr.shape[0]; right_ind = left_ind + n_rows\n",
    "            recombined_arrays[var_name][left_ind:right_ind, :] = arr\n",
    "            left_ind = right_ind\n",
    "\n",
    "    #Make Unique\n",
    "    for var_name in var_names:\n",
    "        recombined_arrays[var_name]=MakeUnique(recombined_arrays[var_name])\n",
    "        \n",
    "    # Write to file\n",
    "    SaveOutFile(ModelData,DataManager, recombined_arrays,job_id=\"combined\")\n",
    "\n",
    "    return recombined_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0f1b74d1-ba85-4b12-9e14-3100ba52fe1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining data for out_arr\n",
      "Combining data for save_arr\n",
      "Combining data for save2_arr\n",
      "Saved output to /mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/Code/OUTPUT/Project_Algorithms/Tracking_Algorithms/1km_5min_34nz/Lagrangian_UpdraftTracking/Lagrangian_UpdraftTracking_1km_5min_34nz_jobcombined.h5 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if recombine==True:\n",
    "    recombined_arrays = Recombine(num_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065dee06-3321-4b77-b329-71ce5d137525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a49b86f-d738-406d-8ba4-a325f2d69898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d599022-7a7c-4b97-9168-94781a663336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a25eeb2-6b94-436c-bff5-142551453075",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e6ea522e-734b-407b-9cfa-081d43743c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'out_arr': array([], shape=(0, 3), dtype=int32),\n",
       " 'save2_arr': array([], shape=(0, 3), dtype=int32),\n",
       " 'save_arr': array([], shape=(0, 3), dtype=int32)}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LOADING BACK IN\n",
    "Dictionary = LoadOutFile(ModelData, DataManager, job_id=\"combined\")\n",
    "Dictionary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "work"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
