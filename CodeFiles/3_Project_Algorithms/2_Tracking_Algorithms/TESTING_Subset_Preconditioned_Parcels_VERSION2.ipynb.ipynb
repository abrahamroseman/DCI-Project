{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a45ec61-2f30-456b-aaa6-6ecb9dee0299",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS FUNCTION IS FOR RUNNING WITH SLURM JOB ARRAY\n",
    "#(SPLITS UP JOB_ARRAY BELOW INTO EVEN MORE TASKS)\n",
    "def StartSlurmJobArray(num_jobs,num_slurm_jobs, ISRUN):\n",
    "    job_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0)) #this is the current SBATCH job id\n",
    "    if job_id==0: job_id=2\n",
    "    if ISRUN==False:\n",
    "        start_job=1;end_job=num_jobs+1\n",
    "        return start_job,end_job\n",
    "    total_elements=num_jobs #total num of variables\n",
    "\n",
    "    job_range = total_elements // num_slurm_jobs  # Base size for each chunk\n",
    "    remaining = total_elements % num_slurm_jobs   # Number of chunks with 1 extra \n",
    "    \n",
    "    # Function to compute the start and end for each job_id\n",
    "    def get_job_range(job_id, num_slurm_jobs):\n",
    "        job_id-=1\n",
    "        # Add one extra element to the first 'remaining' chunks\n",
    "        start_job = job_id * job_range + min(job_id, remaining)\n",
    "        end_job = start_job + job_range + (1 if job_id < remaining else 0)\n",
    "    \n",
    "        if job_id == num_slurm_jobs - 1: \n",
    "            end_job = total_elements \n",
    "        return start_job, end_job\n",
    "    # def job_testing():\n",
    "    #     #TESTING\n",
    "    #     start=[];end=[]\n",
    "    #     for job_id in range(1,num_slurm_jobs+1):\n",
    "    #         start_job, end_job = get_job_range(job_id)\n",
    "    #         print(start_job,end_job)\n",
    "    #         start.append(start_job)\n",
    "    #         end.append(end_job)\n",
    "    #     print(np.all(start!=end))\n",
    "    #     print(len(np.unique(start))==len(start))\n",
    "    #     print(len(np.unique(end))==len(end))\n",
    "    # job_testing()\n",
    "    # if sbatch==True:\n",
    "        \n",
    "    start_job, end_job = get_job_range(job_id, num_slurm_jobs)\n",
    "    index_adjust=start_job\n",
    "    # print(f'start_job = {start_job}, end_job = {end_job}')\n",
    "    if start_job==0: start_job=1\n",
    "    if end_job==total_elements: end_job+=1\n",
    "    return start_job,end_job\n",
    "\n",
    "# job_id=1\n",
    "# [start_slurm_job,end_slurm_job,slurm_index_adjust]=StartSlurmJobArray(num_jobs,num_slurm_jobs,ISRUN)\n",
    "# parcel=parcel1.isel(xh=slice(start_job,end_job))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea4665a-f810-40bb-9ab6-7797ca412b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.lines as mlines\n",
    "import xarray as xr\n",
    "import os; import time\n",
    "import pickle\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f80db3-4515-4edc-bce9-1443f7757243",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAIN DIRECTORIES\n",
    "mainDirectory='/mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/'\n",
    "scratchDirectory='/home/air673/koa_scratch/'\n",
    "codeDirectory='/mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/Project_Algorithms/Tracking_Algorithms'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe24ed9-df5a-4677-bef6-967e6f4ec1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADING DATA\n",
    "def GetDataDirectories(simulationNumber):\n",
    "    if simulationNumber == 1:\n",
    "        Directory=os.path.join(mainDirectory,'Model/cm1r20.3/run')\n",
    "        res='1km'; t_res='5min'; Np_str='1e6'; Nz_str='34'\n",
    "    elif simulationNumber == 2:\n",
    "        Directory=scratchDirectory\n",
    "        res='1km'; t_res='1min'; Np_str='50e6'; Nz_str='95'\n",
    "    elif simulationNumber == 3:\n",
    "        Directory=scratchDirectory\n",
    "        res='250m'; t_res='1min'; Np_str='50e6'; Nz_str='95'\n",
    "        \n",
    "    dataDirectory = os.path.join(Directory, f\"cm1out_{res}_{t_res}_{Nz_str}nz.nc\")\n",
    "    parcelDirectory = os.path.join(Directory,f\"cm1out_pdata_{res}_{t_res}_{Np_str}np.nc\")\n",
    "    return dataDirectory, parcelDirectory, res,t_res,Np_str,Nz_str\n",
    "    \n",
    "def GetData(dataDirectory, parcelDirectory):\n",
    "    dataNC = xr.open_dataset(dataDirectory, decode_timedelta=True) \n",
    "    parcelNC = xr.open_dataset(parcelDirectory, decode_timedelta=True) \n",
    "    return dataNC,parcelNC\n",
    "\n",
    "def SubsetDataVars(dataNC):\n",
    "    varList = [\"thflux\", \"qvflux\", \"tsk\", \"cape\", \n",
    "               \"cin\", \"lcl\", \"lfc\", \"th\",\n",
    "               \"prs\", \"rho\", \"qv\", \"qc\",\n",
    "               \"qr\", \"qi\", \"qs\",\"qg\", \n",
    "               \"buoyancy\", \"uinterp\", \"vinterp\", \"winterp\",]\n",
    "    \n",
    "    varList += [\"ptb_hadv\", \"ptb_vadv\", \"ptb_hidiff\", \"ptb_vidiff\",\n",
    "                \"ptb_hturb\", \"ptb_vturb\", \"ptb_mp\", \"ptb_rdamp\", \n",
    "                \"ptb_rad\", \"ptb_div\", \"ptb_diss\",]\n",
    "    \n",
    "    varList += [\"qvb_hadv\", \"qvb_vadv\", \"qvb_hidiff\", \"qvb_vidiff\", \n",
    "                \"qvb_hturb\", \"qvb_vturb\", \"qvb_mp\",]\n",
    "    \n",
    "    varList += [\"wb_hadv\", \"wb_vadv\", \"wb_hidiff\", \"wb_vidiff\",\n",
    "                \"wb_hturb\", \"wb_vturb\", \"wb_pgrad\", \"wb_rdamp\", \"wb_buoy\",]\n",
    "\n",
    "    return dataNC[varList]\n",
    "\n",
    "[dataDirectory,parcelDirectory, res,t_res,Np_str,Nz_str] = GetDataDirectories(simulationNumber=1)\n",
    "[data1,parcel1] = GetData(dataDirectory, parcelDirectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bbdd60-20fe-43f7-a5f0-4603ca2f3c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "job_array=False;index_adjust=0\n",
    "ocean_fraction=2/8\n",
    "start_time = time.time();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1ddf61-3a6e-4722-820b-1ac60875bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a9d763-a807-4f5d-9014-9e939f42f1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "def print_memory_usage(note=\"\"):\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem = process.memory_info().rss / 1024 ** 2  # in MB\n",
    "    print(f\"[{note}] Memory usage: {mem:.2f} MB\")\n",
    "print_memory_usage(\"Current Memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2278b9ae-e490-422e-8614-cb0f6428a989",
   "metadata": {},
   "outputs": [],
   "source": [
    "times=data1['time'].values/(1e9 * 60); times=times.astype(float);\n",
    "minutes=1/times[1] #1 / minutes per timestep = timesteps per minute\n",
    "kms=np.argmax(data1['xh'].values-data1['xh'][0].values >= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a91fd70-216c-453d-ae84-dcc08a1e0106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "dir2='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "path=dir2+'../Functions/'\n",
    "sys.path.append(path)\n",
    "\n",
    "import NumericalFunctions\n",
    "from NumericalFunctions import * # import NumericalFunctions \n",
    "import PlottingFunctions\n",
    "from PlottingFunctions import * # import PlottingFunctions\n",
    "\n",
    "\n",
    "# # Get all functions in NumericalFunctions\n",
    "# import inspect\n",
    "# functions = [f[0] for f in inspect.getmembers(NumericalFunctions, inspect.isfunction)]\n",
    "# functions\n",
    "\n",
    "# # Get all functions in NumericalFunctions\n",
    "# import inspect\n",
    "# functions = [f[0] for f in inspect.getmembers(PlottingFunctions, inspect.isfunction)]\n",
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8971ba04-183f-4714-ad4f-f62bb03e3ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#READING BACK IN\n",
    "########################\n",
    "def LoadFinalData(in_file):\n",
    "    dict = {}\n",
    "    with h5py.File(in_file, 'r') as f:\n",
    "        for key in f.keys():\n",
    "            dict[key] = f[key][:]\n",
    "    return dict\n",
    "\n",
    "def LoadAllCloudBase():\n",
    "    dir2 = dir + f'Project_Algorithms/Tracking_Algorithms/OUTPUT/'\n",
    "    in_file = dir2 + f\"all_cloudbase_{res}_{t_res}_{Np_str}.pkl\"\n",
    "    with open(in_file, 'rb') as f:\n",
    "        all_cloudbase = pickle.load(f)\n",
    "    return(all_cloudbase)\n",
    "min_all_cloudbase=np.nanmin(LoadAllCloudBase())\n",
    "print(f\"Minimum Cloudbase is: {min_all_cloudbase*1000:.0f} m\")\n",
    "\n",
    "def LoadMeanLFC():\n",
    "    dir2 = dir + f'Project_Algorithms/Tracking_Algorithms/OUTPUT/'\n",
    "    in_file = dir2 + f\"MeanLFC_{res}_{t_res}_{Np_str}.pkl\"\n",
    "    with open(in_file, 'rb') as f:\n",
    "        MeanLFC = pickle.load(f)\n",
    "    return MeanLFC\n",
    "MeanLFC=LoadMeanLFC()\n",
    "print(f\"Mean LFC is: {MeanLFC:.0f} m\\n\")\n",
    "\n",
    "dir2 = dir + f'Project_Algorithms/Tracking_Algorithms/OUTPUT/'\n",
    "in_file=dir2+f\"parcel_tracking_SUBSET_{res}_{t_res}_{Np_str}.h5\"\n",
    "final_dict=LoadFinalData(in_file)\n",
    "\n",
    "\n",
    "#DYNAMICALLY CREATING VARIABLES\n",
    "for key, value in final_dict.items():\n",
    "    globals()[key] = value\n",
    "\n",
    "# #DYNAMICALLY PRINTING VARIABLE SIZES\n",
    "# for key in final_dict:\n",
    "#     print(f\"{key} has {final_dict[key].shape[0]} parcels\")\n",
    "\n",
    "# PRINTING VARIABLE SIZES (ONE BY ONE)\n",
    "print(f'ALL: {len(CL_ALL_out_arr)} CL parcels and {len(nonCL_ALL_out_arr)} nonCL parcels')\n",
    "print(f'SHALLOW: {len(CL_SHALLOW_out_arr)} CL parcels and {len(nonCL_SHALLOW_out_arr)} nonCL parcels')\n",
    "print(f'DEEP: {len(CL_DEEP_out_arr)} CL parcels and {len(nonCL_DEEP_out_arr)} nonCL parcels')\n",
    "print('\\n')\n",
    "print(f'ALL: {len(SBZ_ALL_out_arr)} SBZ parcels and {len(nonSBZ_ALL_out_arr)} nonSBZ parcels')\n",
    "print(f'SHALLOW: {len(SBZ_SHALLOW_out_arr)} SBZ parcels and {len(nonSBZ_SHALLOW_out_arr)} nonSBZ parcels')\n",
    "print(f'DEEP: {len(SBZ_DEEP_out_arr)} SBZ parcels and {len(nonSBZ_DEEP_out_arr)} nonSBZ parcels')\n",
    "print('\\n')\n",
    "print(f'ALL: {len(ColdPool_ALL_out_arr)} ColdPool parcels')\n",
    "print(f'SHALLOW: {len(ColdPool_SHALLOW_out_arr)} ColdPool parcels')\n",
    "print(f'DEEP: {len(ColdPool_DEEP_out_arr)} ColdPool parcels')\n",
    "\n",
    "\n",
    "def apply_job_array_filter(start_job, end_job):\n",
    "    # print(\"APPLYING JOB ARRAY\")\n",
    "\n",
    "    def job_filter(arr):\n",
    "        return arr[(arr[:, 0] >= start_job) & (arr[:, 0] < end_job)]\n",
    "\n",
    "    target_names = [\n",
    "        'CL_ALL_out_arr', 'nonCL_ALL_out_arr',\n",
    "        'CL_SHALLOW_out_arr', 'nonCL_SHALLOW_out_arr',\n",
    "        'CL_DEEP_out_arr', 'nonCL_DEEP_out_arr',\n",
    "        'SBZ_ALL_out_arr', 'nonSBZ_ALL_out_arr',\n",
    "        'SBZ_SHALLOW_out_arr', 'nonSBZ_SHALLOW_out_arr',\n",
    "        'SBZ_DEEP_out_arr', 'nonSBZ_DEEP_out_arr',\n",
    "        'ColdPool_ALL_out_arr', 'ColdPool_SHALLOW_out_arr', 'ColdPool_DEEP_out_arr'\n",
    "    ]\n",
    "\n",
    "    for name in target_names:\n",
    "        globals()[name+'_2'] = job_filter(globals()[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e201180-7978-4a9f-9033-9b33e98069c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "#SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7344a22-56f8-405a-89f4-4052a072cd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSING=False\n",
    "PROCESSING=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c882cdc-f4ef-474a-b302-28d76d48470d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeDictionary(**vars):\n",
    "    return vars\n",
    "    \n",
    "def GetData1(start_job,end_job):\n",
    "    dir2=dir+'Project_Algorithms/Lagrangian_Arrays/OUTPUT/'\n",
    "    in_file=dir2+f'lagrangian_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "    \n",
    "    var_names = ['W', 'QCQI']\n",
    "    data_dict = make_data_dict(in_file,var_names,read_type,start_job,end_job)\n",
    "    W, QCQI = (data_dict[k] for k in var_names)\n",
    "    \n",
    "    # #Making Time Matrix\n",
    "    # rows, cols = A.shape[0], A.shape[1]\n",
    "    # T = np.arange(rows).reshape(-1, 1) * np.ones((1, cols), dtype=int)\n",
    "    \n",
    "    dir2=dir+'Project_Algorithms/Lagrangian_Arrays/OUTPUT/'\n",
    "    in_file=dir2+f'lagrangian_VARS_array_{res}_{t_res}_{Np_str}.h5'\n",
    "    \n",
    "    # var_names = ['QV','TH','TH_V','TH_E','RH_VAPOR','RH_ICE','BUOYANCY_CM1','BUOYANCY_FULL','BUOYANCY_FULL_EACH_T','HMC']\n",
    "    var_names = ['QV','RH_VAPOR','TH_V','TH_E','MSE','HMC','VMF_G','VMF_C']\n",
    "    data_dict = make_data_dict(in_file,var_names,read_type,start_job,end_job)\n",
    "    QV, RH_VAPOR, TH_V, TH_E, MSE, HMC, VMF_G, VMF_C = [data_dict[k] for k in var_names] #, QI, QR\n",
    "\n",
    "    VARs=MakeDictionary(W=W, QCQI=QCQI, QV=QV, RH_VAPOR=RH_VAPOR,\n",
    "                        TH_V=TH_V, TH_E=TH_E, MSE=MSE, HMC=HMC, VMF_G=VMF_G, VMF_C=VMF_C)\n",
    "    return VARs\n",
    "\n",
    "def GetData2(start_job,end_job):    \n",
    "    dir2=dir+'Project_Algorithms/Lagrangian_Arrays/OUTPUT/'\n",
    "    if PROCESSING==True:\n",
    "        in_file=dir2+f'lagrangian_ED_VARS_array_{res}_{t_res}_{Np_str}.h5'\n",
    "    elif PROCESSING==False:\n",
    "        in_file=dir2+f'lagrangian_ED_VARS_array_{res}_{t_res}_{Np_str}_ORIGINAL.h5'\n",
    "    \n",
    "    var_names = ['E_G','E_C','D_C','D_G']\n",
    "    data_dict = make_data_dict(in_file,var_names,read_type,start_job,end_job)\n",
    "    E_G, E_C, D_C, D_G = (data_dict[k] for k in var_names)\n",
    "\n",
    "    NET_G, NET_C = E_G-D_G, E_C-D_C\n",
    "\n",
    "    VARs=MakeDictionary(E_G=E_G, E_C=E_C, D_C=D_C, D_G=D_G, NET_G=NET_G, NET_C=NET_C)\n",
    "    return VARs\n",
    "\n",
    "def GetData3(start_job,end_job):    \n",
    "    dir3=dir+'Project_Algorithms/Lagrangian_Arrays/OUTPUT/'\n",
    "    if PROCESSING==True:\n",
    "        in_file=dir3+f'lagrangian_Combined_ED_VARS_array_{res}_{t_res}_{Np_str}.h5' \n",
    "    elif PROCESSING==False:\n",
    "        in_file=dir3+f'lagrangian_Combined_ED_VARS_array_{res}_{t_res}_{Np_str}_ORIGINAL.h5' \n",
    "        \n",
    "    var_names = [\"C_TO_G_E\",\"G_TO_C_E\",\"C_TO_G_D\",\"G_TO_C_D\"]\n",
    "    data_dict = make_data_dict(in_file,var_names,read_type,start_job,end_job)\n",
    "    [C_TO_G_E, G_TO_C_E, C_TO_G_D, G_TO_C_D] = (data_dict[k] for k in var_names)\n",
    "\n",
    "    VARs=MakeDictionary(C_TO_G_E=C_TO_G_E, G_TO_C_E=G_TO_C_E, C_TO_G_D=C_TO_G_D, G_TO_C_D=G_TO_C_D)\n",
    "    return VARs\n",
    "\n",
    "\n",
    "def GetData4(start_job,end_job):    \n",
    "    dir3=dir+'Project_Algorithms/Lagrangian_Arrays/OUTPUT/'\n",
    "    if PROCESSING==True:\n",
    "        in_file=dir3+f'lagrangian_BUOYANCY_VARS_array_{res}_{t_res}_{Np_str}.h5' \n",
    "    elif PROCESSING==False:\n",
    "        in_file=dir3+f'lagrangian_BUOYANCY_VARS_array_{res}_{t_res}_{Np_str}_ORIGINAL.h5' \n",
    "        \n",
    "    var_names = ['BUOYANCY','TH_TERM','RV_TERM','RL_TERM']\n",
    "    data_dict = make_data_dict(in_file,var_names,read_type,start_job,end_job)\n",
    "    [BUOYANCY,TH_TERM,RV_TERM,RL_TERM] = (data_dict[k] for k in var_names)\n",
    "\n",
    "    VARs=MakeDictionary(BUOYANCY=BUOYANCY,TH_TERM=TH_TERM,RV_TERM=RV_TERM,RL_TERM=RL_TERM)\n",
    "    return VARs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c4f6ff-5d29-478c-9625-e21b676ab067",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "#DATA SETUP\n",
    "################################\n",
    "#*#*\n",
    "data_type=\"Tracked_Properties\" \n",
    "# data_type=\"Tracked_Entrainment_VMF\"\n",
    "# data_type=\"Tracked_Combined_Entrainment\" \n",
    "\n",
    "if data_type==\"Tracked_Properties\":\n",
    "    GetData=GetData1\n",
    "    variables = [\"W\",\"QCQI\",\"QV\",\"RH_VAPOR\",\"TH_V\",\"TH_E\",\"MSE\",\"HMC\",'VMF_G','VMF_C']\n",
    "elif data_type==\"Tracked_Entrainment_VMF\":\n",
    "    GetData=GetData2\n",
    "    variables = [\"VMF_C\",\"VMF_G\",\"E_G\",\"E_C\",\"D_C\",\"D_G\",\"NET_G\",\"NET_C\"]\n",
    "elif data_type==\"Tracked_Combined_Entrainment\":\n",
    "    GetData=GetData3\n",
    "    variables = [\"C_TO_G_E\",\"G_TO_C_E\",\"C_TO_G_D\",\"G_TO_C_D\"]\n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688c8ef3-e3be-4b32-91b3-378306c269fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "#JOB ARRAY SETUP\n",
    "################################\n",
    "#*#*\n",
    "# how many total jobs are being run? i.e. array=1-100 ==> num_jobs=100\n",
    "if Np_str=='1e6':\n",
    "    num_jobs=60 #1M parcels\n",
    "    num_slurm_jobs=10\n",
    "if Np_str=='50e6':\n",
    "    num_jobs=200 #50M parcels\n",
    "    num_slurm_jobs=60\n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2574efd-e888-435e-bae8-53880f499f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "#DATA LOADING FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c90f210-53db-47ab-b03e-616a985f9cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JOB ARRAY SETUP\n",
    "def StartJobArray(job_id,num_jobs):\n",
    "    total_elements=len(parcel1['xh']) #total num of variables\n",
    "\n",
    "    if num_jobs >= total_elements:\n",
    "        raise ValueError(\"Number of jobs cannot be greater than or equal to total elements.\")\n",
    "    \n",
    "    job_range = total_elements // num_jobs  # Base size for each chunk\n",
    "    remaining = total_elements % num_jobs   # Number of chunks with 1 extra \n",
    "    \n",
    "    # Function to compute the start and end for each job_id\n",
    "    def get_job_range(job_id, num_jobs):\n",
    "        job_id-=1\n",
    "        # Add one extra element to the first 'remaining' chunks\n",
    "        start_job = job_id * job_range + min(job_id, remaining)\n",
    "        end_job = start_job + job_range + (1 if job_id < remaining else 0)\n",
    "    \n",
    "        if job_id == num_jobs - 1: \n",
    "            end_job = total_elements #- 1\n",
    "        return start_job, end_job\n",
    "    # def job_testing():\n",
    "    #     #TESTING\n",
    "    #     start=[];end=[]\n",
    "    #     for job_id in range(1,num_jobs+1):\n",
    "    #         start_job, end_job = get_job_range(job_id)\n",
    "    #         print(start_job,end_job)\n",
    "    #         start.append(start_job)\n",
    "    #         end.append(end_job)\n",
    "    #     print(np.all(start!=end))\n",
    "    #     print(len(np.unique(start))==len(start))\n",
    "    #     print(len(np.unique(end))==len(end))\n",
    "    # job_testing()\n",
    "\n",
    "    # if sbatch==True:\n",
    "    #     job_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0)) #this is the current SBATCH job id\n",
    "    #     if job_id==0: job_id=1\n",
    "        \n",
    "    start_job, end_job = get_job_range(job_id, num_jobs)\n",
    "    index_adjust=start_job\n",
    "    # print(f'start_job = {start_job}, end_job = {end_job}')\n",
    "    return start_job,end_job,index_adjust\n",
    "\n",
    "# job_id=1\n",
    "# [start_job,end_job,index_adjust]=StartJobArray(job_id,num_jobs)\n",
    "# parcel=parcel1.isel(xh=slice(start_job,end_job))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498c4228-bc42-4478-8914-b43de8df388c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Back Data Later\n",
    "##############\n",
    "def make_data_dict(in_file,var_names,read_type,start_job,end_job):\n",
    "    if read_type=='h5py':\n",
    "        with h5py.File(in_file, 'r') as f:\n",
    "            data_dict = {var_name: f[var_name][:,start_job:end_job] for var_name in var_names}\n",
    "            \n",
    "    elif read_type=='xarray':\n",
    "        in_data = xr.open_dataset(\n",
    "            in_file,\n",
    "            engine='h5netcdf',\n",
    "            phony_dims='sort',\n",
    "            chunks={'phony_dim_0': 100, 'phony_dim_1': 1_000_000} \n",
    "        )\n",
    "        data_dict = {k: in_data[k][:,start_job:end_job].compute().data for k in var_names}\n",
    "    return data_dict\n",
    "\n",
    "# read_type='xarray'\n",
    "read_type='h5py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6738aa93-22b9-4ab3-b6a5-a2ca9c5d11a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetSpatialData(start_job,end_job):\n",
    "    dir2=dir+'Project_Algorithms/Lagrangian_Arrays/OUTPUT/'\n",
    "    in_file=dir2+f'lagrangian_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "    \n",
    "    var_names = ['Z', 'Y', 'X', 'z']\n",
    "    data_dict = make_data_dict(in_file,var_names,read_type,start_job,end_job)\n",
    "    Z, Y, X, parcel_z = (data_dict[k] for k in var_names)\n",
    "    \n",
    "    # #Making Time Matrix\n",
    "    # rows, cols = A.shape[0], A.shape[1]\n",
    "    # T = np.arange(rows).reshape(-1, 1) * np.ones((1, cols), dtype=int)\n",
    "    \n",
    "    return Z,Y,X,parcel_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4647cbea-dddf-4d67-9bb9-684c3352c8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcb030e-688d-4d29-9ad8-4f5cc5b968e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "#MAKE PROFILES FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a589cd5-6ab7-4048-9700-db5970a82281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SubsetTimeOverlap(Array, ts_window):\n",
    "    t1 = Array[:, 1]\n",
    "    t2 = Array[:, 2]\n",
    "    w1, w2 = ts_window[0], ts_window[-1]\n",
    "    keep = (t2 >= w1) & (t1 <= w2)\n",
    "    return Array[keep]\n",
    "def build_ZYX(ps, t1s, t2s, Z, Y, X, index_adjust):\n",
    "    rows = []\n",
    "    for p, t1, t2 in zip(ps.astype(int), t1s.astype(int), t2s.astype(int)):\n",
    "        t_range = np.arange(t1, t2 + 1)\n",
    "        rows.append(np.vstack([\n",
    "            np.full_like(t_range, p),\n",
    "            t_range,\n",
    "            Z[t_range, p - index_adjust],\n",
    "            Y[t_range, p - index_adjust],\n",
    "            X[t_range, p - index_adjust]\n",
    "        ]))\n",
    "    return np.hstack(rows) if rows else np.empty((5, 0))\n",
    "def match_p_values(A, B, tol=1e-8):\n",
    "    \"\"\"\n",
    "    Find parcel IDs (p values) in A that have a matching column in B.\n",
    "    First row (p) is matched exactly; rows 1..4 are matched with tolerance.\n",
    "\n",
    "    Args:\n",
    "        A: np.ndarray of shape (5, NA) [p, t, z, y, x]\n",
    "        B: np.ndarray of shape (5, NB) [p, t, z, y, x]\n",
    "        tol: tolerance for np.isclose on rows 1..4\n",
    "\n",
    "    Returns:\n",
    "        p_values: array of parcel IDs from A that match a column in B\n",
    "        i_idx: indices in A of matching columns\n",
    "        j_idx: indices in B of matching columns\n",
    "    \"\"\"\n",
    "    if A.size == 0 or B.size == 0:\n",
    "        return np.array([])\n",
    "\n",
    "    # exact match on p\n",
    "    eq = np.empty((5, A.shape[1], B.shape[1]), dtype=bool)\n",
    "    eq[0] = (A[0, :, None] == B[0, None, :])\n",
    "\n",
    "    # tolerant match on t,z,y,x\n",
    "    eq[1:] = np.isclose(A[1:, :, None], B[1:, None, :], atol=tol, rtol=0)\n",
    "\n",
    "    # find all column pairs that match entirely\n",
    "    pairwise = eq.all(axis=0)\n",
    "    i_idx, j_idx = np.where(pairwise) #columns in A and B respectively\n",
    "\n",
    "    # extract p values from A\n",
    "    p_values = A[0, i_idx]\n",
    "\n",
    "    return p_values\n",
    "def _init_empty(dtype):\n",
    "    return np.empty((0, 3), dtype=dtype)\n",
    "def _append_output(p_matches, ts_window, out_arr_group, acc_preconditioner, acc_preconditioned):\n",
    "    chunk_preconditioner = np.column_stack([\n",
    "        p_matches,\n",
    "        np.full_like(p_matches, ts_window[0]),\n",
    "        np.full_like(p_matches, ts_window[-1])\n",
    "    ])\n",
    "    chunk_preconditioned = np.where(np.isin(out_arr_group[:, 0], p_matches))[0]\n",
    "    if chunk_preconditioner.size:\n",
    "        acc_preconditioner = np.vstack([acc_preconditioner, chunk_preconditioner])\n",
    "    if chunk_preconditioned.size:\n",
    "        acc_preconditioned = np.vstack([acc_preconditioned, chunk_preconditioned])\n",
    "    return acc_preconditioner, acc_preconditioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1064ce-1a2f-4664-8852-739582083ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MoistPreconditioningSubset(Z,Y,X,type1='CL'): #type1_options = ['CL', 'nonCL', 'SBZ', 'nonSBZ', 'ColdPool']\n",
    "    global index_adjust\n",
    "\n",
    "    #READING IN TRACKED PARCEL LISTS\n",
    "    out_arr_ALL=globals()[f\"{type1}_ALL_out_arr\"+\"_2\"].copy()\n",
    "    out_arr_SHALLOW=globals()[f\"{type1}_SHALLOW_out_arr\"+\"_2\"].copy()\n",
    "    out_arr_DEEP=globals()[f\"{type1}_DEEP_out_arr\"+\"_2\"].copy()\n",
    "\n",
    "    #Initialize Outputs Before the Loop\n",
    "    out_arr_Preconditioner_SHALLOW, out_arr_Preconditioner_DEEP, \\\n",
    "    out_arr_Preconditioned_SHALLOW, out_arr_Preconditioned_DEEP = \\\n",
    "        (_init_empty(out_arr_ALL.dtype) for _ in range(4))\n",
    "\n",
    "\n",
    "    #CALCULATING\n",
    "    for row in range(out_arr_ALL.shape[0]): #Running through All Parcels\n",
    "        after=out_arr_ALL[row,3]\n",
    "        p=out_arr_ALL[row,0]\n",
    "        \n",
    "        #Getting Time from ALL Parcel Detrainment plus Another 30 Minutes\n",
    "        ts_end = min(out_arr_ALL[row, 2] + 1 + after, len(data1['time'])) #this takes care of exceeding buffers\n",
    "        t_end_clip = min(ts_end + int(30 * minutes), len(data1['time']) - 1) #clipping so doesn't go past data end time\n",
    "        ts_window = np.arange(ts_end + 1, t_end_clip + 1) #looking for detrained moisture entering other parcels within 30 mins\n",
    "        if ts_window.size == 0: #checking for emptiness\n",
    "            continue\n",
    "        \n",
    "        #Find if Parcel Enters another SHALLOW or DEEP Updraft\n",
    "        zs_window=Z[ts_window,p-index_adjust]\n",
    "        ys_window=Y[ts_window,p-index_adjust]\n",
    "        xs_window=X[ts_window,p-index_adjust]\n",
    "        \n",
    "        #Subsetting out Times where No Overlap with Detrained Parcels Occur\n",
    "        Array_SHALLOW=SubsetTimeOverlap(out_arr_SHALLOW.copy(), ts_window); Array_DEEP=SubsetTimeOverlap(out_arr_DEEP.copy(), ts_window)\n",
    "        Array_SHALLOW=Array_SHALLOW[Array_SHALLOW[:,0]!=p]; Array_DEEP=Array_DEEP[Array_DEEP[:,0]!=p]\n",
    "        \n",
    "        #GETTING ps, t1s, and t2s\n",
    "        ps_SHALLOW=Array_SHALLOW[:,0]; ps_DEEP=Array_DEEP[:,0]\n",
    "        t1s_SHALLOW=Array_SHALLOW[:,1]; t1s_DEEP=Array_DEEP[:,1]\n",
    "        t2s_SHALLOW=Array_SHALLOW[:,2]; t2s_DEEP=Array_DEEP[:,2]\n",
    "\n",
    "        #Getting list of p,t,Z,Y,X for each SHALLOW/DEEP parcel \n",
    "        ZYX_window_ALL = np.vstack([np.ones_like(ts_window)*p, ts_window, zs_window, ys_window, xs_window])\n",
    "        ZYX_window_SHALLOW = build_ZYX(ps_SHALLOW, t1s_SHALLOW, t2s_SHALLOW, Z, Y, X, index_adjust)\n",
    "        ZYX_window_DEEP = build_ZYX(ps_DEEP,    t1s_DEEP,    t2s_DEEP,    Z, Y, X, index_adjust)\n",
    "\n",
    "        #FINDING MATCHES \n",
    "        #where parcels from ALL enter other SHALLOW or DEEP trajectories\n",
    "        p_matches_SHALLOW = match_p_values(ZYX_window_ALL, ZYX_window_SHALLOW)\n",
    "        p_matches_DEEP = match_p_values(ZYX_window_ALL, ZYX_window_DEEP)\n",
    "\n",
    "        #FORMING OUTPUT ARRAY (consolidated)\n",
    "        # SHALLOW\n",
    "        out_arr_Preconditioner_SHALLOW, out_arr_Preconditioned_SHALLOW = _append_output(\n",
    "            p_matches_SHALLOW, ts_window, out_arr_SHALLOW,\n",
    "            out_arr_Preconditioner_SHALLOW, out_arr_Preconditioned_SHALLOW\n",
    "        )\n",
    "        # DEEP\n",
    "        out_arr_Preconditioner_DEEP, out_arr_Preconditioned_DEEP = _append_output(\n",
    "            p_matches_DEEP, ts_window, out_arr_DEEP,\n",
    "            out_arr_Preconditioner_DEEP, out_arr_Preconditioned_DEEP\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        f\"Preconditioner_SHALLOW_{type1}\": out_arr_Preconditioner_SHALLOW,\n",
    "        f\"Preconditioner_DEEP_{type1}\": out_arr_Preconditioner_DEEP,\n",
    "        f\"Preconditioned_SHALLOW_{type1}\": out_arr_Preconditioned_SHALLOW,\n",
    "        f\"Preconditioned_DEEP_{type1}\": out_arr_Preconditioned_DEEP\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bdb80d-deca-4537-9af8-7c7e28bdffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunCalculation_Simultaneous(Z,Y,X):\n",
    "    #VARs is a dictionary created by the function MakeDictionary\n",
    "    Dictionary = {}\n",
    "\n",
    "    type1_options = ['CL', 'nonCL', 'SBZ', 'nonSBZ', 'ColdPool']\n",
    "\n",
    "    for t1 in type1_options:\n",
    "        # print(f\"Running for type {t1}\")\n",
    "        Single_Dictionary=MoistPreconditioningSubset(Z,Y,X,type1=t1)\n",
    "        Dictionary.update(Single_Dictionary)\n",
    "    return Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800c41ee-806f-4a2c-aba0-018aaf41b0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveData(data_dict, job_id):\n",
    "    dir2 = dir + f'Project_Algorithms/Tracking_Algorithms/subsetting/'\n",
    "    out_file = dir2 + f\"parcel_tracking_SUBSET_Preconditioning_{res}_{t_res}_{Np_str}_{job_id}\" #*#*#\n",
    "\n",
    "    # Write the data to HDF5 file\n",
    "    with h5py.File(out_file, 'w') as h5f:\n",
    "        for key, value in data_dict.items():\n",
    "            h5f.create_dataset(key, data=value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e962cd8f-5764-4719-b1de-b2fd37cc605e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "#RUNNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e59cd5-6eab-46ee-ba2c-d31108d17dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*#* BIG ISSUE: it seems like job_array causes algorithm to miss out of some"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c912a1-c1d4-47b4-8687-e90d54c06778",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[start_slurm_job,end_slurm_job]=StartSlurmJobArray(num_jobs=num_jobs,num_slurm_jobs=num_slurm_jobs,ISRUN=False) #if ISRUN is False, then will not run using slurm_job_array\n",
    "print(f\"Running on Slurm_Jobs for Slurm_Job_Ids: {(start_slurm_job,end_slurm_job-1)}\")\n",
    "\n",
    "job_id_list=np.arange(start_slurm_job,end_slurm_job)\n",
    "for job_id in job_id_list:\n",
    "    if job_id % 1 == 0: print(f'current job_id = {job_id}')\n",
    "    [start_job,end_job,index_adjust]=StartJobArray(job_id,num_jobs)\n",
    "\n",
    "    #SLICING DATA\n",
    "    parcel=parcel1.isel(xh=slice(start_job,end_job))\n",
    "    apply_job_array_filter(start_job, end_job)\n",
    "\n",
    "    #GETTING DATA AND PUTTING IN A DICTIONARY\n",
    "    [Z,Y,X,parcel_z] = GetSpatialData(start_job,end_job) \n",
    "\n",
    "    #CALCULATING AND SAVING\n",
    "    Dictionary=RunCalculation_Simultaneous(Z,Y,X) #VERSION2: EACH VARIABLE SIMULTANEOUSLY (RECOMMENDED)\n",
    "    SaveData(Dictionary,job_id)\n",
    "    if any(arr.size > 0 for arr in Dictionary.values())==True: #*#*TESTING\n",
    "        print(f'non-empty')\n",
    "\n",
    "    # print_memory_usage(\"Current Memory\")\n",
    "    if job_id==job_id_list[0]:\n",
    "        Dictionary_keys=list(Dictionary.keys())\n",
    "    del Dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8289aa5b-e45e-467f-87d6-a8e9fa84d8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "#RECOMBINING\n",
    "recombine=False #KEEP FALSE WHEN RUNNING\n",
    "recombine=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f20409f-95b4-4e09-b6dd-e499cc79b194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadData(var_name,job_id):\n",
    "    dir2 = dir + f'Project_Algorithms/Tracking_Algorithms/subsetting/'\n",
    "    out_file=dir2+f\"parcel_tracking_SUBSET_Preconditioning_{res}_{t_res}_{Np_str}_{job_id}\"\n",
    "    with h5py.File(out_file, 'r') as f:\n",
    "        out = f[var_name][:]\n",
    "    return out\n",
    "# ReadData(var_name,job_id)\n",
    "def SaveFinalData(dict,out_file):    \n",
    "    with h5py.File(out_file,'w') as f:\n",
    "        for key in dict:\n",
    "            print(key)\n",
    "            f.create_dataset(key, data=dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0032dae0-9a0f-4f87-8d16-7fbe9a701e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "if recombine==True:\n",
    "    #GETTING COUNTS FOR MAKING INITIAL RECOMBINED ARRAYS LATER\n",
    "    count_dict = {key: [] for key in Dictionary_keys}\n",
    "    def MakeCount(count_dict):\n",
    "        print('Getting Tracked Parcel Count')\n",
    "        for job_id in np.arange(1,num_jobs+1):\n",
    "            if job_id % 10==0: print(f\"current job_id: {job_id}\")\n",
    "            for key in count_dict:\n",
    "                data_dict_key=ReadData(key,job_id)\n",
    "                count_dict[key].append(data_dict_key.shape[0])\n",
    "        combined_counts={key: sum(counts) for key, counts in count_dict.items()}\n",
    "        return combined_counts\n",
    "    combined_counts=MakeCount(count_dict)\n",
    "    print(combined_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4f5edd-4690-46d5-bad3-6d8fc06f7dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "if recombine==True:\n",
    "    def MakeEmpty(counts_dict):\n",
    "        empty_dict = {}\n",
    "        for key, count in counts_dict.items():\n",
    "            empty_dict[key] = np.zeros((count, 4), dtype=int)\n",
    "        return empty_dict\n",
    "    dict=MakeEmpty(combined_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafc249e-7c80-4feb-90bd-a8b591309e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if recombine==True:\n",
    "    for job_id in np.arange(1,num_jobs+1):\n",
    "        if job_id % 10==0: print(f\"current job_id: {job_id}\")\n",
    "            \n",
    "        for key in dict:\n",
    "            var=ReadData(key,job_id)\n",
    "            if var.size!=0:\n",
    "                a=np.where(np.all(dict[key] == 0, axis=1))[0][0]\n",
    "                b=a+var.shape[0]\n",
    "                # print(key,a,b) #TESTING\n",
    "                dict[key][a:b]=var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aff49ad-bd6e-42e1-9e37-c33bdb9f6ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if recombine==True:\n",
    "    dir2 = dir + f'Project_Algorithms/Tracking_Algorithms/OUTPUT/'\n",
    "    out_file=dir2+f\"parcel_tracking_SUBSET_Preconditioning_{res}_{t_res}_{Np_str}.h5\"\n",
    "    SaveFinalData(dict,out_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "work"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
