{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969c6cb7-1643-4447-88ec-a5330c725265",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS FUNCTION IS FOR RUNNING WITH SLURM JOB ARRAY\n",
    "#(SPLITS UP JOB_ARRAY BELOW INTO EVEN MORE TASKS)\n",
    "def StartSlurmJobArray(num_jobs,num_slurm_jobs, ISRUN):\n",
    "    job_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0)) #this is the current SBATCH job id\n",
    "    if job_id==0: job_id=1\n",
    "    if ISRUN==False:\n",
    "        start_job=1;end_job=num_jobs+1\n",
    "        return start_job,end_job\n",
    "    total_elements=num_jobs #total num of variables\n",
    "\n",
    "    job_range = total_elements // num_slurm_jobs  # Base size for each chunk\n",
    "    remaining = total_elements % num_slurm_jobs   # Number of chunks with 1 extra \n",
    "    \n",
    "    # Function to compute the start and end for each job_id\n",
    "    def get_job_range(job_id, num_slurm_jobs):\n",
    "        job_id-=1\n",
    "        # Add one extra element to the first 'remaining' chunks\n",
    "        start_job = job_id * job_range + min(job_id, remaining)\n",
    "        end_job = start_job + job_range + (1 if job_id < remaining else 0)\n",
    "    \n",
    "        if job_id == num_slurm_jobs - 1: \n",
    "            end_job = total_elements \n",
    "        return start_job, end_job\n",
    "    # def job_testing():\n",
    "    #     #TESTING\n",
    "    #     start=[];end=[]\n",
    "    #     for job_id in range(1,num_slurm_jobs+1):\n",
    "    #         start_job, end_job = get_job_range(job_id)\n",
    "    #         print(start_job,end_job)\n",
    "    #         start.append(start_job)\n",
    "    #         end.append(end_job)\n",
    "    #     print(np.all(start!=end))\n",
    "    #     print(len(np.unique(start))==len(start))\n",
    "    #     print(len(np.unique(end))==len(end))\n",
    "    # job_testing()\n",
    "    # if sbatch==True:\n",
    "        \n",
    "    start_job, end_job = get_job_range(job_id, num_slurm_jobs)\n",
    "    index_adjust=start_job\n",
    "    # print(f'start_job = {start_job}, end_job = {end_job}')\n",
    "    if start_job==0: start_job=1\n",
    "    if end_job==total_elements: end_job+=1\n",
    "    return start_job,end_job\n",
    "# job_id=1\n",
    "# [start_slurm_job,end_slurm_job,slurm_index_adjust]=StartSlurmJobArray(num_jobs,num_slurm_jobs,ISRUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db11099-d71a-464f-b01c-34d6797ef72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.lines as mlines\n",
    "import xarray as xr\n",
    "import os; import time\n",
    "import pickle\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd960628-c670-49c9-b586-2a1aecc96c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAIN DIRECTORIES\n",
    "mainDirectory='/mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/'\n",
    "scratchDirectory='/home/air673/koa_scratch/'\n",
    "codeDirectory='/mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/Project_Algorithms/Tracking_Algorithms'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2750bd32-beb7-4b9e-8a99-6c33e40004d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADING DATA\n",
    "def GetDataDirectories(simulationNumber):\n",
    "    if simulationNumber == 1:\n",
    "        Directory=os.path.join(mainDirectory,'Model/cm1r20.3/run')\n",
    "        res='1km'; t_res='5min'; Np_str='1e6'; Nz_str='34'\n",
    "    elif simulationNumber == 2:\n",
    "        Directory=scratchDirectory\n",
    "        res='1km'; t_res='1min'; Np_str='50e6'; Nz_str='95'\n",
    "    elif simulationNumber == 3:\n",
    "        Directory=scratchDirectory\n",
    "        res='250m'; t_res='1min'; Np_str='50e6'; Nz_str='95'\n",
    "        \n",
    "    dataDirectory = os.path.join(Directory, f\"cm1out_{res}_{t_res}_{Nz_str}nz.nc\")\n",
    "    parcelDirectory = os.path.join(Directory,f\"cm1out_pdata_{res}_{t_res}_{Np_str}np.nc\")\n",
    "    return dataDirectory, parcelDirectory, res,t_res,Np_str,Nz_str\n",
    "    \n",
    "def GetData(dataDirectory, parcelDirectory):\n",
    "    dataNC = xr.open_dataset(dataDirectory, decode_timedelta=True) \n",
    "    parcelNC = xr.open_dataset(parcelDirectory, decode_timedelta=True) \n",
    "    return dataNC,parcelNC\n",
    "\n",
    "def SubsetDataVars(dataNC):\n",
    "    varList = [\"thflux\", \"qvflux\", \"tsk\", \"cape\", \n",
    "               \"cin\", \"lcl\", \"lfc\", \"th\",\n",
    "               \"prs\", \"rho\", \"qv\", \"qc\",\n",
    "               \"qr\", \"qi\", \"qs\",\"qg\", \n",
    "               \"buoyancy\", \"uinterp\", \"vinterp\", \"winterp\",]\n",
    "    \n",
    "    varList += [\"ptb_hadv\", \"ptb_vadv\", \"ptb_hidiff\", \"ptb_vidiff\",\n",
    "                \"ptb_hturb\", \"ptb_vturb\", \"ptb_mp\", \"ptb_rdamp\", \n",
    "                \"ptb_rad\", \"ptb_div\", \"ptb_diss\",]\n",
    "    \n",
    "    varList += [\"qvb_hadv\", \"qvb_vadv\", \"qvb_hidiff\", \"qvb_vidiff\", \n",
    "                \"qvb_hturb\", \"qvb_vturb\", \"qvb_mp\",]\n",
    "    \n",
    "    varList += [\"wb_hadv\", \"wb_vadv\", \"wb_hidiff\", \"wb_vidiff\",\n",
    "                \"wb_hturb\", \"wb_vturb\", \"wb_pgrad\", \"wb_rdamp\", \"wb_buoy\",]\n",
    "\n",
    "    return dataNC[varList]\n",
    "\n",
    "[dataDirectory,parcelDirectory, res,t_res,Np_str,Nz_str] = GetDataDirectories(simulationNumber=1)\n",
    "[data1,parcel1] = GetData(dataDirectory, parcelDirectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053ae141-a937-47d0-a19b-862697fe1328",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceca7434-3c4d-4e23-9556-e680ccb23524",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4430f783-c58e-4585-ad82-e8a9bd3b018b",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "times=data1['time'].values/(1e9 * 60); times=times.astype(float);\n",
    "minutes=1/times[1] #1 / minutes per timestep = timesteps per minute\n",
    "kms=np.argmax(data1['xh'].values-data1['xh'][0].values >= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7931efc8-eda3-40b4-96f7-e50b59e13282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "dir2='/mnt/lustre/koa/koastore/torri_group/air_directory/DCI-Project/'\n",
    "path=dir2+'../Functions/'\n",
    "sys.path.append(path)\n",
    "\n",
    "import NumericalFunctions\n",
    "from NumericalFunctions import * # import NumericalFunctions \n",
    "import PlottingFunctions\n",
    "from PlottingFunctions import * # import PlottingFunctions\n",
    "\n",
    "\n",
    "# # Get all functions in NumericalFunctions\n",
    "# import inspect\n",
    "# functions = [f[0] for f in inspect.getmembers(NumericalFunctions, inspect.isfunction)]\n",
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7335eb9f-8fcf-4e3d-a2f0-aa00e795e64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Global variable to store original stdout\n",
    "_original_stdout = sys.stdout\n",
    "\n",
    "def BlockPrint():\n",
    "    \"\"\"Suppress all print() output.\"\"\"\n",
    "    global _original_stdout\n",
    "    sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "def RestorePrint():\n",
    "    \"\"\"Restore print() output.\"\"\"\n",
    "    global _original_stdout\n",
    "    sys.stdout.close()\n",
    "    sys.stdout = _original_stdout\n",
    "\n",
    "# NO_PRINT=False\n",
    "NO_PRINT=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbd538d-a080-402a-a043-9fb34e95a02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28f9dd4-1dde-4ce3-9917-1df11f878221",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "#JOB ARRAY SETUP\n",
    "################################\n",
    "#*#*\n",
    "# how many total jobs are being run? i.e. array=1-100 ==> num_jobs=100\n",
    "if '1e6' in Np_str:\n",
    "    num_jobs=60 #1M parcels\n",
    "    num_slurm_jobs=10\n",
    "if '50e6' in Np_str:\n",
    "    num_jobs=200 #50M parcels\n",
    "    num_slurm_jobs=60\n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0ad1ff-734d-4814-86ef-8c6d73e52821",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "#DATA LOADING FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95408a4c-65cd-4a3c-8953-edc972bb5d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JOB ARRAY SETUP\n",
    "def StartJobArray(job_id,num_jobs):\n",
    "    total_elements=len(parcel1['xh']) #total num of variables\n",
    "\n",
    "    if num_jobs >= total_elements:\n",
    "        raise ValueError(\"Number of jobs cannot be greater than or equal to total elements.\")\n",
    "    \n",
    "    job_range = total_elements // num_jobs  # Base size for each chunk\n",
    "    remaining = total_elements % num_jobs   # Number of chunks with 1 extra \n",
    "    \n",
    "    # Function to compute the start and end for each job_id\n",
    "    def get_job_range(job_id, num_jobs):\n",
    "        job_id-=1\n",
    "        # Add one extra element to the first 'remaining' chunks\n",
    "        start_job = job_id * job_range + min(job_id, remaining)\n",
    "        end_job = start_job + job_range + (1 if job_id < remaining else 0)\n",
    "    \n",
    "        if job_id == num_jobs - 1: \n",
    "            end_job = total_elements #- 1\n",
    "        return start_job, end_job\n",
    "    # def job_testing():\n",
    "    #     #TESTING\n",
    "    #     start=[];end=[]\n",
    "    #     for job_id in range(1,num_jobs+1):\n",
    "    #         start_job, end_job = get_job_range(job_id)\n",
    "    #         print(start_job,end_job)\n",
    "    #         start.append(start_job)\n",
    "    #         end.append(end_job)\n",
    "    #     print(np.all(start!=end))\n",
    "    #     print(len(np.unique(start))==len(start))\n",
    "    #     print(len(np.unique(end))==len(end))\n",
    "    # job_testing()\n",
    "\n",
    "    # if sbatch==True:\n",
    "    #     job_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0)) #this is the current SBATCH job id\n",
    "    #     if job_id==0: job_id=1\n",
    "        \n",
    "    start_job, end_job = get_job_range(job_id, num_jobs)\n",
    "    index_adjust=start_job\n",
    "    # print(f'start_job = {start_job}, end_job = {end_job}')\n",
    "    return start_job,end_job,index_adjust\n",
    "# job_id=1\n",
    "# [start_job,end_job,index_adjust]=StartJobArray(job_id,num_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff859fa-09d1-410f-8c3a-b5f8a840e45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SUBSETTING PARCEL DATA\n",
    "def GetData(parcel1,start_job,end_job):\n",
    "    parcel=parcel1.isel(xh=slice(start_job,end_job))\n",
    "    return parcel\n",
    "# parcel=GetData(parcel1,start_job,end_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0b814b-8426-4310-a5e1-465fdb1aecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetSpatialData(start_job,end_job):\n",
    "    dir2=dir+'Project_Algorithms/Lagrangian_Arrays/OUTPUT/'\n",
    "    open_file=dir2+f'lagrangian_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "    with h5py.File(open_file, 'r') as f:\n",
    "        parcel_z = f['z'][:,start_job:end_job]\n",
    "        parcel_x = f['x'][:,start_job:end_job] \n",
    "\n",
    "        parcel_u = f['u'][:,start_job:end_job] \n",
    "        parcel_w = f['w'][:,start_job:end_job] \n",
    "        \n",
    "        # Load the dataset by its name\n",
    "        Z = f['Z'][:,start_job:end_job]\n",
    "        Y = f['Y'][:,start_job:end_job]\n",
    "        X = f['X'][:,start_job:end_job]\n",
    "        W = f['W'][:,start_job:end_job]\n",
    "\n",
    "    return parcel_z,parcel_x,parcel_u,parcel_w,Z,Y,X,W\n",
    "# [parcel_z,parcel_x,parcel_u,parcel_w,Z,Y,X,W] = GetSpatialData(start_job,end_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e428e62d-8d62-4411-b2a4-f6e84aebec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetLFCData(start_job,end_job):\n",
    "    dir2=dir+'Project_Algorithms/Lagrangian_Arrays/OUTPUT/'\n",
    "    open_file=dir2+f'lagrangian_LFC_LCL_binary_array_{res}_{t_res}_{Np_str}.h5'\n",
    "    with h5py.File(open_file, 'r') as f:\n",
    "        # Load the dataset by its name\n",
    "        LFC = f['LFC'][:,start_job:end_job]\n",
    "        LCL = f['LCL'][:,start_job:end_job]\n",
    "    return LFC,LCL\n",
    "# LFC,LCL = GetLFCData(start_job,end_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d870bad5-ac84-40d8-85a4-293750f205fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "#LOADING CL MAXS FROM CL TRACKING ALGORITHM\n",
    "# File path\n",
    "file_path = f'/mnt/lustre/koa/koastore/torri_group/air_directory/' \\\n",
    "            f'DCI-Project/Project_Algorithms/Tracking_Algorithms/' \\\n",
    "            f'CL_Tracking_Out/' \\\n",
    "            f'whereCL_{res}_{t_res}_ALL_CLS.h5'\n",
    "\n",
    "# Open dataset (as it's valid NetCDF)\n",
    "whereCL = xr.open_dataset(file_path, engine='netcdf4')['maxconv_x']\n",
    "\n",
    "def Get_Conv_X(t,z,y):\n",
    "    # Conv_X_Max=whereCL[t,z,y,:].data\n",
    "    Conv_X_Max=whereCL.isel(time=t,z=z,y=y).data\n",
    "    return Conv_X_Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc9aece-9953-47b6-a4a5-0e03463efcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e02e69-4333-411f-8340-6781385925b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updated Lagrangian Tracking Algorithm\n",
    "\n",
    "#Algorithm Steps:\n",
    "#(1) Find the first time a parcel is above the LFC:\n",
    "#(2) First check if the parcel ascends (w>=0.1) for another 20 minutes\n",
    "#(3) If so, find first time, the parcel slows down (w<0.1)\n",
    "#(4) If that time is when the parcel is above 750m, save it, \"forget\", and move on to next parcel\n",
    "#(5) If that time is when the parcel is below 750m, check if it is within 2km of the CL_Max found from the CL Tracking Algorithm\n",
    "#(6) If the parcel is near the CL, store in, otherwise save it, \"forget\", and move on to next parcel\n",
    "#(7) Continue to next parcel\n",
    "\n",
    "#(Also, if during, traceback, the parcel escapes the x or z boundary, \"forget\" parcel, and move on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e645e76-ec24-4464-bc25-f4d9632cf0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numerical Settings\n",
    "Nt=len(data1['time']) \n",
    "dt=times[1]*60\n",
    "#Height parcel must be below to be counted\n",
    "CLmaxheight=750 #750m\n",
    "#BL slow-down-threshold\n",
    "w_thresh=0.1\n",
    "def GetNp(parcel):\n",
    "    Np=len(parcel['xh'])\n",
    "    # ascend_lst=[]\n",
    "    return Np\n",
    "# Np=GetNp(parcel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f5e82e-6419-49c5-901e-dc7de91030da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if ((x + dt*u)==0) or ((z + dt*w)==0)\n",
    "# u=u[t,Z[t,p],Y[t,p],X[t,p]]; W=W[t,p]\n",
    "# [u[t,Z[t,p],Y[t,p],X[t,p]] for t in time_arr] >np.max(data['xf'].values) or < np.min(data['xf'].values)\n",
    "# similarly for w\n",
    "################################################################################################################\n",
    "#BOUNDARY-ESCAPE CONDITION\n",
    "xmin=np.min(data1['xf'].values)*1e3\n",
    "xmax=np.max(data1['xf'].values)*1e3\n",
    "zmin=np.min(data1['zf'].values)*1e3\n",
    "zmax=np.max(data1['zf'].values)*1e3\n",
    "\n",
    "def check_boundary(p,where_BL,above_LFC):\n",
    "    time_arr=np.arange(where_BL,above_LFC)\n",
    "\n",
    "    def get_x(t,p):\n",
    "        # return parcel['x'][t,p].item()\n",
    "        return parcel_x[t,p] \n",
    "    def get_u(t,p):\n",
    "        # return data['uinterp'].isel(time=t,zh=Z[t,p],yh=Y[t,p],xh=X[t,p]).item() #TESTING\n",
    "        # return parcel['u'][t,p].item() \n",
    "        return parcel_u[t,p]\n",
    "    def get_z(t,p):\n",
    "        # return parcel['z'][t,p].item()\n",
    "        return parcel_z[t,p]\n",
    "    def get_w(t,p):\n",
    "        # return data['winterp'].isel(time=t,zh=Z[t,p],yh=Y[t,p],xh=X[t,p]).item()\n",
    "        # return parcel['w'][t,p].item()\n",
    "        return parcel_w[t,p]\n",
    "        \n",
    "\n",
    "    # x_tend = [get_x(t, p) + dt * get_u(t, z, y, x)   #THIS IS OLD, LESS IDEAL\n",
    "    #       for (t, z, y, x) in zip(time_arr, Z[time_arr, p], Y[time_arr, p], X[time_arr, p])] \n",
    "    # z_tend = [get_z(t, p) + dt * get_w(t, z, y, x)  \n",
    "    #       for (t, z, y, x) in zip(time_arr, Z[time_arr, p], Y[time_arr, p], X[time_arr, p])] \n",
    "    \n",
    "    x_tend = [get_x(t, p) + dt * get_u(t,p)   \n",
    "          for (t, z, y, x) in zip(time_arr, Z[time_arr, p], Y[time_arr, p], X[time_arr, p])] \n",
    "    z_tend = [get_z(t, p) + dt * get_w(t,p)  \n",
    "          for (t, z, y, x) in zip(time_arr, Z[time_arr, p], Y[time_arr, p], X[time_arr, p])] \n",
    "\n",
    "    x_bound=any(val < xmin or val > xmax for val in x_tend)*1\n",
    "    z_bound=any(val < zmin or val > zmax for val in z_tend)*1\n",
    "\n",
    "    out=(x_bound,z_bound)\n",
    "    if out[0]==1:\n",
    "        print(f'parcel {p} crossed x-boundary between t={where_BL} and t={above_LFC}')\n",
    "    elif out[1]==1:\n",
    "        print(f'parcel {p} crossed z-boundary between t={where_BL} and t={above_LFC}')\n",
    "    return out\n",
    "#############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387b950e-bac3-4933-9a40-095ca570d4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Output Storage Vector\n",
    "def InitializeData(Np):\n",
    "    out_arr=np.zeros((Np,3),dtype=np.int32) \n",
    "    save_arr=np.zeros((Np,3),dtype=np.int32) #This one is for saving continued-ascent, slow-below-750m parcels that are not with 2 km of CL\n",
    "    save2_arr=np.zeros((Np,3),dtype=np.int32) #This one is for saving continued-ascent, slow-above-750m parcels\n",
    "    return out_arr,save_arr,save2_arr\n",
    "# [out_arr,save_arr,save2_arr]=InitializeData(Np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0eed3f-6b8c-4230-9abc-37e560986dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################################\n",
    "#The Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1f6fe0-558b-4087-8ba1-f6ca642d5b4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ParcelTracking(Np,W,LFC,parcel_z,Z,Y,X,out_arr,save_arr,save2_arr):\n",
    "    #1--------------Looping over each parcel\n",
    "    for count,p in enumerate(np.arange(Np)): \n",
    "        if np.mod(p,5e4)==0: print(f'current parcel: {p}/{Np}')\n",
    "        \n",
    "        W_p = W[:,p]\n",
    "        LFC_p = LFC[:,p] \n",
    "       \n",
    "        #----FIND WHERE PARCEL IS ABOVE LFC----\n",
    "        indices = np.where(LFC_p == 1)[0]; above_LFC = indices[0] if indices.size > 0 else -999; #FIRST TIME ABOVE LFC\n",
    "        if above_LFC ==-999:\n",
    "            # print(f'parcel {p} never above LFC')\n",
    "            continue #if the parcel is never above the LFC, skip the parcel\n",
    "        \n",
    "        #----CHECK IF ASCENDS FOR >= 20 minutes AFTER LFC----\n",
    "        ascend_array=W_p[above_LFC+1:]\n",
    "        indices=np.where(ascend_array==0)[0]; ascend_stop=indices[0] if indices.size > 0 else 10000; #location of where parcel stops ascending (labeled 10000 to mark for future analysis)\n",
    "        # ascend_lst.append(ascend_stop) #(also store for histogram)\n",
    "        if ascend_stop>=20*minutes:\n",
    "        \n",
    "            #----FIND THE FIRST TIME W_p<=w_thresh----\n",
    "            indices=np.where(W_p[0:above_LFC]<w_thresh)[0]\n",
    "            where_BL=indices[-1] if indices.size > 0 else -999 #FIRST PRIOR TIME W<0.1 (IN THE BL) (ADDED 1 TO GET TIME RIGHT AFTER INTERACTION)\n",
    "            if where_BL ==-999:\n",
    "                # print(f'parcel {p} w is never below threshold prior to t={above_LFC}')\n",
    "                continue #if the parcel never slows down backwards in time (unlikely), skip the parcel\n",
    "                \n",
    "            #check for boundary escapes\n",
    "            ################################\n",
    "            future_location=check_boundary(p,where_BL,above_LFC)\n",
    "            if (future_location[0]+future_location[1]>=1): continue #if parcel crosses boundary, skips current parcel\n",
    "            ################################\n",
    "            \n",
    "            #----CHECK IF PARCEL SLOWED DOWN LOW ENOUGH----\n",
    "            if parcel_z[where_BL,p]<=CLmaxheight: #PARCEL MUST BE BELOW 750m WHEN CONTACTING CL #***\n",
    "            # if LCL[where_BL,p]==0: #PARCEL MUST BE BELOW LCL WHEN CONTACTING CL (not recommended)\n",
    "        \n",
    "                #----CHECK IF CL IS WITHIN 2km----\n",
    "                #Find the CL-max x-location\n",
    "                t=where_BL; z=Z[where_BL,p]; y=Y[where_BL,p]; x=X[where_BL,p]\n",
    "                CONV_X=Get_Conv_X(t,z,y)\n",
    "                within_CL=np.any(np.isin(CONV_X, np.arange(x-2*kms,x+3*kms)))\n",
    "                \n",
    "                if within_CL==True:\n",
    "                    #save X's (t,p) \n",
    "                    print(f'Parcel {p} is success at time {where_BL}')\n",
    "                    out_arr[p,0]=p\n",
    "                    out_arr[p,1]=where_BL\n",
    "                    out_arr[p,2]=above_LFC \n",
    "                else: #continued-ascent, slow-below-750m parcels that are not with 2 km of CL\n",
    "                    #SAVE PARCEL\n",
    "                    # print(f'Parcel {p} not near CL at t={where_BL}')\n",
    "                    save_arr[p,0]=p\n",
    "                    save_arr[p,1]=where_BL\n",
    "                    save_arr[p,2]=above_LFC \n",
    "        \n",
    "            else: #continued-ascent, slow-above-750m parcels\n",
    "                #SAVE PARCEL\n",
    "                # print(f'Parcel {p} above {CLmaxheight}m at t={where_BL}')\n",
    "                save2_arr[p,0]=p\n",
    "                save2_arr[p,1]=where_BL\n",
    "                save2_arr[p,2]=above_LFC         \n",
    "                \n",
    "            #END OF LOOP, THEN WE MOVE ON TO NEXT PARCEL p\n",
    "    return out_arr,save_arr,save2_arr\n",
    "# [out_arr,save_arr,save2_arr]=ParcelTracking(Np,W,LFC,parcel_z,Z,Y,X,out_arr,save_arr,save2_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eade392-bb34-4c0f-a93a-1cc1da5fea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CorrectParcelID(out_arr,save_arr,save2_arr,index_adjust):\n",
    "    #CORRECTING DATA PARCEL ID BASED ON JOB NUMBER\n",
    "    #####################################################\n",
    "    out_arr[np.where(np.any(out_arr != 0, axis=1))[0],0]+=index_adjust #*needed for job array*+=index_adjust #*needed for job array*\n",
    "    save_arr[np.where(np.any(save_arr != 0, axis=1))[0],0]+=index_adjust #*needed for job array*+=index_adjust #*needed for job array*\n",
    "    save2_arr[np.where(np.any(save2_arr != 0, axis=1))[0],0]+=index_adjust #*needed for job array*+=index_adjust #*needed for job array*\n",
    "    return out_arr,save_arr,save2_arr\n",
    "# [out_arr,save_arr,save2_arr]=CorrectParcelID(out_arr,save_arr,save2_arr,index_adjust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b12a89-f869-4b35-81b5-5f1b56cacbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVING BLANK ROWS\n",
    "def RemoveZeroRows(arr):\n",
    "    arr = arr[~np.all(arr == 0, axis=1)]\n",
    "    return arr\n",
    "# out_arr=RemoveZeroRows(out_arr);save_arr=RemoveZeroRows(save_arr);save2_arr=RemoveZeroRows(save2_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbefa819-624e-4353-bd13-ed131d46539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveOutFile(out_arr,save_arr,save2_arr,job_id):\n",
    "    # #Storing output and save data\n",
    "    #USES H5 FILE\n",
    "    ###################################################################################################################################\n",
    "    out_file = dir + f'Project_Algorithms/Tracking_Algorithms/trackout/parcel_tracking_{res}_{t_res}_{Np_str}_{job_id}.h5'\n",
    "    with h5py.File(out_file, 'w') as hf:\n",
    "        hf.create_dataset('out_arr', data=out_arr, compression=\"gzip\")\n",
    "        hf.create_dataset('save_arr', data=save_arr, compression=\"gzip\")\n",
    "        hf.create_dataset('save2_arr', data=save2_arr, compression=\"gzip\")\n",
    "# SaveOutFile(out_arr,save_arr,save2_arr,job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9dd1af-32c7-4389-a1c6-ec1930730bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "#RUNNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2677b5c1-8300-40b4-961f-e8acfb0f98eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[start_slurm_job,end_slurm_job]=StartSlurmJobArray(num_jobs=num_jobs,num_slurm_jobs=num_slurm_jobs,ISRUN=True) #if ISRUN is False, then will not run using slurm_job_array\n",
    "print(f\"Running on Slurm_Jobs for Slurm_Job_Ids: {(start_slurm_job,end_slurm_job-1)}\")\n",
    "StartTime = time.time()\n",
    "job_id_list=np.arange(start_slurm_job,end_slurm_job)\n",
    "for job_id in job_id_list:\n",
    "    if job_id % 1 == 0: print(f'current job_id = {job_id}')\n",
    "    [start_job,end_job,index_adjust]=StartJobArray(job_id,num_jobs)\n",
    "    \n",
    "\n",
    "    #SLICING DATA\n",
    "    parcel=GetData(parcel1,start_job,end_job)\n",
    "\n",
    "    #GETTING REQUIRED DATA\n",
    "    [parcel_z,parcel_x,parcel_u,parcel_w,Z,Y,X,W] = GetSpatialData(start_job,end_job)\n",
    "    LFC,LCL = GetLFCData(start_job,end_job)\n",
    "\n",
    "    #INITIALIZING DATA\n",
    "    Np=GetNp(parcel)\n",
    "    [out_arr,save_arr,save2_arr]=InitializeData(Np)\n",
    "\n",
    "    #RUNNING ALGORITHM\n",
    "    start_time = time.time()\n",
    "    if NO_PRINT==True: BlockPrint()\n",
    "    [out_arr,save_arr,save2_arr]=ParcelTracking(Np,W,LFC,parcel_z,Z,Y,X,out_arr,save_arr,save2_arr)\n",
    "    if NO_PRINT==True: RestorePrint()\n",
    "    end_time = time.time(); elapsed_time = end_time - start_time; print(f\"Elapsed Time: {elapsed_time} seconds\")  \n",
    "\n",
    "    #CORRECTING PARCEL ID FOR JOBARRAY\n",
    "    [out_arr,save_arr,save2_arr]=CorrectParcelID(out_arr,save_arr,save2_arr,index_adjust)\n",
    "\n",
    "    #REMOVING BLANK ROWS FROMRESULTS\n",
    "    out_arr=RemoveZeroRows(out_arr);save_arr=RemoveZeroRows(save_arr);save2_arr=RemoveZeroRows(save2_arr)\n",
    "\n",
    "    #SAVING\n",
    "    SaveOutFile(out_arr,save_arr,save2_arr,job_id)\n",
    "EndTime = time.time(); ElapsedTime = EndTime - StartTime; print(f\"Total Elapsed Time: {ElapsedTime} seconds\")      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7a78cc-b1ba-4495-a482-dc791048a1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "#Run after finishing job_array\n",
    "recombine=False #KEEP FALSE WHEN JOB_ARRAY IS RUNNING\n",
    "recombine=True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0c83bc-0063-4eae-80af-317f43baf3a5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Recombine(num_jobs):\n",
    "    var_names = ['out_arr', 'save_arr', 'save2_arr']\n",
    "    recombined_arrays = {}  # Store final arrays here\n",
    "\n",
    "    def MakeUnique(arr):\n",
    "        return np.unique(arr, axis=0)\n",
    "    def get_total_count(var_name):\n",
    "        return sum(\n",
    "            h5py.File(dir + f'Project_Algorithms/Tracking_Algorithms/trackout/parcel_tracking_{res}_{t_res}_{Np_str}_{job_id}.h5', 'r')[var_name].shape[0]\n",
    "            for job_id in range(1, num_jobs + 1)\n",
    "        )\n",
    "\n",
    "    def load_file(job_id, var_name):\n",
    "        in_file = dir + f'Project_Algorithms/Tracking_Algorithms/trackout/parcel_tracking_{res}_{t_res}_{Np_str}_{job_id}.h5'\n",
    "        with h5py.File(in_file, 'r') as hf:\n",
    "            arr = hf[var_name][:]\n",
    "        return arr\n",
    "\n",
    "    # Preallocate arrays\n",
    "    for var_name in var_names:\n",
    "        total_count = get_total_count(var_name)\n",
    "        recombined_arrays[var_name] = np.zeros((total_count, 3), dtype=np.int32)\n",
    "\n",
    "    # Fill arrays\n",
    "    for var_name in var_names:\n",
    "        print(f\"Combining data for {var_name}\")\n",
    "        left_ind = 0\n",
    "        for job_id in range(1, num_jobs + 1):\n",
    "            # if job_id % 10 == 0: print(f\"{var_name}: processing job {job_id}\")\n",
    "            arr = load_file(job_id, var_name)\n",
    "            n_rows = arr.shape[0]; right_ind = left_ind + n_rows\n",
    "            recombined_arrays[var_name][left_ind:right_ind, :] = arr\n",
    "            left_ind = right_ind\n",
    "\n",
    "    #Make Unique\n",
    "    for var_name in var_names:\n",
    "        recombined_arrays[var_name]=MakeUnique(recombined_arrays[var_name])\n",
    "        \n",
    "    # Write to file\n",
    "    out_file = dir + f'Project_Algorithms/Tracking_Algorithms/OUTPUT/parcel_tracking_{res}_{t_res}_{Np_str}.h5' #*#*\n",
    "    with h5py.File(out_file, 'w') as hf:\n",
    "        for var_name in var_names:\n",
    "\n",
    "            hf.create_dataset(var_name, data=recombined_arrays[var_name], compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1b74d1-ba85-4b12-9e14-3100ba52fe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if recombine==True:\n",
    "    Recombine(num_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de980f1-5ff2-4049-bfe2-269aed4d8037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065dee06-3321-4b77-b329-71ce5d137525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d599022-7a7c-4b97-9168-94781a663336",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a25eeb2-6b94-436c-bff5-142551453075",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ea522e-734b-407b-9cfa-081d43743c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADING BACK IN\n",
    "def load_file():\n",
    "    in_file=dir+f'Project_Algorithms/Tracking_Algorithms/parcel_tracking_{res}_{t_res}_{Np_str}.h5'\n",
    "    with h5py.File(in_file, 'r') as hf:\n",
    "        out_arr=hf['out_arr'][:]\n",
    "        save_arr=hf['save_arr'][:]\n",
    "        save2_arr=hf['save2_arr'][:]\n",
    "    return out_arr,save_arr,save2_arr\n",
    "[out_arr,save_arr,save2_arr]=load_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e9364b-9d61-4d9a-9fe4-46dfd32245bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "#TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87562d0-b7f7-430d-845e-1239e42ad061",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #TESTING\n",
    "# out_file = dir + f'Project_Algorithms/Tracking_Algorithms/parcel_tracking_{res}_{t_res}_{Np_str}.h5'\n",
    "# with h5py.File(out_file, 'r') as hf:\n",
    "#     test=hf['out_arr'][0:232]\n",
    "# np.all(test==out_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93cea9b-f8a7-418c-8137-99354634d8ac",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#TESTING (ADDITIONAL TRACKED PARCELS FROM JOB 60\n",
    "# # ind=0\n",
    "# ind+=1;print(ind)\n",
    "# ind_arr = out_arr[ind]\n",
    "# p=ind_arr[0]-index_adjust\n",
    "# t1=ind_arr[1]-10\n",
    "# t2=ind_arr[1]+10\n",
    "# plt.plot(parcel_z[t1:t2,p])\n",
    "# plt.xlabel('t');plt.ylabel('z')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "work"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
