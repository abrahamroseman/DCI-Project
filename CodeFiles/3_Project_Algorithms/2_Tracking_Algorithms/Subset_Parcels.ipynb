{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "3e20afff-5c3b-49aa-b9b0-5d10b15fc658",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "#ENVIRONMENT SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "ceeecbb2-8c23-4058-a16c-1975f4ec70ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import matplotlib.gridspec as gridspec\n",
    "import xarray as xr\n",
    "\n",
    "import sys; import os; import time; from datetime import timedelta\n",
    "import pickle\n",
    "import h5py\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "d4b81c58-468e-45db-a885-d66eb92ab0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAIN DIRECTORIES\n",
    "def GetDirectories():\n",
    "    mainDirectory='/mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/'\n",
    "    mainCodeDirectory=os.path.join(mainDirectory,\"Code/CodeFiles/\")\n",
    "    scratchDirectory='/mnt/lustre/koa/scratch/air673/'\n",
    "    codeDirectory=os.getcwd()\n",
    "    return mainDirectory,mainCodeDirectory,scratchDirectory,codeDirectory\n",
    "\n",
    "[mainDirectory,mainCodeDirectory,scratchDirectory,codeDirectory] = GetDirectories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "486889b9-f836-4eac-a7b1-1baefca7feb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT CLASSES\n",
    "sys.path.append(os.path.join(mainCodeDirectory,\"2_Variable_Calculation\"))\n",
    "from CLASSES_Variable_Calculation import ModelData_Class, SlurmJobArray_Class, DataManager_Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "666f6349-17ae-426f-af05-7b014e1bce7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CM1 Data Summary ===\n",
      " Simulation #:   1\n",
      " Resolution:     1km\n",
      " Time step:      5min\n",
      " Vertical levels:34\n",
      " Parcels:        1e6\n",
      " Data file:      /mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/Model/cm1r20.3/run/cm1out_1km_5min_34nz.nc\n",
      " Parcel file:    /mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/Model/cm1r20.3/run/cm1out_pdata_1km_5min_1e6np.nc\n",
      " Time steps:     133\n",
      "========================= \n",
      "\n",
      "=== DataManager Summary ===\n",
      " inputDirectory #:   /mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/Code/OUTPUT/Variable_Calculation/TimeSplitModelData\n",
      " outputDirectory #:   /mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/Code/OUTPUT/Project_Algorithms/Tracking_Algorithms\n",
      " inputDataDirectory #:   /mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/Code/OUTPUT/Variable_Calculation/TimeSplitModelData/1km_5min_34nz/ModelData\n",
      " inputParcelDirectory #:   /mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/Code/OUTPUT/Variable_Calculation/TimeSplitModelData/1km_5min_34nz/ParcelData\n",
      " outputDataDirectory #:   /mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/Code/OUTPUT/Project_Algorithms/Tracking_Algorithms/1km_5min_34nz/Lagrangian_UpdraftTracking\n",
      "========================= \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#data loading class\n",
    "ModelData = ModelData_Class(mainDirectory, scratchDirectory, simulationNumber=1)\n",
    "#data manager class\n",
    "DataManager = DataManager_Class(mainDirectory, scratchDirectory, ModelData.res, ModelData.t_res, ModelData.Nz_str,\n",
    "                                ModelData.Np_str, dataType=\"Tracking_Algorithms\", dataName=\"Lagrangian_UpdraftTracking\",\n",
    "                                dtype='float32',codeSection = \"Project_Algorithms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "1f4e0d3a-2612-483f-86a2-86991e0ac520",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT FUNCTIONS\n",
    "sys.path.append(os.path.join(mainCodeDirectory,\"2_Variable_Calculation\"))\n",
    "import FUNCTIONS_Variable_Calculation\n",
    "from FUNCTIONS_Variable_Calculation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c6f95ada-7430-4693-a1fe-c47491ce05fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT CLASSES\n",
    "sys.path.append(os.path.join(mainCodeDirectory,\"3_Project_Algorithms\",\"2_Tracking_Algorithms\"))\n",
    "from CLASSES_TrackingAlgorithms import TrackingAlgorithms_DataLoading_Class, SlurmJobArray_Class, Results_InputOutput_Class, TrackedParcel_Loading_Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "36ab89cf-6fc3-4a0e-b20d-52cb1b3f69e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "#SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "c58086e0-c188-48e9-99e2-967ffdec6a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "#JOB ARRAY SETUP\n",
    "################################\n",
    "# how many total jobs are being run? i.e. array=1-100 ==> num_jobs=100\n",
    "if '1e6' in ModelData.Np_str:\n",
    "    num_jobs=60 #1M parcels\n",
    "    num_slurm_jobs=10\n",
    "if '50e6' in ModelData.Np_str:\n",
    "    num_jobs=200 #50M parcels\n",
    "    num_slurm_jobs=60\n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "add22ad5-7bf2-4ddf-9137-d99c560c1c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "#MODEL AND ALGORITHM NUMERICAL PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "7f0fd291-1eed-4ce1-b3f2-678da1c32bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "times=ModelData.time/(1e9 * 60); times=times.astype(float);\n",
    "minutes=1/times[1] #1 / minutes per timestep = timesteps per minute\n",
    "kms=np.argmax(ModelData.xh-ModelData.xh[0] >= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "5c8640a9-64ea-4325-b6d4-355d2690d124",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "#DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "0968d219-f754-4a6e-a9a7-77b0ed180a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = f\"/mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/Code/OUTPUT/Variable_Calculation/LagrangianArrays/{ModelData.res}_{ModelData.t_res}_{ModelData.Nz_str}nz/Lagrangian_Binary_Array/\"\n",
    "Lagrangian_Binary_Array_Data,files = OpenMultipleSingleTimes_LagrangianArray(directory, ModelData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "68971fdc-bfc9-405b-bc7c-553bde792828",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = f\"/mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/Code/OUTPUT/Variable_Calculation/LagrangianArrays/{ModelData.res}_{ModelData.t_res}_{ModelData.Nz_str}nz/LFC/\"\n",
    "\n",
    "LFC_LCL_Data,files = OpenMultipleSingleTimes_LagrangianArray(directory, ModelData,pattern=\"LFC_*.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "f609baf2-2210-4f52-b4a0-8544747a798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "#DATA LOADING FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "6b6b0624-95b9-466b-b978-ede06d908172",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def GetSpatialData(Lagrangian_Binary_Array_Data, start_job,end_job):\n",
    "    W = Lagrangian_Binary_Array_Data['W'].isel(p=slice(start_job,end_job)).data.compute()\n",
    "    QCQI = Lagrangian_Binary_Array_Data['QCQI'].isel(p=slice(start_job,end_job)).data.compute()\n",
    "    Z = Lagrangian_Binary_Array_Data['Z'].isel(p=slice(start_job,end_job)).data.compute()\n",
    "    Y = Lagrangian_Binary_Array_Data['Y'].isel(p=slice(start_job,end_job)).data.compute()\n",
    "    X = Lagrangian_Binary_Array_Data['X'].isel(p=slice(start_job,end_job)).data.compute()\n",
    "\n",
    "    parcel_z = Lagrangian_Binary_Array_Data['z'].isel(p=slice(start_job,end_job)).data.compute()\n",
    "\n",
    "    return W,QCQI,Z,Y,X,parcel_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "38c56cec-2745-42e5-a283-40206839ca02",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def GetLFCData(LFC_LCL_Data, start_job,end_job):\n",
    "    LFC = LFC_LCL_Data['LFC'].isel(p=slice(start_job,end_job)).data.compute()\n",
    "    LCL = LFC_LCL_Data['LCL'].isel(p=slice(start_job,end_job)).data.compute()\n",
    "    return LFC,LCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "cc25c858-06e9-443f-a662-44a99a74c832",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "#MORE FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "78f9486f-b6f7-4ad1-8608-b02e21456ce9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#DOMAIN SUBSETTING \n",
    "#finding time subset\n",
    "def GetTimeSubset(noSubset=False):\n",
    "    if noSubset == True:\n",
    "        t_start=0; t_end=ModelData.Ntime+1\n",
    "        # print(f't in {t_start}:{t_end}')\n",
    "        tSubset = np.arange(t_start,t_end+1)\n",
    "    else:\n",
    "        dt=ModelData.time[1].item()/1e9 #seconds per timestep\n",
    "        dhours=(dt/60**2) #hours per timestep\n",
    "    \n",
    "        #Finding Left Boundary\n",
    "        start_hour=4 #10:00 am\n",
    "        t_start=int(start_hour/dhours)\n",
    "        \n",
    "        #Finding Right Boundary\n",
    "        end_hour=11 #5pm\n",
    "        t_end=int(end_hour/dhours)+1\n",
    "        \n",
    "        #printing\n",
    "        # print(f't in {t_start}:{t_end}')\n",
    "        tSubset = np.arange(t_start,t_end+1)\n",
    "    return tSubset\n",
    "\n",
    "# def GetZSubset(noSubset=True): #(not in use)\n",
    "#     if noSubset == True:\n",
    "#         zh_start=0; zh_end=ModelData.Nzh\n",
    "#         zf_start=0; zf_end=ModelData.Nzf\n",
    "#     else:\n",
    "#         #Finding Boundarys\n",
    "#         zhs=ModelData.zh\n",
    "#         zh_start=0; zh_end=int(np.where(zhs>=19)[0][0])\n",
    "#         zfs=ModelData.zf\n",
    "#         zf_start=0; zf_end=int(np.where(zfs>=20)[0][0])\n",
    "\n",
    "#     print(f'zh in {zh_start}:{zh_end}'+f', zf in {zf_start}:{zf_end}')\n",
    "#     zhSubset = np.arange(zh_start,zh_end+1)\n",
    "#     zfSubset = np.arange(zf_start,zf_end+1)\n",
    "#     return zhSubset, zfSubset\n",
    "\n",
    "# def GetYSubset(noSubset=True): #(not in use)\n",
    "\n",
    "def GetXSubset(noSubset=False):\n",
    "\n",
    "    xh = ModelData.xh - ModelData.xh[0] \n",
    "    xf = ModelData.xf - ModelData.xf[0]\n",
    "    xh_max = xh[-1] #total physical length\n",
    "    xf_max = xf[-1]\n",
    "\n",
    "    if noSubset == True:\n",
    "        xh_start=0; xh_end = xh_max+1\n",
    "        xf_start=0; xf_end = xf_max+1\n",
    "    else:\n",
    "        #Finding Left Boundary\n",
    "        ocean_percent=0.25\n",
    "        left_to_coast=ModelData.xh[0]+(ModelData.xh[-1]-ModelData.xh[0])*ocean_percent\n",
    "        xh_start=np.where(ModelData.xh>=left_to_coast)[0][0]\n",
    "        xf_start=np.where(ModelData.xf>=left_to_coast)[0][0]\n",
    "        \n",
    "        #Finding Right Boundary\n",
    "        right_fraction=80/100\n",
    "        \n",
    "        # Find index where physical location exceeds 80% of domain\n",
    "        xh_end = np.where(xh > right_fraction * xh_max)[0][0]+1\n",
    "        xf_end = np.where(xf > right_fraction * xf_max)[0][0]+1\n",
    "           \n",
    "    # print(f'x in {xh_start}:{xh_end} (from coast to 80% of domain, ocean excluded)')\n",
    "    xhSubset = (xh_start,xh_end+1)\n",
    "    xfSubset = (xf_start,xf_end+1)\n",
    "    return xhSubset, xfSubset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "7f14fb21-a5b9-4737-a448-fc5f23e4e6ef",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#DOMAIN SUBSETTING\n",
    "def DOMAIN_SUBSET(out_arr,index_adjust, X):\n",
    "    print(f'length before: {len(out_arr)}')\n",
    "\n",
    "    tSubset = GetTimeSubset(noSubset=False) #used in recombining code at the bottom\n",
    "    xSubset = GetXSubset(noSubset=False)\n",
    "    #SUBSETTING CODE\n",
    "    \n",
    "    ################\n",
    "    ts,ps=out_arr[:,1],out_arr[:,0]\n",
    "\n",
    "    #GETTING X VALUES OF EACH PARCEL \n",
    "    xs=X[ts,ps-index_adjust]\n",
    "\n",
    "    #GETTING SUBSET CONDITIONS\n",
    "    cond1=(xs>=xSubset[0][0])&(xs<=xSubset[0][1])\n",
    "    cond2=(out_arr[:,1]>=tSubset[0])&(out_arr[:,1]<=tSubset[-1])\n",
    "    combined_conds=cond1&cond2\n",
    "\n",
    "    #SUBSETTING\n",
    "    where=np.where(combined_conds)\n",
    "    out_arr=out_arr[where]\n",
    "\n",
    "    print(f'==> length after: {len(out_arr)}'+'\\n')\n",
    "    return out_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "923dbd13-76cb-4e71-9949-016b528a6ffb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_file():\n",
    "    Dictionary = Results_InputOutput_Class.LoadOutFile(ModelData, DataManager, job_id=\"combined\")\n",
    "    out_arr = Dictionary['out_arr']\n",
    "    save_arr = Dictionary['save_arr']\n",
    "    save2_arr = Dictionary['save2_arr']\n",
    "    return out_arr,save_arr,save2_arr\n",
    "\n",
    "def GetALLArrays_CL(start_job,end_job,index_adjust, X, subset=True):\n",
    "    #loading back in\n",
    "    [out_arr,save_arr,save2_arr]=load_file()\n",
    "    \n",
    "    # print(np.where(np.all(out_arr==0,axis=1))) #testing\n",
    "    # print(f'there are a total of {len(out_arr)} CL parcels and {len(save_arr)} nonCL parcels'+'\\n')\n",
    "    \n",
    "    #applying job_array to parcel number\n",
    "    ####################################\n",
    "    print('Applying Job Array')\n",
    "    out_arr=SlurmJobArray_Class.job_filter(out_arr, start_job,end_job)\n",
    "    save_arr=SlurmJobArray_Class.job_filter(save_arr, start_job,end_job)\n",
    "\n",
    "    # print(np.where(np.all(out_arr==0,axis=1))) #testing\n",
    "    # print(f'there are a total of {len(out_arr)} CL parcels and {len(save_arr)} nonCL parcels'+'\\n')\n",
    "    \n",
    "    #CHOOSING UNIQUE INDEXES (just in case)\n",
    "    ###############################################################################\n",
    "    def remove_duplicates(arr):\n",
    "        lst = []\n",
    "        unique_values, counts = np.unique(arr[:, 0], return_counts=True)\n",
    "        duplicates = unique_values[counts > 1]\n",
    "        for elem in duplicates:\n",
    "            idx = np.where(arr[:, 0] == elem)[0]\n",
    "            extras = idx[np.where(arr[idx, 1] != np.min(arr[idx, 1]))]\n",
    "            lst.extend(extras)\n",
    "        mask = np.ones(len(arr), dtype=bool)\n",
    "        mask[lst] = False\n",
    "        return arr[mask]\n",
    "    out_arr=remove_duplicates(out_arr)\n",
    "    save_arr=remove_duplicates(save_arr)\n",
    "    ###############################################################################\n",
    "    # print(np.where(np.all(out_arr==0,axis=1))) #TESTING\n",
    "    \n",
    "    ############################################################ \n",
    "    #SUBSETTING\n",
    "    if subset==True:\n",
    "        out_arr_subset=DOMAIN_SUBSET(out_arr,index_adjust, X)\n",
    "        save_arr_subset=DOMAIN_SUBSET(save_arr,index_adjust, X)\n",
    "    ############################################################\n",
    "    # print(np.where(np.all(out_arr==0,axis=1))) #TESTING\n",
    "    \n",
    "    ALL_out_arr=out_arr_subset.copy(); ALL_save_arr=save_arr_subset.copy()\n",
    "    return ALL_out_arr,ALL_save_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f01f8db6-703e-4190-91cf-e261ec79e342",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #search for deep convective parcels within lagrangian tracking output     \n",
    "# ##############################################################\n",
    "# def ThresholdFilter_local_z_maxima(out_arr,parcel_z, zthresh,index_adjust,mode=\"SHALLOW\"):\n",
    "    \n",
    "#     out_ind=[];\n",
    "#     for ind in range(len(out_arr)): \n",
    "#         # if np.mod(ind,5000)==0: print(f'{ind}/{len(out_arr)}')\n",
    "#         #CHECK TO SSEE IF NEXT MOST LOCAL TIME MAX GOES ABOVE ZTHRESHS \n",
    "\n",
    "#         #Get Ascending Range Past LFC For Maximum 120 Minutes Simulation Time\n",
    "#         nummins=120; numsteps=int(nummins/times[1])\n",
    "#         aboverange=np.arange(out_arr[ind,2],out_arr[ind,2]+numsteps,1) #range of times between current time and numsteps later\n",
    "#         aboverange=aboverange[aboverange<ModelData.Ntime] #caps out at max time\n",
    "#         above=parcel_z[aboverange,out_arr[ind,0]-index_adjust]/1000\n",
    "\n",
    "#         #CALCULUS LOCAL MAX ALGORITHM\n",
    "#         #Takes The time derivative \n",
    "#         ddx=ddt(above)\n",
    "\n",
    "#         #Checks whether the Local Time Max Is Located Above zthresh\n",
    "#         signs = np.sign(ddx)\n",
    "#         signs_diff=np.diff(signs)\n",
    "#         local_maxes=np.where((signs_diff != 0) & (signs_diff < 0))[0]+1 #make sure +1 is here\n",
    "#         if len(local_maxes)==0:\n",
    "#             local_maxes=[0]\n",
    "#         max_z = above[local_maxes[0]] #maximum z of parcel\n",
    "\n",
    "#         #Apply threshold condition based on mode (in SHALLOW or DEEP)\n",
    "#         if mode.lower() == \"deep\":\n",
    "#             condition = np.any(max_z >= zthresh)\n",
    "#         elif mode.lower() == \"shallow\":\n",
    "#             condition = np.any(max_z <= zthresh)\n",
    "#         else:\n",
    "#             raise ValueError(f\"Invalid mode '{mode}'. Use 'deep' or 'shallow'.\")\n",
    "\n",
    "#         if condition:\n",
    "#             out_ind.append(ind)\n",
    "\n",
    "#     #SUBSET OUT FOR FINAL RESULT\n",
    "#     out_arr=out_arr[out_ind,:]\n",
    "#     # print(f'> {zthresh} km. {len(out_arr)} leftover parcels')\n",
    "#     return out_arr\n",
    "\n",
    "# def ddt(f,dt=1):\n",
    "#     ddx = (\n",
    "#             f[1:  ]\n",
    "#             -\n",
    "#             f[0:-1]\n",
    "#         ) / (\n",
    "#         2 * dt\n",
    "#     )\n",
    "#     return ddt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8de9378e-d1c2-4d05-84c0-83a979563c10",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#search for deep convective parcels within lagrangian tracking output     \n",
    "##############################################################\n",
    "def ThresholdFilter(out_arr, QCQI,parcel_z,index_adjust,mode=\"SHALLOW\"):\n",
    "    \n",
    "    out_ind=[];\n",
    "    for ind in range(len(out_arr)): \n",
    "        # if np.mod(ind,5000)==0: print(f'{ind}/{len(out_arr)}')\n",
    "        #CHECK TO SSEE IF NEXT MOST LOCAL TIME MAX GOES ABOVE ZTHRESHS \n",
    "\n",
    "        #Get Ascending Range Past LFC For Maximum 120 Minutes Simulation Time\n",
    "        nummins=120; numsteps=int(nummins/times[1])\n",
    "        aboverange=np.arange(out_arr[ind,2],out_arr[ind,2]+numsteps,1) #range of times between current time and numsteps later\n",
    "        aboverange=aboverange[aboverange<ModelData.Ntime] #caps out at max time\n",
    "\n",
    "        #Getting qcqi Trajectory\n",
    "        z_series=parcel_z[aboverange,out_arr[ind,0]-index_adjust]/1000\n",
    "        qcqi_series = QCQI[aboverange, out_arr[ind,0] - index_adjust]\n",
    "        \n",
    "        #FINDING WHERE qc+qi IS FIRST < 1e-6\n",
    "        where_CT = np.where(qcqi_series < 1e-6)[0]\n",
    "\n",
    "        if len(where_CT) > 0:\n",
    "            first_idx = where_CT[0]\n",
    "            z_CT = z_series[first_idx]\n",
    "        else:\n",
    "            z_CT = np.nan\n",
    "\n",
    "        # Apply threshold condition\n",
    "        if mode.lower() == \"deep\":\n",
    "            zthresh=6 #km\n",
    "            condition = (z_CT >= zthresh)\n",
    "        elif mode.lower() == \"shallow\":\n",
    "            zthresh=4 #km\n",
    "            condition = (z_CT <= zthresh)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid mode '{mode}'. Use 'deep' or 'shallow'.\")\n",
    "\n",
    "        if condition:\n",
    "            out_ind.append(ind)\n",
    "\n",
    "    #SUBSET OUT FOR FINAL RESULT\n",
    "    out_arr=out_arr[out_ind,:]\n",
    "    # print(f'> {zthresh} km. {len(out_arr)} leftover parcels')\n",
    "    return out_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "264b5df8-9d7f-4fed-bfad-2d13bb896a83",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#CL and NONCL\n",
    "\n",
    "#SHALLOW\n",
    "def GetSHALLOWArrays_CL(ALL_out_arr,ALL_save_arr,QCQI,parcel_z,index_adjust):\n",
    "    SHALLOW_out_arr=ThresholdFilter(ALL_out_arr,QCQI,parcel_z,index_adjust,mode='SHALLOW') #nonCL\n",
    "    SHALLOW_save_arr=ThresholdFilter(ALL_save_arr,QCQI,parcel_z,index_adjust,mode='SHALLOW') #nonCL\n",
    "    return SHALLOW_out_arr,SHALLOW_save_arr\n",
    "\n",
    "#DEEP\n",
    "def GetDEEPArrays_CL(ALL_out_arr,ALL_save_arr,QCQI,parcel_z,index_adjust):    \n",
    "    DEEP_out_arr=ThresholdFilter(ALL_out_arr,QCQI,parcel_z,index_adjust,mode='DEEP') #CL\n",
    "    DEEP_save_arr=ThresholdFilter(ALL_save_arr,QCQI,parcel_z,index_adjust,mode='DEEP') #nonCL\n",
    "    return DEEP_out_arr,DEEP_save_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "06cdb2f1-723b-4fbc-ae1e-a0340b1ed47d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# SBF and nonSBF\n",
    "\n",
    "#getting convergence xmax\n",
    "def Get_AvgConvergence(t):\n",
    "\n",
    "    timeString = ModelData.timeStrings[t]\n",
    "    outputDataDirectory=os.path.normpath(os.path.join(DataManager.outputDataDirectory,\"..\",\"Eulerian_CLTracking\"))\n",
    "    Dictionary = TrackingAlgorithms_DataLoading_Class.LoadData(ModelData, DataManager, timeString,\n",
    "                     dataName=\"Eulerian_CLTracking\",outputDataDirectory=outputDataDirectory,printstatement=False)\n",
    "    avgConvergence = Dictionary[\"avgConvergence\"]\n",
    "    return avgConvergence\n",
    "    \n",
    "def find_SBF_xmaxs():\n",
    "    xmaxs=[]\n",
    "    for t in range(ModelData.Ntime)[1:]:\n",
    "        if t == 0:\n",
    "            avgConvergence_max=np.nan\n",
    "        else:\n",
    "            avgConvergence = Get_AvgConvergence(t)\n",
    "            avgConvergence_max=np.max(avgConvergence)\n",
    "            xmax = np.where(avgConvergence==avgConvergence_max)[0][0]\n",
    "            xmaxs.append(xmax)\n",
    "    return xmaxs\n",
    "xmaxs=find_SBF_xmaxs()\n",
    "\n",
    "#subsetting SBF parcels\n",
    "def subset_SBF(out_arr, X, index_adjust):\n",
    "    SBF_subset=[]\n",
    "    \n",
    "    for ind in np.arange(out_arr.shape[0]):\n",
    "        \n",
    "        row=out_arr[ind]\n",
    "        p=row[0]\n",
    "        t=row[1]\n",
    "\n",
    "        #checked if parcel is initially within 10 km of the SBF\n",
    "        if X[t,p-index_adjust] in np.arange( (xmaxs[t]-10*kms),(xmaxs[t]+10*kms) +1): \n",
    "            SBF_subset.append(ind)\n",
    "    \n",
    "    SBF_out_arr=out_arr[SBF_subset]\n",
    "    print(f'there are a total of {len(SBF_out_arr)} ALL SBF CL parcels')\n",
    "\n",
    "    valid_range=np.arange(out_arr.shape[0])\n",
    "    nonSBF_out_arr=out_arr[list(set(valid_range) - set(SBF_subset))]\n",
    "    print(f'there are a total of {len(nonSBF_out_arr)} ALL nonSBF CL parcels')\n",
    "    return SBF_out_arr,nonSBF_out_arr\n",
    "        \n",
    "def GetArrays_SBF(ALL_out_arr,QCQI,parcel_z, \n",
    "                  X, index_adjust):\n",
    "\n",
    "    #SUBSETTING OUT SHALLOW AND DEEP FROM SBF AND NONSBF\n",
    "    [ALL_SBF_out_arr,ALL_nonSBF_out_arr]=subset_SBF(ALL_out_arr, X, index_adjust)\n",
    "    SHALLOW_SBF_out_arr=ThresholdFilter(ALL_SBF_out_arr,QCQI,parcel_z,index_adjust,mode='SHALLOW')\n",
    "    SHALLOW_nonSBF_out_arr=ThresholdFilter(ALL_nonSBF_out_arr,QCQI,parcel_z,index_adjust,mode='SHALLOW')\n",
    "    DEEP_SBF_out_arr=ThresholdFilter(ALL_SBF_out_arr,QCQI,parcel_z,index_adjust,mode='DEEP')\n",
    "    DEEP_nonSBF_out_arr=ThresholdFilter(ALL_nonSBF_out_arr,QCQI,parcel_z,index_adjust,mode='DEEP')\n",
    "\n",
    "    return ALL_SBF_out_arr,ALL_nonSBF_out_arr,SHALLOW_SBF_out_arr,SHALLOW_nonSBF_out_arr,DEEP_SBF_out_arr,DEEP_nonSBF_out_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "2213a4d8-0641-4b76-9572-a08cf0813bb8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_ColdPool(out_arr1,out_arr2):\n",
    "    arr1 = out_arr1[:,0] #CL\n",
    "    arr2 = out_arr2[:,0] #nonSBF\n",
    "    common_values = np.intersect1d(arr1, arr2)\n",
    "    indices_arr1 = np.where(np.isin(arr1, common_values))[0]  # Indices in arr1\n",
    "    ColdPool_out_arr=out_arr1[indices_arr1]\n",
    "    return ColdPool_out_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "db122386-74d7-41ec-a56a-cb0f0a0d65f1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "counts=[[],[],[],[],[],[]]\n",
    "def AddCounts(counts,ALL_out_arr,SHALLOW_out_arr,DEEP_out_arr,ALL_save_arr,SHALLOW_save_arr,DEEP_save_arr, job_id):\n",
    "    counts[0].append(ALL_out_arr.shape[0])\n",
    "    counts[1].append(SHALLOW_out_arr.shape[0])\n",
    "    counts[2].append(DEEP_out_arr.shape[0])\n",
    "    \n",
    "    counts[3].append(ALL_save_arr.shape[0])\n",
    "    counts[4].append(SHALLOW_save_arr.shape[0])\n",
    "    counts[5].append(DEEP_save_arr.shape[0])\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ab0c63f4-c7a9-45d7-8749-c3e38dff20ea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Additional Columns Functions\n",
    "\n",
    "#after ascent arrays\n",
    "def compute_after_arrays(data_dict, W, QCQI, index_adjust):\n",
    "    def find_after_time(out_arr):\n",
    "        wthresh = 0.5\n",
    "        qcqithresh = 1e-6\n",
    "        after_array = np.zeros(len(out_arr), dtype=int)\n",
    "        for count, out_row in enumerate(out_arr):\n",
    "            p = out_row[0]\n",
    "            t2 = out_row[2]\n",
    "            #find where parcel exits cloudy updraft\n",
    "            after = np.where((W[t2:, p - index_adjust] < wthresh) |\n",
    "                             (QCQI[t2:, p - index_adjust] < qcqithresh))\n",
    "            if len(after[0]) != 0:\n",
    "                after_array[count] = after[0][0]\n",
    "        return after_array\n",
    "\n",
    "    # apply to each dataset\n",
    "    after_dict = {}\n",
    "    for key, arr in data_dict.items():\n",
    "        after_dict[key.replace('_arr', '_after_array')] = find_after_time(arr)\n",
    "        ## if \"SHALLOW\" in key: #old method giving no after_time to shallow parcels (not recommended)\n",
    "        ##     after_dict[key.replace('_arr', '_after_array')] = np.zeros(len(arr), dtype=int) \n",
    "        ## else:\n",
    "        ##     after_dict[key.replace('_arr', '_after_array')] = find_after_time(arr)\n",
    "    return after_dict\n",
    "\n",
    "def AddColumn_AfterAscent(data_dict,after_dict):\n",
    "    for (key1,key2) in zip(data_dict,after_dict):\n",
    "        data_dict[key1][:, 3] = after_dict[key2]\n",
    "    return data_dict\n",
    "\n",
    "#SBF left of right flag arrays\n",
    "def compute_SBFLeftRight(data_dict, X, xmaxs, index_adjust):\n",
    "    def find_SBFLeftRight(out_arr):\n",
    "        SBFLeftRight_array = np.zeros(len(out_arr), dtype=int)\n",
    "        for count, out_row in enumerate(out_arr):\n",
    "            #getting indexes\n",
    "            p = out_row[0]\n",
    "            t1 = out_row[1]\n",
    "    \n",
    "            #get X data\n",
    "            xPosition = X[t1,p - index_adjust]\n",
    "            xmax_T = xmaxs[t1]\n",
    "    \n",
    "            # SBFLeftRight_array[count] = -1 if xPosition < xmax_T else 1\n",
    "            SBFLeftRight_array[count] = -1 if xPosition < xmax_T else (1 if xPosition > xmax_T else 0)\n",
    "        return SBFLeftRight_array\n",
    "    \n",
    "    # apply to each dataset\n",
    "    SBFLeftRight_dict = {}\n",
    "    for key, arr in data_dict.items():\n",
    "        SBFLeftRight_dict[key.replace('_arr', '_SBFLeftRight_array')] = find_SBFLeftRight(arr)\n",
    "    return SBFLeftRight_dict\n",
    "\n",
    "def AddColumn_SBFLeftRight(data_dict,SBFLeftRight_dict):\n",
    "    for (key1, key2) in zip(data_dict, SBFLeftRight_dict):\n",
    "        data_dict[key1][:, 4] = SBFLeftRight_dict[key2]\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "06bda937-2a6d-4bfd-8e91-941d55a3e474",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_mean_cloud_base(out_arr, Z, Y, X, W, QCQI, index_adjust):\n",
    "    #FINDING MEAN CLOUD BASE \n",
    "    zhs=ModelData.zh\n",
    "    w_thresh2=0.5\n",
    "    qcqi_thresh=1e-6\n",
    "    type='all'\n",
    "    \n",
    "    profile_array =np.zeros((len(zhs), 2)) #column 1: var, column 2: counter, column 3: list of zhs\n",
    "    profile_array[:,1]=zhs;\n",
    "    \n",
    "    # cloudbase_lst=[]\n",
    "    after=4 #20 minutes\n",
    "    for row in range(out_arr.shape[0]):\n",
    "        if np.mod(row,3000)==0: print(f'{row}/{out_arr.shape[0]}')\n",
    "        p=out_arr[row,0]\n",
    "        \n",
    "        # ts=np.arange(out_arr[row,4],out_arr[row,5]+1 + after)\n",
    "        ts_end = min(out_arr[row, 2] + 1 + after, ModelData.Ntime) #this takes care of exceeding buffers\n",
    "        ts = np.arange(out_arr[row, 1], ts_end)\n",
    "        \n",
    "        zs=Z[ts,p-index_adjust] #JOBARRAY INDEX_ADJUST\n",
    "        ys=Y[ts,p-index_adjust] #JOBARRAY INDEX_ADJUST\n",
    "        xs=X[ts,p-index_adjust] #JOBARRAY INDEX_ADJUST\n",
    "    \n",
    "        ws=W[ts,p-index_adjust] #JOBARRAY INDEX_ADJUST\n",
    "        qcqis=QCQI[ts,p-index_adjust] #JOBARRAY INDEX_ADJUST\n",
    "        where=np.where((ws>=w_thresh2) & (qcqis>=qcqi_thresh))\n",
    "        profile_array[zs[where],0]+=1\n",
    "    \n",
    "    # all_cloudbase=zhs[np.where(profile_array[:,0]!=0)[0][0]]\n",
    "    nonzero_indices = np.where(profile_array[:, 0] != 0)[0]\n",
    "    if len(nonzero_indices) > 0:\n",
    "        all_cloudbase = zhs[nonzero_indices[0]]\n",
    "    else:\n",
    "        all_cloudbase = np.nan\n",
    "    return all_cloudbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c3446b02-1ec7-404d-8ca8-241469c8917b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_mean_LFC(out_arr, Z, Y, X, LFC, index_adjust):\n",
    "    #FINDING MEAN CLOUD BASE \n",
    "    zhs=ModelData.zh\n",
    "    w_thresh2=0.5\n",
    "    qcqi_thresh=1e-6\n",
    "    type='all'\n",
    "    \n",
    "    lfc_array =np.zeros((1, 2)) #column 1: var, column 2: counter, column 3: list of zhs\n",
    "    Mean_LFC_array = []\n",
    "    \n",
    "    # cloudbase_lst=[]\n",
    "    after=4 #20 minutes\n",
    "    for row in range(out_arr.shape[0]):\n",
    "        if np.mod(row,3000)==0: print(f'{row}/{out_arr.shape[0]}')\n",
    "        p=out_arr[row,0]\n",
    "        \n",
    "        # ts=np.arange(out_arr[row,4],out_arr[row,5]+1 + after)\n",
    "        ts_end = min(out_arr[row, 2] + 1 + after, ModelData.Ntime) #this takes care of exceeding buffers\n",
    "        ts = np.arange(out_arr[row, 1], ts_end)\n",
    "        \n",
    "        zs=Z[ts,p-index_adjust] #JOBARRAY INDEX_ADJUST\n",
    "        ys=Y[ts,p-index_adjust] #JOBARRAY INDEX_ADJUST\n",
    "        xs=X[ts,p-index_adjust] #JOBARRAY INDEX_ADJUST\n",
    "    \n",
    "        lfcs=LFC[ts,p-index_adjust] #JOBARRAY INDEX_ADJUST #*******\n",
    "        lfcs=lfcs[lfcs>0]\n",
    "        lfc_array[0,0]+=np.sum(lfcs);lfc_array[0,1]+=len(lfcs)\n",
    "\n",
    "        Mean_LFC = lfc_array[0,0]/ lfc_array[0,1]\n",
    "        Mean_LFC_array.append(Mean_LFC)\n",
    "    return Mean_LFC_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "74f9472a-64c6-402e-8ae3-fe171756508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "#RUNNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d2cf758c-184f-477f-a3a9-ae0f67649351",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def RunCode(job_id_list):\n",
    "    count_dict = {key: [] for key in [\n",
    "        'CL_ALL_out_arr', 'CL_SHALLOW_out_arr', 'CL_DEEP_out_arr',\n",
    "        'nonCL_ALL_out_arr', 'nonCL_SHALLOW_out_arr', 'nonCL_DEEP_out_arr',\n",
    "    \n",
    "        'SBF_ALL_out_arr', 'nonSBF_ALL_out_arr',\n",
    "        'SBF_SHALLOW_out_arr', 'nonSBF_SHALLOW_out_arr',\n",
    "        'SBF_DEEP_out_arr', 'nonSBF_DEEP_out_arr',\n",
    "    \n",
    "        'ColdPool_ALL_out_arr', 'ColdPool_SHALLOW_out_arr',\n",
    "        'ColdPool_DEEP_out_arr'\n",
    "    ]}\n",
    "    \n",
    "    all_cloudbase=[]\n",
    "    for job_id in job_id_list:\n",
    "        if job_id % 10: print(f\"current job_id: {job_id}\")\n",
    "\n",
    "        #starting jobarray\n",
    "        [start_job, end_job, index_adjust] = SlurmJobArray_Class.StartJobArray(ModelData,job_id,num_jobs)\n",
    "\n",
    "        print(\"getting variables\")\n",
    "        [W, QCQI, Z, Y, X, parcel_z] = GetSpatialData(Lagrangian_Binary_Array_Data, start_job,end_job)\n",
    "        LFC,LCL = GetLFCData(LFC_LCL_Data, start_job,end_job)\n",
    "    \n",
    "        #CL and nonCL\n",
    "        print(\"subsetting CL and nonCL\")\n",
    "        [CL_ALL_out_arr, nonCL_ALL_out_arr] = GetALLArrays_CL(start_job,end_job,index_adjust, X)\n",
    "        [CL_SHALLOW_out_arr, nonCL_SHALLOW_out_arr] = GetSHALLOWArrays_CL(CL_ALL_out_arr, nonCL_ALL_out_arr,QCQI,parcel_z,index_adjust)\n",
    "        [CL_DEEP_out_arr, nonCL_DEEP_out_arr] = GetDEEPArrays_CL(CL_ALL_out_arr, nonCL_ALL_out_arr,QCQI,parcel_z,index_adjust)\n",
    "    \n",
    "        #SBF and nonSBF \n",
    "        print(\"subsetting SBF and nonSBF\")\n",
    "        [SBF_ALL_out_arr, nonSBF_ALL_out_arr,\n",
    "         SBF_SHALLOW_out_arr, nonSBF_SHALLOW_out_arr,\n",
    "         SBF_DEEP_out_arr, nonSBF_DEEP_out_arr] = GetArrays_SBF(CL_ALL_out_arr,QCQI,parcel_z, \n",
    "                                                                X, index_adjust)\n",
    "        \n",
    "        # ColdPool\n",
    "        print(\"subsetting ColdPool\")\n",
    "        ColdPool_ALL_out_arr = get_ColdPool(CL_ALL_out_arr, nonSBF_ALL_out_arr)\n",
    "        ColdPool_SHALLOW_out_arr = get_ColdPool(CL_SHALLOW_out_arr, nonSBF_SHALLOW_out_arr)\n",
    "        ColdPool_DEEP_out_arr = get_ColdPool(CL_DEEP_out_arr, nonSBF_DEEP_out_arr)\n",
    "    \n",
    "        # Create a dictionary of arrays to save (including SBF arrays)\n",
    "        data_dict = {\n",
    "            'CL_ALL_out_arr': CL_ALL_out_arr,\n",
    "            'CL_SHALLOW_out_arr': CL_SHALLOW_out_arr,\n",
    "            'CL_DEEP_out_arr': CL_DEEP_out_arr,\n",
    "            'nonCL_ALL_out_arr': nonCL_ALL_out_arr,\n",
    "            'nonCL_SHALLOW_out_arr': nonCL_SHALLOW_out_arr,\n",
    "            'nonCL_DEEP_out_arr': nonCL_DEEP_out_arr,\n",
    "        \n",
    "            'SBF_ALL_out_arr': SBF_ALL_out_arr, \n",
    "            'nonSBF_ALL_out_arr': nonSBF_ALL_out_arr,\n",
    "            'SBF_SHALLOW_out_arr': SBF_SHALLOW_out_arr,\n",
    "            'nonSBF_SHALLOW_out_arr': nonSBF_SHALLOW_out_arr,\n",
    "            'SBF_DEEP_out_arr': SBF_DEEP_out_arr,\n",
    "            'nonSBF_DEEP_out_arr': nonSBF_DEEP_out_arr,\n",
    "        \n",
    "            'ColdPool_ALL_out_arr': ColdPool_ALL_out_arr,\n",
    "            'ColdPool_SHALLOW_out_arr': ColdPool_SHALLOW_out_arr,\n",
    "            'ColdPool_DEEP_out_arr': ColdPool_DEEP_out_arr\n",
    "        }\n",
    "\n",
    "        print('storing after ascent arrays')\n",
    "        #ADDING ANOTHER COLUMN TO STORE THE AFTER ARRAYS and SBF_left_right\n",
    "        for key, arr in data_dict.items():\n",
    "            new_column = np.zeros((arr.shape[0], 2), dtype=int) #adds two columns\n",
    "            data_dict[key] = np.hstack((arr, new_column))\n",
    "    \n",
    "        # Compute after-arrays\n",
    "        after_dict = compute_after_arrays(data_dict, W, QCQI, index_adjust)\n",
    "        # Adding to Fourth Column\n",
    "        data_dict=AddColumn_AfterAscent(data_dict,after_dict)\n",
    "\n",
    "        # Compute SBF_leftright flags\n",
    "        SBFLeftRight_dict = compute_SBFLeftRight(data_dict, X, xmaxs, index_adjust)\n",
    "        data_dict = AddColumn_SBFLeftRight(data_dict,SBFLeftRight_dict)\n",
    "\n",
    "        #GETTING THE COUNT\n",
    "        print('computing count')\n",
    "        for key in count_dict:\n",
    "            count_dict[key].append(data_dict[key].shape[0])\n",
    "    \n",
    "        #GETTING CLOUDBASE ZLEVEL\n",
    "        print('computing cloudbase')\n",
    "        cloudbase=get_mean_cloud_base(CL_ALL_out_arr, Z, Y, X, W, QCQI, index_adjust)\n",
    "        all_cloudbase.append(cloudbase)\n",
    "        print(all_cloudbase) #*#*\n",
    "    \n",
    "        #GETTING LFC PROFILE\n",
    "        print('computing LFC and LCL')\n",
    "        LFC_profile=get_mean_LFC(CL_ALL_out_arr, Z, Y, X, LFC, index_adjust)\n",
    "        LCL_profile=get_mean_LFC(CL_ALL_out_arr, Z, Y, X, LCL, index_adjust)\n",
    "        \n",
    "        # Call SaveData with the dictionary\n",
    "        print('saving')\n",
    "        Results_InputOutput_Class.SaveAllCloudBase_Job(ModelData,DataManager, all_cloudbase,job_id)\n",
    "        Results_InputOutput_Class.SaveLFC_Profile_Job(ModelData,DataManager, LFC_profile,job_id, Ltype=\"LFC\")\n",
    "        Results_InputOutput_Class.SaveLFC_Profile_Job(ModelData,DataManager, LCL_profile,job_id, Ltype=\"LCL\")\n",
    "        Dictionary = {**data_dict, **after_dict}\n",
    "        Results_InputOutput_Class.SaveOutFile(ModelData,DataManager, Dictionary,f\"{job_id}_SUBSET\")\n",
    "\n",
    "        # if job_id == job_id_list[0]: break #testing\n",
    "        # if job_id == job_id_list[1]: break #testing\n",
    "    combined_counts={key: sum(counts) for key, counts in count_dict.items()}\n",
    "    print(f\"combined_counts = {combined_counts}\")\n",
    "\n",
    "    return Dictionary\n",
    "    # return Dictionary, X, index_adjust #TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894a2aaf-d436-43d7-951f-0a30fb1d9671",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#starting job arrays\n",
    "[start_slurm_job,end_slurm_job]=SlurmJobArray_Class.StartSlurmJobArray(num_jobs=num_jobs,num_slurm_jobs=num_slurm_jobs,ISRUN=True) #if ISRUN is False, then will not run using slurm_job_array\n",
    "print(f\"Running on Slurm_Jobs for Slurm_Job_Ids: {(start_slurm_job,end_slurm_job-1)}\")\n",
    "job_id_list=np.arange(start_slurm_job,end_slurm_job)\n",
    "\n",
    "#running algorithm\n",
    "StartTime = time.time()\n",
    "Dictionary = RunCode(job_id_list)\n",
    "EndTime = time.time(); ElapsedTime = EndTime - StartTime; print(f\"Total Elapsed Time: {ElapsedTime} seconds\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49e37a9-84ac-4644-94aa-d342ff149efb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "4008baac-8f32-4d3d-be88-509797b5b36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################### \n",
    "#RECOMBINING\n",
    "recombine=False #KEEP FALSE WHEN RUNNING\n",
    "recombine=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "70796f21-cc5d-49ba-b938-67d173a0df9e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def ReadData(varName,job_id):\n",
    "    out = Results_InputOutput_Class.LoadOutFile(ModelData,DataManager,f\"{job_id}_SUBSET\",varName = varName)\n",
    "    return out\n",
    "# ReadData(var_name,job_id)\n",
    "def SaveFinalData(ModelData,DataManager, Dictionary):    \n",
    "    Results_InputOutput_Class.SaveOutFile(ModelData,DataManager, Dictionary,job_id=\"combined_SUBSET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "6bc16b17-520c-4f2a-aa36-e56dcc0e4674",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def MakeCount(count_dict):\n",
    "    # print('Getting Tracked Parcel Count')\n",
    "    for job_id in np.arange(1,num_jobs+1):\n",
    "        if job_id % 10==0: print(f\"current job_id: {job_id}\")\n",
    "        \n",
    "        for key in count_dict:\n",
    "            data_dict_key=ReadData(key,job_id)\n",
    "            count_dict[key].append(data_dict_key.shape[0])\n",
    "    combined_counts={key: sum(counts) for key, counts in count_dict.items()}\n",
    "    return combined_counts\n",
    "\n",
    "def GetCombinedCounts():\n",
    "    #GETTING COUNTS FOR MAKING INITIAL RECOMBINED ARRAYS LATER\n",
    "    count_dict = {key: [] for key in [\n",
    "        'CL_ALL_out_arr', 'CL_SHALLOW_out_arr', 'CL_DEEP_out_arr',\n",
    "        'nonCL_ALL_out_arr', 'nonCL_SHALLOW_out_arr', 'nonCL_DEEP_out_arr',\n",
    "    \n",
    "        'SBF_ALL_out_arr', 'nonSBF_ALL_out_arr',\n",
    "        'SBF_SHALLOW_out_arr', 'nonSBF_SHALLOW_out_arr',\n",
    "        'SBF_DEEP_out_arr', 'nonSBF_DEEP_out_arr',\n",
    "    \n",
    "        'ColdPool_ALL_out_arr', 'ColdPool_SHALLOW_out_arr',\n",
    "        'ColdPool_DEEP_out_arr'\n",
    "    ]}\n",
    "\n",
    "    combined_counts=MakeCount(count_dict)\n",
    "    print(combined_counts)\n",
    "    return combined_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "172ce6e2-536a-4bb0-b77d-7ea63c0b2c90",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def MakeEmpty(counts_dict):\n",
    "    empty_dict = {}\n",
    "    for key, count in counts_dict.items():\n",
    "        empty_dict[key] = np.zeros((count, 5), dtype=int)\n",
    "    return empty_dict\n",
    "    \n",
    "def MakeOutputDictionary():\n",
    "    \n",
    "    Dictionary=MakeEmpty(combined_counts)\n",
    "    for job_id in np.arange(1,num_jobs+1):\n",
    "        if job_id % 10==0: print(f\"current job_id: {job_id}\")\n",
    "            \n",
    "        for key in Dictionary:\n",
    "            var=ReadData(key,job_id)\n",
    "            if var.size!=0:\n",
    "                a=np.where(np.all(Dictionary[key] == 0, axis=1))[0][0]\n",
    "                b=a+var.shape[0]\n",
    "                # print(key,a,b) #TESTING\n",
    "                Dictionary[key][a:b]=var\n",
    "    return Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "5c041bff-820d-4cee-97f3-fb3b243980b1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def CombineCloudBase():\n",
    "\n",
    "    #initializing\n",
    "    all_cloudbase=[]\n",
    "\n",
    "    #running\n",
    "    for job_id in np.arange(1,num_jobs+1):\n",
    "        all_cloudbase_job = Results_InputOutput_Class.LoadAllCloudBase_Job(ModelData,DataManager,\n",
    "                     job_id)['all_cloudbase']\n",
    "        all_cloudbase+=list(all_cloudbase_job)\n",
    "    all_cloudbase=np.array(all_cloudbase)\n",
    "\n",
    "    # saving\n",
    "    Results_InputOutput_Class.SaveAllCloudBase_Combined(ModelData,DataManager,\n",
    "                              all_cloudbase)\n",
    "    return all_cloudbase\n",
    "    \n",
    "def CombineLFC_LCL():\n",
    "\n",
    "        #initializing\n",
    "        MeanLFC=[]\n",
    "        MeanLCL=[]\n",
    "\n",
    "        #running\n",
    "        for job_id in np.arange(1,num_jobs+1):\n",
    "            LoadedLFC = Results_InputOutput_Class.LoadLFC_Profile_Job(ModelData,DataManager,\n",
    "                         job_id, Ltype=\"LFC\")[\"LFC_profile\"]\n",
    "            LoadedLCL = Results_InputOutput_Class.LoadLFC_Profile_Job(ModelData,DataManager,\n",
    "                         job_id, Ltype=\"LCL\")[\"LCL_profile\"]\n",
    "            MeanLFC+=list(LoadedLFC)\n",
    "            MeanLCL+=list(LoadedLCL)\n",
    "\n",
    "        MeanLFC = np.array(MeanLFC)/1e3\n",
    "        MeanLCL = np.array(MeanLCL)/1e3\n",
    "\n",
    "        #saving\n",
    "        Results_InputOutput_Class.SaveLFC_Profile_Combined(ModelData,DataManager,\n",
    "                        MeanLFC, Ltype = \"LFC\")\n",
    "        Results_InputOutput_Class.SaveLFC_Profile_Combined(ModelData,DataManager,\n",
    "                        MeanLCL, Ltype = \"LCL\")\n",
    "        return MeanLFC, MeanLCL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc55e99-3946-432b-a02a-cc4dce153fad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Recombining Tracked Parcels\n",
    "if recombine==True:\n",
    "    combined_counts = GetCombinedCounts()\n",
    "    Dictionary = MakeOutputDictionary()\n",
    "    SaveFinalData(ModelData,DataManager, Dictionary)\n",
    "\n",
    "#Recombining LFC and LCL\n",
    "if recombine==True:\n",
    "    all_cloudbase=CombineCloudBase()\n",
    "    print(f\"cloudbase_mean = {np.mean(all_cloudbase)}\")\n",
    "    \n",
    "    MeanLFC_Array, MeanLCL_Array =CombineLFC_LCL()\n",
    "    print(f\"LFCMean = {np.mean(MeanLFC_Array)}\")\n",
    "    print(f\"LCLMean = {np.mean(MeanLCL_Array)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378f8704-32ea-493e-9ef9-223dd98ae69f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d978f129-ef67-4264-87ce-9c1920fcf0a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccddce0c-4f20-40a4-9ee3-a093bcdfed79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c105c157-f0c9-48e4-b290-4ad0db81d4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#READING BACK IN SUBSETTED TRACKED PARCEL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "c044ba30-8034-41b1-a2f1-e2911e3063e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trackedArrays,LevelsDictionary = TrackedParcel_Loading_Class.LoadingSubsetParcelData(ModelData,DataManager,\n",
    "#                                                          Results_InputOutput_Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a68ec4ef-e92d-4588-a2b1-4da13f81cd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d17fc09b-ad33-4802-9601-85c9c703e772",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# dir = \"/mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/Code/CodeFiles/3_Project_Algorithms/2_Tracking_Algorithms/Old_Version/OUTPUT\"\n",
    "# fileName = \"parcel_tracking_SUBSET_1km_5min_1e6.h5\"\n",
    "# filePath = os.path.join(dir,fileName)\n",
    "\n",
    "\n",
    "# with h5py.File(filePath, 'r') as f:\n",
    "#     print(f.keys())\n",
    "#     out = f[\"CL_ALL_out_arr\"][:]\n",
    "\n",
    "\n",
    "# one = trackedArrays[\"CL\"][\"ALL\"] #get trackedArrays from above \"loading back in\" code\n",
    "# two = out\n",
    "\n",
    "# col1 = one[:, 0]\n",
    "# col2 = two[:, 0]\n",
    "\n",
    "# set1 = set(col1)\n",
    "# set2 = set(col2)\n",
    "\n",
    "# # Intersection  values in both\n",
    "# common = set1 & set2\n",
    "\n",
    "# # Only in one or two\n",
    "# only_in_one = set1 - set2\n",
    "# only_in_two = set2 - set1\n",
    "\n",
    "# print(\"Common elements:\", len(common))\n",
    "# print(\" Only in one:\", len(only_in_one))\n",
    "# print(\" Only in two:\", len(only_in_two))\n",
    "\n",
    "# # Optional: show some examples\n",
    "# print(\"\\nExamples only in one:\", list(only_in_one)[:10])\n",
    "# print(\"Examples only in two:\", list(only_in_two)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "33e6c3ae-45ac-46dd-89ab-2176f96ed9b2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #TESTING SBF_LeftRight\n",
    "# print(Dictionary.keys())\n",
    "# arr = Dictionary[\"CL_DEEP_out_arr\"]\n",
    "# ps = arr[:, 0].astype(int) - index_adjust\n",
    "# ts = arr[:, 1].astype(int)\n",
    "# xmaxs_arr = np.array(xmaxs)[ts]\n",
    "\n",
    "# # Compare all elements at once\n",
    "# a = np.where(X[ts, ps] < xmaxs_arr, -1,\n",
    "#              np.where(X[ts, ps] > xmaxs_arr, 1, 0))\n",
    "# print(np.all(arr[:,4] == a))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "work"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
