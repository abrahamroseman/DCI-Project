{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7ab83605-4442-49f4-bdc9-baf18009f3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "#ENVIRONMENT SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2e58693e-90de-4e46-bfb9-f42344e087ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import matplotlib.gridspec as gridspec\n",
    "import xarray as xr\n",
    "import os; import time\n",
    "import pickle\n",
    "import h5py\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "43c64e7f-7e28-49b9-af39-5b9b00b9e4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAIN DIRECTORIES\n",
    "def GetDirectories():\n",
    "    mainDirectory='/mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/'\n",
    "    mainCodeDirectory=os.path.join(mainDirectory,\"Code/CodeFiles/\")\n",
    "    scratchDirectory='/mnt/lustre/koa/scratch/air673/'\n",
    "    codeDirectory=os.getcwd()\n",
    "    return mainDirectory,mainCodeDirectory,scratchDirectory,codeDirectory\n",
    "\n",
    "[mainDirectory,mainCodeDirectory,scratchDirectory,codeDirectory] = GetDirectories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "25eea9d0-8db6-415c-9fa3-0bc3e1214dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "#LOADING CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5c4d6c5e-57f8-4287-8574-8264456175d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT CLASSES (from current directory)\n",
    "sys.path.append(os.path.join(mainCodeDirectory,\"2_Variable_Calculation\"))\n",
    "from CLASSES_Variable_Calculation import ModelData_Class, SlurmJobArray_Class, DataManager_Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d180ceb2-17de-4256-ae17-6c6c64f34295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CM1 Data Summary ===\n",
      " Simulation #:   1\n",
      " Resolution:     1km\n",
      " Time step:      5min\n",
      " Vertical levels:34\n",
      " Parcels:        1e6\n",
      " Data file:      /mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/Model/cm1r20.3/run/cm1out_1km_5min_34nz.nc\n",
      " Parcel file:    /mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/Model/cm1r20.3/run/cm1out_pdata_1km_5min_1e6np.nc\n",
      " Time steps:     133\n",
      "========================= \n",
      "\n",
      "=== DataManager Summary ===\n",
      " inputDirectory #:   /mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/Code/OUTPUT/Variable_Calculation/TimeSplitModelData\n",
      " outputDirectory #:   /mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/Code/OUTPUT/Variable_Calculation/EntrainmentCalculation\n",
      " inputDataDirectory #:   /mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/Code/OUTPUT/Variable_Calculation/TimeSplitModelData/1km_5min_34nz/ModelData\n",
      " inputParcelDirectory #:   /mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/Code/OUTPUT/Variable_Calculation/TimeSplitModelData/1km_5min_34nz/ParcelData\n",
      " outputDataDirectory #:   /mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/Code/OUTPUT/Variable_Calculation/EntrainmentCalculation/1km_5min_34nz/Entrainment_Preprocessing\n",
      "========================= \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#data loading class\n",
    "ModelData = ModelData_Class(mainDirectory, scratchDirectory, simulationNumber=1)\n",
    "#data manager class\n",
    "DataManager = DataManager_Class(mainDirectory, scratchDirectory, ModelData.res, ModelData.t_res, ModelData.Nz_str,\n",
    "                                ModelData.Np_str, dataType=\"EntrainmentCalculation\", dataName=\"Entrainment_Preprocessing\",\n",
    "                                dtype='int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9dd576c8-4c88-4723-9bcd-a6fb46a5f841",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT CLASSES\n",
    "sys.path.append(os.path.join(mainCodeDirectory,\"3_Project_Algorithms\",\"2_Tracking_Algorithms\"))\n",
    "from CLASSES_TrackingAlgorithms import SlurmJobArray_Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c52513f6-ee19-499c-9286-85ea96bed804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path=os.path.join(mainCodeDirectory,'Functions/')\n",
    "sys.path.append(path)\n",
    "\n",
    "import NumericalFunctions\n",
    "from NumericalFunctions import * # import NumericalFunctions \n",
    "import PlottingFunctions\n",
    "from PlottingFunctions import * # import PlottingFunctions\n",
    "\n",
    "\n",
    "# # Get all functions in NumericalFunctions\n",
    "# import inspect\n",
    "# functions = [f[0] for f in inspect.getmembers(NumericalFunctions, inspect.isfunction)]\n",
    "# functions\n",
    "\n",
    "# # Get all functions in NumericalFunctions\n",
    "# import inspect\n",
    "# functions = [f[0] for f in inspect.getmembers(PlottingFunctions, inspect.isfunction)]\n",
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ea4cd6ce-d9ad-45b1-965e-f1c9a23cf924",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "#SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1f2fc8ab-3213-4fc4-ad2f-9b3542c236fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "#JOB ARRAY SETUP\n",
    "################################\n",
    "#*#*\n",
    "# how many total jobs are being run? i.e. array=1-100 ==> num_jobs=100\n",
    "if ModelData.Np_str=='1e6': #1M parcels\n",
    "    num_jobs=60  \n",
    "    num_slurm_jobs=20 #this is the number that goes into sbatch script\n",
    "elif ModelData.Np_str=='50e6': #50M parcels\n",
    "    num_jobs=1200 \n",
    "    num_slurm_jobs=150 #this is the number that goes into sbatch script\n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "17544be7-9a5b-4431-ab34-c0f781f67ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "#DATA LOADING FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0dd4360b-5993-4c36-87bf-4e5d08d07c40",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#INITIALIZE DATA FUNCTION\n",
    "###############################################################\n",
    "def InitiateArray(out_file, vars, t_chunk_size, p_chunk_size, t_size=None, p_size=None):\n",
    "    if t_size is None:\n",
    "        t_size = ModelData.Ntime  # Number of timesteps\n",
    "    if p_size is None:\n",
    "        p_size = ModelData.Np  # Number of parcel indexes\n",
    "\n",
    "    with h5py.File(out_file, 'w') as f:\n",
    "        for var_name in vars:\n",
    "            if var_name not in f:\n",
    "                # Set dtype conditionally\n",
    "                if var_name in ['Z', 'Y', 'X']:\n",
    "                    dtype = np.uint16\n",
    "                elif var_name in ['A_g','A_c','PROCESSED_A_g','PROCESSED_A_c']:\n",
    "                    dtype = np.bool_\n",
    "                else:\n",
    "                    dtype = np.float32  # or whatever your default is\n",
    "\n",
    "                f.create_dataset(\n",
    "                    var_name,\n",
    "                    shape=(t_size, p_size),\n",
    "                    chunks=(t_chunk_size, p_chunk_size),\n",
    "                    dtype=dtype\n",
    "                )\n",
    "def InitiateArray_Job(Np,out_file, vars, t_chunk_size, p_chunk_size, t_size=None, p_size=None):\n",
    "    if t_size is None:\n",
    "        t_size = ModelData.Ntime  # Number of timesteps\n",
    "    if p_size is None:\n",
    "        p_size = Np # Number of vertical levels\n",
    "\n",
    "    with h5py.File(out_file, 'w') as f:\n",
    "        for var_name in vars:\n",
    "            if var_name not in f:\n",
    "                # Set dtype conditionally\n",
    "                if var_name in ['Z', 'Y', 'X']:\n",
    "                    dtype = np.uint16\n",
    "                elif var_name in ['A_g','A_c','PROCESSED_A_g','PROCESSED_A_c']:\n",
    "                    dtype = np.bool_\n",
    "                else:\n",
    "                    dtype = np.float32  # or whatever your default is\n",
    "\n",
    "                f.create_dataset(\n",
    "                    var_name,\n",
    "                    shape=(t_size, p_size),\n",
    "                    chunks=(t_chunk_size, p_chunk_size),\n",
    "                    dtype=dtype\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f86d54-6692-492a-b0a2-44567d79a47f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MULTIPLE FILES\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from glob import glob\n",
    "\n",
    "def TestingTimes(files):\n",
    "    print(files)\n",
    "    for f in files:\n",
    "        m = re.search(r'_(\\d+-\\d+-\\d+)\\.h5$', f)\n",
    "        if m:\n",
    "            print(m.group(1))\n",
    "def OpenMultipleSingleTimes_LagrangianArray(directory, ModelData, pattern=\"Lagrangian_Binary_Array_*.h5\"):\n",
    "    \"\"\"\n",
    "    Load a sequence of Lagrangian .h5 files (each a single timestep)\n",
    "    into one xarray.Dataset with dimensions (time, p),\n",
    "    enforcing time order from ModelData.timeStrings.\n",
    "    \"\"\"\n",
    "    # --- Find all available files\n",
    "    files_all = glob(os.path.join(directory, pattern))\n",
    "    if not files_all:\n",
    "        raise FileNotFoundError(f\"No files found in {directory} matching {pattern}\")\n",
    "\n",
    "    # --- Build the correctly ordered list according to ModelData.timeStrings\n",
    "    files = []\n",
    "    for t in ModelData.timeStrings:\n",
    "        time_pattern = f\"_{t}.h5\"\n",
    "        matched = [f for f in files_all if f.endswith(time_pattern)]\n",
    "        if matched:\n",
    "            files.append(matched[0])\n",
    "        else:\n",
    "            print(f\"Missing file for time {t}\")\n",
    "\n",
    "    #####\n",
    "    # TestingTimes(files) \n",
    "    #####\n",
    "    \n",
    "    # --- Convert ModelData.timeStrings (like ['0-00-00', '0-05-00', ...]) to pandas datetime\n",
    "    #     using an arbitrary date\n",
    "    times = pd.to_datetime([t.replace('-', ':') for t in ModelData.timeStrings], format=\"%H:%M:%S\")\n",
    "\n",
    "    # --- Open and concatenate along time\n",
    "    ds = xr.open_mfdataset(\n",
    "        files,\n",
    "        engine=\"h5netcdf\",\n",
    "        phony_dims=\"sort\",\n",
    "        combine=\"nested\",\n",
    "        concat_dim=\"time\",\n",
    "    )\n",
    "\n",
    "    # --- Rename the phony dimension to 'p'\n",
    "    if \"phony_dim_0\" in ds.dims:\n",
    "        ds = ds.rename({\"phony_dim_0\": \"p\"})\n",
    "\n",
    "    # --- Assign your correct time coordinate\n",
    "    ds = ds.assign_coords(time=times)\n",
    "\n",
    "    return ds, files\n",
    "\n",
    "\n",
    "directory = f\"/mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/Code/OUTPUT/Variable_Calculation/LagrangianArrays/{ModelData.res}_{ModelData.t_res}_{ModelData.Nz_str}nz/Lagrangian_Binary_Array/\"\n",
    "\n",
    "print(\"Loading Combined Lagrangian_Binary_Array\")\n",
    "Lagrangian_Binary_Array,files = OpenMultipleSingleTimes_LagrangianArray(directory, ModelData)\n",
    "print(\"Done\",\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2376c6a0-87b9-4b10-a025-440f6f2daab6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Reading Back Data Later\n",
    "##############\n",
    "def make_data_dict(var_names,start_job,end_job):\n",
    "\n",
    "    data_dict = {var_name: Lagrangian_Binary_Array[var_name].isel(p=slice(start_job, end_job)).data.compute()\n",
    "                 for var_name in var_names}\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f7f6be76-90b3-4a05-94af-b163cedc638f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def GetArrays(start_job,end_job):    \n",
    "    var_names = ['A_g', 'A_c']\n",
    "    data_dict = make_data_dict(var_names,start_job,end_job)\n",
    "    A_g, A_c = (data_dict[k] for k in var_names)\n",
    "    return A_g, A_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3d4247af-e710-4460-b363-11ea09477d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "#FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "279c30c2-0351-4a31-a8a4-98cced1b8f32",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def extend_idxs(f,case):\n",
    "    out=np.sort(np.add.outer(f, np.arange(case)).ravel())\n",
    "\n",
    "    # #OLD METHOD (SLOW)\n",
    "    # if np.any(f)==True:\n",
    "    #     out=np.sort(np.concatenate([np.arange(idx, idx + case-1+1) for idx in f]))\n",
    "    # else: \n",
    "    #     out=f\n",
    "    return out\n",
    "\n",
    "def find_sandwiched_patterns(changes, case):\n",
    "    arr=changes\n",
    "    \n",
    "    window_size = case + 1  # e.g., for case=2, window_size = 3\n",
    "    # The interior zeros count is (window_size - 2) which is case - 1\n",
    "    pattern1 = np.array([-1] + [0]*(case - 1) + [1])\n",
    "    pattern2 = np.array([1] + [0]*(case - 1) + [-1])\n",
    "    # print(pattern1,pattern2)\n",
    "    \n",
    "    # Manually construct sliding windows\n",
    "    windows = np.array([arr[i:i + window_size] for i in range(len(arr) - window_size + 1)])\n",
    "    # print(\"Sliding windows:\\n\", windows) #TESTING\n",
    "    \n",
    "    #THE ALGORITHM\n",
    "    turb_d=[]\n",
    "    turb_e=[]\n",
    "    count=0;max_iter=ModelData.Ntime;\n",
    "    while np.any(((windows == pattern1) | (windows == pattern2)).all(axis=1)):\n",
    "        count+=1; \n",
    "        if count>=max_iter: \n",
    "            print(count)\n",
    "            break\n",
    "        \n",
    "        next_ind = np.where(((windows == pattern1) | (windows == pattern2)).all(axis=1))[0][0]\n",
    "        \n",
    "        if (windows[next_ind] == pattern1).all():\n",
    "            turb_d.append(next_ind)\n",
    "        elif (windows[next_ind] == pattern2).all(): \n",
    "            turb_e.append(next_ind) #append to list\n",
    "    \n",
    "        windows[0:next_ind+(case)+1,:] = 0 #removes from windows\n",
    "    \n",
    "    turb_d=np.array(turb_d,dtype=int); turb_e=np.array(turb_e,dtype=int)\n",
    "\n",
    "    #EXTEND REST OF INDEXES TO PROCESS\n",
    "    turb_d=extend_idxs(turb_d,case=case)\n",
    "    turb_e=extend_idxs(turb_e,case=case)\n",
    "    return turb_d,turb_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "106dbc65-0e40-4a4b-b0ff-01497659bea4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # TESTING\n",
    "# changes = np.array([0,0,0,-1,1,0,0,-1,0,0,0,1,-1,0,0])\n",
    "# [a,b] = find_sandwiched_patterns(changes, case=1) #<=1 in a row timesteps are removed\n",
    "# print(\"Case matches at indices:\", a,b)\n",
    "\n",
    "# changes = np.array([0,0,0,-1,0,1,0,0,-1,0,0,1,0,-1,0,0])\n",
    "# [a,b] = find_sandwiched_patterns(changes, case=2) #<=2 in a row timesteps are removed\n",
    "# print(\"Case matches at indices:\", a,b)\n",
    "\n",
    "# changes = np.array([0,0,0,-1,0,0,1,0,0,0,0,1,0,0,-1,0,0])\n",
    "# [a,b] = find_sandwiched_patterns(changes, case=3) #<=3 in a row timesteps are removed\n",
    "# print(\"Case matches at indices:\", a,b)\n",
    "\n",
    "# changes = np.array([0,0,0,-1,0,0,0,1,0,0,0,0,1,0,0,-1,0,0])\n",
    "# [a,b] = find_sandwiched_patterns(changes, case=4) #<=4 in a row timesteps are removed\n",
    "# print(\"Case matches at indices:\", a,b)\n",
    "\n",
    "# changes = np.array([0,0,0,-1,0,0,0,0,1,0,0,0,0,1,0,0,-1,0,0])\n",
    "# [a,b] = find_sandwiched_patterns(changes, case=5) #<=5 in a row timesteps are removed\n",
    "# print(\"Case matches at indices:\", a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2f56dd61-519d-454f-a230-1a592b3f789b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "###### (amount of time inside/outside of cloud to count as entrainment/detrainment)\n",
    "mins_thresh=5 #5 mins\n",
    "######\n",
    "\n",
    "t_per_mins=1/((ModelData.time[1]-ModelData.time[0])/1e9/60).item() #timesteps per minute (<=1)\n",
    "def get_changes(B):\n",
    "    changes = np.diff(np.concatenate(([B[0]], B)))  # Add 0s to detect edges\n",
    "    return changes\n",
    "def PreProcessing(A,p):\n",
    "    B = A[:,p]*1\n",
    "\n",
    "    # Find the changes in the array\n",
    "    changes=get_changes(B)\n",
    "    # print(f'B = {B}'); print(f'changes = {changes}') \n",
    "\n",
    "    #Determining the Case Number\n",
    "    case=int(t_per_mins*mins_thresh)\n",
    "    \n",
    "    if case>1:\n",
    "        for case_ind in np.arange(case,0,-1): \n",
    "        # for case_ind in [case]:\n",
    "            #Calling Algorithm and Correcting Parcel Data\n",
    "            [turb_d,turb_e]=find_sandwiched_patterns(changes, case=case_ind)\n",
    "            B[turb_d]=1\n",
    "            B[turb_e]=0     \n",
    "            changes=get_changes(B)\n",
    "            # print(B)\n",
    "    elif case==1:\n",
    "        #Calling Algorithm and Correcting Parcel Data\n",
    "        [turb_d,turb_e]=find_sandwiched_patterns(changes, case=case)\n",
    "        B[turb_d]=1\n",
    "        B[turb_e]=0\n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "21cd316f-4f77-4980-9b6a-14d2c9cc2e2c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #TESTING #CASE COUNTDOWN\n",
    "# #shows the removal process (same for cases less than applicable case)\n",
    "# B=np.array([1,1,1,1,0,0,0,1,1,1,1,0,0,0,1,0,0,1,1,1,0,0,0,1])\n",
    "#                     #,#,#,        #,#,#       #,#,#\n",
    "# print(B)\n",
    "\n",
    "# #APPLYING\n",
    "# changes=get_changes(B)\n",
    "# [turb_d,turb_e]=find_sandwiched_patterns(changes,case=3)\n",
    "# print(turb_d,turb_e)\n",
    "\n",
    "# B[turb_d]=1\n",
    "# B[turb_e]=0\n",
    "# print(B)\n",
    "\n",
    "# #APPLYING\n",
    "# changes=get_changes(B)\n",
    "# [turb_d,turb_e]=find_sandwiched_patterns(changes,case=2)\n",
    "# print(turb_d,turb_e)\n",
    "\n",
    "# B[turb_d]=1\n",
    "# B[turb_e]=0\n",
    "# print(B)\n",
    "\n",
    "# #APPLYING\n",
    "# changes=get_changes(B)\n",
    "# [turb_d,turb_e]=find_sandwiched_patterns(changes,case=1)\n",
    "# print(turb_d,turb_e)\n",
    "\n",
    "# B[turb_d]=1\n",
    "# B[turb_e]=0\n",
    "# print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "fe7d17ea-0807-4abb-bb7a-75014d2e6739",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUNNING\n",
    "def Apply(A_g,A_c, Np):\n",
    "    PROCESSED_A_g = A_g.copy()\n",
    "    PROCESSED_A_c = A_c.copy()\n",
    "    for p in np.arange(Np):\n",
    "        if np.mod(p,1e3)==0: print(f\"{p}/{Np}\")\n",
    "        out1=PreProcessing(PROCESSED_A_g,p); PROCESSED_A_g[:,p]=out1\n",
    "        out2=PreProcessing(PROCESSED_A_c,p); PROCESSED_A_c[:,p]=out2\n",
    "    return PROCESSED_A_g,PROCESSED_A_c\n",
    "\n",
    "def CheckDifferences(A_g,A_c, PROCESSED_A_g,PROCESSED_A_c):\n",
    "    diff_g = np.sum(A_g != PROCESSED_A_g)\n",
    "    diff_c = np.sum(A_c != PROCESSED_A_c)\n",
    "    print(f\"Differences: A_g={diff_g}, A_c={diff_c}\")\n",
    "\n",
    "#SAVING\n",
    "def Save(Np,PROCESSED_A_g,PROCESSED_A_c,job_id):\n",
    "    out_file=DataManager.outputDataDirectory+f'/processed_binary_arrays_{ModelData.res}_{ModelData.t_res}_{ModelData.Np_str}_{job_id}.h5'\n",
    "    print(f\"Saving as {out_file}\",\"\\n\")\n",
    "    \n",
    "    vars=['PROCESSED_A_g','PROCESSED_A_c']\n",
    "    InitiateArray_Job(Np,out_file,vars,t_chunk_size=50,p_chunk_size=1000)\n",
    "    \n",
    "    with h5py.File(out_file, 'a') as f: \n",
    "        f['PROCESSED_A_g'][:]=PROCESSED_A_g\n",
    "        f['PROCESSED_A_c'][:]=PROCESSED_A_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e9fada64-99ef-4cb1-9de0-da3e990cbbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "#RUNNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7e4774-d1fa-40b1-878b-c1a9878375ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "########################################\n",
    "#RUNNING\n",
    "[start_slurm_job,end_slurm_job]=SlurmJobArray_Class.StartSlurmJobArray(num_jobs=num_jobs,num_slurm_jobs=num_slurm_jobs,ISRUN=True) #if ISRUN is False, then will not run using slurm_job_array\n",
    "\n",
    "print(f\"Running on Slurm_Jobs for Slurm_Job_Ids: {(start_slurm_job,end_slurm_job-1)}\")\n",
    "\n",
    "job_id_list=np.arange(start_slurm_job,end_slurm_job)\n",
    "for job_id in job_id_list:\n",
    "    if job_id % 1 ==0: print(f\"current job_id = {job_id}\\n\")\n",
    "    [start_job,end_job,index_adjust]=SlurmJobArray_Class.StartJobArray(ModelData, job_id, num_jobs)\n",
    "    print(f'Running on Parcels {start_job}-{end_job}')\n",
    "    Np=end_job-start_job\n",
    "    [A_g,A_c]=GetArrays(start_job,end_job)\n",
    "    [PROCESSED_A_g,PROCESSED_A_c]=Apply(A_g,A_c, Np)\n",
    "    CheckDifferences(A_g,A_c, PROCESSED_A_g,PROCESSED_A_c)\n",
    "    Save(Np,PROCESSED_A_g,PROCESSED_A_c,job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3b7f87ec-d5b8-48b8-8a46-591df5982b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMBINING JOB_ARRAYS AFTER RUNNING\n",
    "########################################################################\n",
    "recombine=False #KEEP FALSE WHEN JOB ARRAY IS RUNNING\n",
    "recombine=True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e08bfbca-e2a0-4ade-a64d-32a4887855cf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def Recombine(num_jobs):\n",
    "#     dir2=dir+'Project_Algorithms/Entrainment/Processing_Out/'\n",
    "#     dir3=dir+'Project_Algorithms/Entrainment/OUTPUT/'\n",
    "#     out_file=dir3+f'processed_binary_arrays_{res}_{t_res}_{Np_str}.h5'\n",
    "    \n",
    "#     vars=['PROCESSED_A_g','PROCESSED_A_c']\n",
    "#     InitiateArray(out_file,vars,t_chunk_size=50,p_chunk_size=100_000)\n",
    "    \n",
    "#     with h5py.File(out_file, 'r+') as f_out:\n",
    "        \n",
    "#         for job_id in np.arange(1,num_jobs+1):\n",
    "#             if np.mod(job_id,5)==0: print(f\"job_id = {job_id}\")\n",
    "#             [a,b,_] = StartJobArray(num_jobs,job_id)\n",
    "        \n",
    "#             in_file=DataManager.outputDataDirectory+f'processed_binary_arrays_{res}_{t_res}_{Np_str}_{job_id}.h5'\n",
    "    \n",
    "#             with h5py.File(in_file, 'r') as f_in: \n",
    "#                 for var in vars:\n",
    "#                     f_out[var][:,a:b]=f_in[var][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "beda1d51-af31-41d6-934a-c15f4034746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recombine_Dask():\n",
    "    #DASK-ENABLED\n",
    "    import glob\n",
    "    import re\n",
    "    import dask\n",
    "    from dask.diagnostics import ProgressBar\n",
    "    \n",
    "    def recombine_func(in_files,out_file):\n",
    "        # matching_files = sorted(glob.glob(in_files))\n",
    "        matching_files = sorted(\n",
    "        glob.glob(in_files),\n",
    "        key=lambda f: int(re.search(r'_(\\d+)\\.h5$', f).group(1))\n",
    "    )\n",
    "        \n",
    "        print(f'recombining {len(matching_files)} files')\n",
    "        print('Forming Dask Delayed Object')\n",
    "        # out=xr.open_mfdataset(matching_files,engine='h5netcdf',concat_dim='phony_dim_1',combine='nested',phony_dims='sort')\n",
    "        if ModelData.Np_str == '1e6':\n",
    "            dim_1_length = 100_000\n",
    "        elif ModelData.Np_str == '50e6':\n",
    "            dim_1_length = 500_000\n",
    "        chunks={'phony_dim_0': -1, 'phony_dim_1':dim_1_length}\n",
    "        out = xr.open_mfdataset(\n",
    "            matching_files,\n",
    "            engine='h5netcdf',\n",
    "            concat_dim='phony_dim_1',\n",
    "            combine='nested',\n",
    "            phony_dims='sort',\n",
    "            chunks=chunks,\n",
    "            parallel=True\n",
    "        ) \n",
    "        print('RUNNING')\n",
    "        with ProgressBar():\n",
    "            delayed_write=out.to_netcdf(out_file, engine='h5netcdf', compute=False)\n",
    "            delayed_write.compute()\n",
    "        return out\n",
    "\n",
    "    ####################################################################################\n",
    "    in_files=DataManager.outputDataDirectory+f'/processed_binary_arrays_{ModelData.res}_{ModelData.t_res}_{ModelData.Np_str}_*.h5'\n",
    "    out_file=DataManager.outputDataDirectory+f'/processed_binary_arrays_{ModelData.res}_{ModelData.t_res}_{ModelData.Np_str}.h5'\n",
    "    out = recombine_func(in_files,out_file)\n",
    "    print(\"\\n\",f\"Saved to {out_file}\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1e476e7e-f39f-46b5-bf38-a2d83fde7852",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recombining 60 files\n",
      "Forming Dask Delayed Object\n",
      "RUNNING\n",
      "[########################################] | 100% Completed | 4.57 sms\n",
      "\n",
      " Saved to /mnt/lustre/koa/koastore/torri_group/air_directory/Projects/DCI-Project/Code/OUTPUT/Variable_Calculation/EntrainmentCalculation/1km_5min_34nz/Entrainment_Preprocessing/processed_binary_arrays_1km_5min_1e6.h5\n"
     ]
    }
   ],
   "source": [
    "if recombine==True:\n",
    "    # Recombine(num_jobs=num_jobs)\n",
    "    out=Recombine_Dask()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
